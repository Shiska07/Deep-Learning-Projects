{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "cLuL-wFr9sl0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "rhaj50yM_9Wp"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVkwuf-Y_-Ae",
        "outputId": "e77ad320-4077-4c73-a78c-10bc9759df3b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read and transform data\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "# image needs to be resized as per the pre-trained model's parameters\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "\n",
        "train = datasets.CIFAR10(root='~/.pytorch/CIFAR10',train=True, download=True,transform=transform)\n",
        "test = datasets.CIFAR10(root='~/.pytorch/CIFAR10',train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a-Lv_8HAGA1",
        "outputId": "8c619cc0-a5ec-4635-e38f-69b9105c6c4d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check image size and labels\n",
        "for images, labels in train_loader:\n",
        "  print(images.size(), labels.size())\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enhfeO5KAfJ8",
        "outputId": "45e8d894-734e-4bad-d3a8-d92a0897434b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 3, 224, 224]) torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pick pre-trained model\n",
        "model = models.vgg16(pretrained = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0bQYdamAk48",
        "outputId": "fce5b19b-e383-438c-e64a-4f34ded9f327"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ENfSj5IA1t2",
        "outputId": "428154ab-198d-40bf-d39c-7c2558a3eb7e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmlT-2VYA5gM",
        "outputId": "9c3df79f-debc-4d47-916b-b8cd23041c14"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Dropout(p=0.5, inplace=False)\n",
              "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (4): ReLU(inplace=True)\n",
              "  (5): Dropout(p=0.5, inplace=False)\n",
              "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze the network parameters\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "vzPGCxkDBeV4"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove last fc layer and treat the network as a fixed feature extractor\n",
        "model.classifier[-1] = nn.Sequential(\n",
        "    nn.Linear(in_features=4096, out_features=10),\n",
        "    nn.LogSoftmax(dim = 1)\n",
        ")"
      ],
      "metadata": {
        "id": "IL0r3nLiDZvm"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJjwkatiDy9P",
        "outputId": "9c8d9031-7840-4783-b6a3-6b550f025a26"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Dropout(p=0.5, inplace=False)\n",
              "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (4): ReLU(inplace=True)\n",
              "  (5): Dropout(p=0.5, inplace=False)\n",
              "  (6): Sequential(\n",
              "    (0): Linear(in_features=4096, out_features=10, bias=True)\n",
              "    (1): LogSoftmax(dim=1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the model outputs log of softmax transformation so we can use NLLLoss\n",
        "criterion = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "zf3grJXKD4o2"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  print(param.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uE5sahgBELT2",
        "outputId": "08fb1382-c148-4465-d1c9-a65c5091a594"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the Fixed Feature Extractor"
      ],
      "metadata": {
        "id": "8Xbyft4uJrGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam"
      ],
      "metadata": {
        "id": "Sn4ss83vFAaH"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "optimizer = Adam(model.parameters())"
      ],
      "metadata": {
        "id": "I58B-PP_KlCX"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check labels format\n",
        "for images,labels in train_loader:\n",
        "  break\n",
        "\n",
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5Uox4zVLm8K",
        "outputId": "11d22580-7757-4fe9-f5d3-e2dc73f11cc2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 7, 5, 4, 1, 8, 6, 2, 5, 2, 0, 5, 5, 7, 8, 5, 4, 0, 9, 9, 8, 5, 4, 8,\n",
              "        5, 6, 7, 0, 6, 9, 1, 8, 8, 3, 3, 4, 6, 0, 2, 7, 3, 5, 2, 4, 1, 4, 0, 8,\n",
              "        5, 3, 9, 2, 0, 0, 1, 4, 0, 7, 8, 2, 0, 8, 7, 5, 5, 1, 8, 8, 9, 9, 2, 4,\n",
              "        1, 9, 6, 5, 7, 7, 9, 0, 8, 6, 7, 2, 1, 5, 1, 2, 5, 3, 0, 1, 1, 8, 3, 8,\n",
              "        2, 5, 0, 6, 0, 4, 8, 1, 5, 6, 5, 6, 4, 7, 3, 6, 5, 3, 9, 8, 0, 9, 6, 0,\n",
              "        5, 5, 4, 8, 3, 5, 4, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "batch_loss = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for batch_n, (X_train, y_train) in enumerate(train_loader,1):\n",
        "    X_train = X_train.to(device)\n",
        "    y_train = y_train.to(device)\n",
        "\n",
        "    # clear gradients\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(X_train)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    batch_loss += loss.item()\n",
        "    print(f'Epoch({epoch}/{epochs}): Batch({batch_n}/{len(train_loader)}): Loss: {loss.item()}')\n",
        "  print(f'Training loss : {batch_loss/len(train_loader)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA0pwMWZKqc_",
        "outputId": "3b1b8f2b-18ec-4452-bbfb-3c4cc5b1e22a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch(0/1): Batch(1/391): Loss: 2.4032492637634277\n",
            "Epoch(0/1): Batch(2/391): Loss: 2.1612887382507324\n",
            "Epoch(0/1): Batch(3/391): Loss: 1.9288427829742432\n",
            "Epoch(0/1): Batch(4/391): Loss: 1.7479336261749268\n",
            "Epoch(0/1): Batch(5/391): Loss: 1.6864545345306396\n",
            "Epoch(0/1): Batch(6/391): Loss: 1.4751477241516113\n",
            "Epoch(0/1): Batch(7/391): Loss: 1.440217137336731\n",
            "Epoch(0/1): Batch(8/391): Loss: 1.298289179801941\n",
            "Epoch(0/1): Batch(9/391): Loss: 1.1416314840316772\n",
            "Epoch(0/1): Batch(10/391): Loss: 1.1939246654510498\n",
            "Epoch(0/1): Batch(11/391): Loss: 1.1375075578689575\n",
            "Epoch(0/1): Batch(12/391): Loss: 1.011133074760437\n",
            "Epoch(0/1): Batch(13/391): Loss: 0.9231693744659424\n",
            "Epoch(0/1): Batch(14/391): Loss: 0.8555132150650024\n",
            "Epoch(0/1): Batch(15/391): Loss: 0.9213075637817383\n",
            "Epoch(0/1): Batch(16/391): Loss: 0.7504632472991943\n",
            "Epoch(0/1): Batch(17/391): Loss: 0.8201310038566589\n",
            "Epoch(0/1): Batch(18/391): Loss: 0.9495233297348022\n",
            "Epoch(0/1): Batch(19/391): Loss: 0.6940032243728638\n",
            "Epoch(0/1): Batch(20/391): Loss: 0.8381094932556152\n",
            "Epoch(0/1): Batch(21/391): Loss: 0.7890923023223877\n",
            "Epoch(0/1): Batch(22/391): Loss: 0.8641448616981506\n",
            "Epoch(0/1): Batch(23/391): Loss: 0.7580078840255737\n",
            "Epoch(0/1): Batch(24/391): Loss: 0.8496526479721069\n",
            "Epoch(0/1): Batch(25/391): Loss: 0.6650763154029846\n",
            "Epoch(0/1): Batch(26/391): Loss: 0.741758406162262\n",
            "Epoch(0/1): Batch(27/391): Loss: 0.745477557182312\n",
            "Epoch(0/1): Batch(28/391): Loss: 0.6938648223876953\n",
            "Epoch(0/1): Batch(29/391): Loss: 0.8150368928909302\n",
            "Epoch(0/1): Batch(30/391): Loss: 0.840390145778656\n",
            "Epoch(0/1): Batch(31/391): Loss: 0.7593737244606018\n",
            "Epoch(0/1): Batch(32/391): Loss: 0.7125914692878723\n",
            "Epoch(0/1): Batch(33/391): Loss: 0.6745232343673706\n",
            "Epoch(0/1): Batch(34/391): Loss: 0.9256305694580078\n",
            "Epoch(0/1): Batch(35/391): Loss: 0.7125751376152039\n",
            "Epoch(0/1): Batch(36/391): Loss: 0.5690928101539612\n",
            "Epoch(0/1): Batch(37/391): Loss: 0.7340681552886963\n",
            "Epoch(0/1): Batch(38/391): Loss: 0.7230612635612488\n",
            "Epoch(0/1): Batch(39/391): Loss: 0.661975085735321\n",
            "Epoch(0/1): Batch(40/391): Loss: 0.7393249273300171\n",
            "Epoch(0/1): Batch(41/391): Loss: 0.8270773887634277\n",
            "Epoch(0/1): Batch(42/391): Loss: 0.6092300415039062\n",
            "Epoch(0/1): Batch(43/391): Loss: 0.6073726415634155\n",
            "Epoch(0/1): Batch(44/391): Loss: 0.7402543425559998\n",
            "Epoch(0/1): Batch(45/391): Loss: 0.6300910711288452\n",
            "Epoch(0/1): Batch(46/391): Loss: 0.7306076288223267\n",
            "Epoch(0/1): Batch(47/391): Loss: 0.7695828676223755\n",
            "Epoch(0/1): Batch(48/391): Loss: 0.6674893498420715\n",
            "Epoch(0/1): Batch(49/391): Loss: 0.8140960931777954\n",
            "Epoch(0/1): Batch(50/391): Loss: 0.7318322658538818\n",
            "Epoch(0/1): Batch(51/391): Loss: 0.5818111896514893\n",
            "Epoch(0/1): Batch(52/391): Loss: 0.7446731328964233\n",
            "Epoch(0/1): Batch(53/391): Loss: 0.636612594127655\n",
            "Epoch(0/1): Batch(54/391): Loss: 0.730907678604126\n",
            "Epoch(0/1): Batch(55/391): Loss: 0.7492749094963074\n",
            "Epoch(0/1): Batch(56/391): Loss: 0.7118741273880005\n",
            "Epoch(0/1): Batch(57/391): Loss: 0.6736013889312744\n",
            "Epoch(0/1): Batch(58/391): Loss: 0.7561132907867432\n",
            "Epoch(0/1): Batch(59/391): Loss: 0.4583297371864319\n",
            "Epoch(0/1): Batch(60/391): Loss: 0.7007416486740112\n",
            "Epoch(0/1): Batch(61/391): Loss: 0.6863059401512146\n",
            "Epoch(0/1): Batch(62/391): Loss: 0.7009633183479309\n",
            "Epoch(0/1): Batch(63/391): Loss: 0.6396533846855164\n",
            "Epoch(0/1): Batch(64/391): Loss: 0.7965248823165894\n",
            "Epoch(0/1): Batch(65/391): Loss: 0.656548023223877\n",
            "Epoch(0/1): Batch(66/391): Loss: 0.7598044276237488\n",
            "Epoch(0/1): Batch(67/391): Loss: 0.5906877517700195\n",
            "Epoch(0/1): Batch(68/391): Loss: 0.6739404797554016\n",
            "Epoch(0/1): Batch(69/391): Loss: 0.6664076447486877\n",
            "Epoch(0/1): Batch(70/391): Loss: 0.6240803003311157\n",
            "Epoch(0/1): Batch(71/391): Loss: 0.5913042426109314\n",
            "Epoch(0/1): Batch(72/391): Loss: 0.5999608039855957\n",
            "Epoch(0/1): Batch(73/391): Loss: 0.6428672075271606\n",
            "Epoch(0/1): Batch(74/391): Loss: 0.6798102259635925\n",
            "Epoch(0/1): Batch(75/391): Loss: 0.6657748222351074\n",
            "Epoch(0/1): Batch(76/391): Loss: 0.6263437867164612\n",
            "Epoch(0/1): Batch(77/391): Loss: 0.7157244682312012\n",
            "Epoch(0/1): Batch(78/391): Loss: 0.6645044088363647\n",
            "Epoch(0/1): Batch(79/391): Loss: 0.606560468673706\n",
            "Epoch(0/1): Batch(80/391): Loss: 0.7410566806793213\n",
            "Epoch(0/1): Batch(81/391): Loss: 0.7051235437393188\n",
            "Epoch(0/1): Batch(82/391): Loss: 0.6165462136268616\n",
            "Epoch(0/1): Batch(83/391): Loss: 0.4923035502433777\n",
            "Epoch(0/1): Batch(84/391): Loss: 0.7142367959022522\n",
            "Epoch(0/1): Batch(85/391): Loss: 0.7253618240356445\n",
            "Epoch(0/1): Batch(86/391): Loss: 0.6883392930030823\n",
            "Epoch(0/1): Batch(87/391): Loss: 0.5180538296699524\n",
            "Epoch(0/1): Batch(88/391): Loss: 0.7063418626785278\n",
            "Epoch(0/1): Batch(89/391): Loss: 0.6612088084220886\n",
            "Epoch(0/1): Batch(90/391): Loss: 0.7041100263595581\n",
            "Epoch(0/1): Batch(91/391): Loss: 0.7401123642921448\n",
            "Epoch(0/1): Batch(92/391): Loss: 0.7433773875236511\n",
            "Epoch(0/1): Batch(93/391): Loss: 0.6743621826171875\n",
            "Epoch(0/1): Batch(94/391): Loss: 0.6555368304252625\n",
            "Epoch(0/1): Batch(95/391): Loss: 0.5811078548431396\n",
            "Epoch(0/1): Batch(96/391): Loss: 0.6966293454170227\n",
            "Epoch(0/1): Batch(97/391): Loss: 0.7487514615058899\n",
            "Epoch(0/1): Batch(98/391): Loss: 0.8178301453590393\n",
            "Epoch(0/1): Batch(99/391): Loss: 0.5659761428833008\n",
            "Epoch(0/1): Batch(100/391): Loss: 0.6274229884147644\n",
            "Epoch(0/1): Batch(101/391): Loss: 0.6200522184371948\n",
            "Epoch(0/1): Batch(102/391): Loss: 0.6472347378730774\n",
            "Epoch(0/1): Batch(103/391): Loss: 0.5031079649925232\n",
            "Epoch(0/1): Batch(104/391): Loss: 0.4420842230319977\n",
            "Epoch(0/1): Batch(105/391): Loss: 0.683826744556427\n",
            "Epoch(0/1): Batch(106/391): Loss: 0.7541075348854065\n",
            "Epoch(0/1): Batch(107/391): Loss: 0.6720537543296814\n",
            "Epoch(0/1): Batch(108/391): Loss: 0.6906940340995789\n",
            "Epoch(0/1): Batch(109/391): Loss: 0.6808722019195557\n",
            "Epoch(0/1): Batch(110/391): Loss: 0.6351901292800903\n",
            "Epoch(0/1): Batch(111/391): Loss: 0.5504622459411621\n",
            "Epoch(0/1): Batch(112/391): Loss: 0.5753659009933472\n",
            "Epoch(0/1): Batch(113/391): Loss: 0.5664161443710327\n",
            "Epoch(0/1): Batch(114/391): Loss: 0.49255093932151794\n",
            "Epoch(0/1): Batch(115/391): Loss: 0.5656121969223022\n",
            "Epoch(0/1): Batch(116/391): Loss: 0.5912045240402222\n",
            "Epoch(0/1): Batch(117/391): Loss: 0.5517740249633789\n",
            "Epoch(0/1): Batch(118/391): Loss: 0.5638564825057983\n",
            "Epoch(0/1): Batch(119/391): Loss: 0.6284650564193726\n",
            "Epoch(0/1): Batch(120/391): Loss: 0.5274463295936584\n",
            "Epoch(0/1): Batch(121/391): Loss: 0.593302309513092\n",
            "Epoch(0/1): Batch(122/391): Loss: 0.6298580169677734\n",
            "Epoch(0/1): Batch(123/391): Loss: 0.5828320980072021\n",
            "Epoch(0/1): Batch(124/391): Loss: 0.623286247253418\n",
            "Epoch(0/1): Batch(125/391): Loss: 0.5615364909172058\n",
            "Epoch(0/1): Batch(126/391): Loss: 0.756867527961731\n",
            "Epoch(0/1): Batch(127/391): Loss: 0.6009388566017151\n",
            "Epoch(0/1): Batch(128/391): Loss: 0.4758915603160858\n",
            "Epoch(0/1): Batch(129/391): Loss: 0.61832195520401\n",
            "Epoch(0/1): Batch(130/391): Loss: 0.545126736164093\n",
            "Epoch(0/1): Batch(131/391): Loss: 0.6937148571014404\n",
            "Epoch(0/1): Batch(132/391): Loss: 0.4989378750324249\n",
            "Epoch(0/1): Batch(133/391): Loss: 0.6845940351486206\n",
            "Epoch(0/1): Batch(134/391): Loss: 0.5629433393478394\n",
            "Epoch(0/1): Batch(135/391): Loss: 0.5116289258003235\n",
            "Epoch(0/1): Batch(136/391): Loss: 0.45203226804733276\n",
            "Epoch(0/1): Batch(137/391): Loss: 0.5830733180046082\n",
            "Epoch(0/1): Batch(138/391): Loss: 0.5071784853935242\n",
            "Epoch(0/1): Batch(139/391): Loss: 0.5914551615715027\n",
            "Epoch(0/1): Batch(140/391): Loss: 0.6157397627830505\n",
            "Epoch(0/1): Batch(141/391): Loss: 0.4947918951511383\n",
            "Epoch(0/1): Batch(142/391): Loss: 0.6484944820404053\n",
            "Epoch(0/1): Batch(143/391): Loss: 0.5973438024520874\n",
            "Epoch(0/1): Batch(144/391): Loss: 0.5010399222373962\n",
            "Epoch(0/1): Batch(145/391): Loss: 0.4925929307937622\n",
            "Epoch(0/1): Batch(146/391): Loss: 0.7053323984146118\n",
            "Epoch(0/1): Batch(147/391): Loss: 0.5100382566452026\n",
            "Epoch(0/1): Batch(148/391): Loss: 0.6040483713150024\n",
            "Epoch(0/1): Batch(149/391): Loss: 0.8729685544967651\n",
            "Epoch(0/1): Batch(150/391): Loss: 0.5955239534378052\n",
            "Epoch(0/1): Batch(151/391): Loss: 0.6267616748809814\n",
            "Epoch(0/1): Batch(152/391): Loss: 0.7860937714576721\n",
            "Epoch(0/1): Batch(153/391): Loss: 0.7594009637832642\n",
            "Epoch(0/1): Batch(154/391): Loss: 0.6068471670150757\n",
            "Epoch(0/1): Batch(155/391): Loss: 0.4974055588245392\n",
            "Epoch(0/1): Batch(156/391): Loss: 0.6854246258735657\n",
            "Epoch(0/1): Batch(157/391): Loss: 0.5808982849121094\n",
            "Epoch(0/1): Batch(158/391): Loss: 0.589069664478302\n",
            "Epoch(0/1): Batch(159/391): Loss: 0.5439069867134094\n",
            "Epoch(0/1): Batch(160/391): Loss: 0.5627340078353882\n",
            "Epoch(0/1): Batch(161/391): Loss: 0.44339749217033386\n",
            "Epoch(0/1): Batch(162/391): Loss: 0.49157166481018066\n",
            "Epoch(0/1): Batch(163/391): Loss: 0.7530549764633179\n",
            "Epoch(0/1): Batch(164/391): Loss: 0.47097641229629517\n",
            "Epoch(0/1): Batch(165/391): Loss: 0.5848166942596436\n",
            "Epoch(0/1): Batch(166/391): Loss: 0.5673763751983643\n",
            "Epoch(0/1): Batch(167/391): Loss: 0.6381819248199463\n",
            "Epoch(0/1): Batch(168/391): Loss: 0.49808257818222046\n",
            "Epoch(0/1): Batch(169/391): Loss: 0.6433039903640747\n",
            "Epoch(0/1): Batch(170/391): Loss: 0.4859280288219452\n",
            "Epoch(0/1): Batch(171/391): Loss: 0.6913810968399048\n",
            "Epoch(0/1): Batch(172/391): Loss: 0.6240368485450745\n",
            "Epoch(0/1): Batch(173/391): Loss: 0.47299185395240784\n",
            "Epoch(0/1): Batch(174/391): Loss: 0.5228874683380127\n",
            "Epoch(0/1): Batch(175/391): Loss: 0.619464635848999\n",
            "Epoch(0/1): Batch(176/391): Loss: 0.6295049786567688\n",
            "Epoch(0/1): Batch(177/391): Loss: 0.6696374416351318\n",
            "Epoch(0/1): Batch(178/391): Loss: 0.5288461446762085\n",
            "Epoch(0/1): Batch(179/391): Loss: 0.7183735966682434\n",
            "Epoch(0/1): Batch(180/391): Loss: 0.6589762568473816\n",
            "Epoch(0/1): Batch(181/391): Loss: 0.5311728119850159\n",
            "Epoch(0/1): Batch(182/391): Loss: 0.6123186349868774\n",
            "Epoch(0/1): Batch(183/391): Loss: 0.6551392674446106\n",
            "Epoch(0/1): Batch(184/391): Loss: 0.48570555448532104\n",
            "Epoch(0/1): Batch(185/391): Loss: 0.4781646430492401\n",
            "Epoch(0/1): Batch(186/391): Loss: 0.722387969493866\n",
            "Epoch(0/1): Batch(187/391): Loss: 0.43322333693504333\n",
            "Epoch(0/1): Batch(188/391): Loss: 0.6421974897384644\n",
            "Epoch(0/1): Batch(189/391): Loss: 0.5712586641311646\n",
            "Epoch(0/1): Batch(190/391): Loss: 0.7705409526824951\n",
            "Epoch(0/1): Batch(191/391): Loss: 0.6590568423271179\n",
            "Epoch(0/1): Batch(192/391): Loss: 0.6218131184577942\n",
            "Epoch(0/1): Batch(193/391): Loss: 0.7420456409454346\n",
            "Epoch(0/1): Batch(194/391): Loss: 0.5940629839897156\n",
            "Epoch(0/1): Batch(195/391): Loss: 0.7332007884979248\n",
            "Epoch(0/1): Batch(196/391): Loss: 0.5620791912078857\n",
            "Epoch(0/1): Batch(197/391): Loss: 0.5770146250724792\n",
            "Epoch(0/1): Batch(198/391): Loss: 0.6176162958145142\n",
            "Epoch(0/1): Batch(199/391): Loss: 0.5963089466094971\n",
            "Epoch(0/1): Batch(200/391): Loss: 0.5127915740013123\n",
            "Epoch(0/1): Batch(201/391): Loss: 0.71864253282547\n",
            "Epoch(0/1): Batch(202/391): Loss: 0.4766566753387451\n",
            "Epoch(0/1): Batch(203/391): Loss: 0.5979772806167603\n",
            "Epoch(0/1): Batch(204/391): Loss: 0.7069411873817444\n",
            "Epoch(0/1): Batch(205/391): Loss: 0.5629386901855469\n",
            "Epoch(0/1): Batch(206/391): Loss: 0.6767867803573608\n",
            "Epoch(0/1): Batch(207/391): Loss: 0.6576508283615112\n",
            "Epoch(0/1): Batch(208/391): Loss: 0.5835750699043274\n",
            "Epoch(0/1): Batch(209/391): Loss: 0.6857598423957825\n",
            "Epoch(0/1): Batch(210/391): Loss: 0.6498769521713257\n",
            "Epoch(0/1): Batch(211/391): Loss: 0.612091600894928\n",
            "Epoch(0/1): Batch(212/391): Loss: 0.5521977543830872\n",
            "Epoch(0/1): Batch(213/391): Loss: 0.7675017714500427\n",
            "Epoch(0/1): Batch(214/391): Loss: 0.493034690618515\n",
            "Epoch(0/1): Batch(215/391): Loss: 0.7562206387519836\n",
            "Epoch(0/1): Batch(216/391): Loss: 0.5869332551956177\n",
            "Epoch(0/1): Batch(217/391): Loss: 0.6139810681343079\n",
            "Epoch(0/1): Batch(218/391): Loss: 0.6635355949401855\n",
            "Epoch(0/1): Batch(219/391): Loss: 0.4956948757171631\n",
            "Epoch(0/1): Batch(220/391): Loss: 0.5190104842185974\n",
            "Epoch(0/1): Batch(221/391): Loss: 0.6123726963996887\n",
            "Epoch(0/1): Batch(222/391): Loss: 0.6109371781349182\n",
            "Epoch(0/1): Batch(223/391): Loss: 0.5122417211532593\n",
            "Epoch(0/1): Batch(224/391): Loss: 0.6583518981933594\n",
            "Epoch(0/1): Batch(225/391): Loss: 0.560452938079834\n",
            "Epoch(0/1): Batch(226/391): Loss: 0.5802958607673645\n",
            "Epoch(0/1): Batch(227/391): Loss: 0.6536931395530701\n",
            "Epoch(0/1): Batch(228/391): Loss: 0.6582875847816467\n",
            "Epoch(0/1): Batch(229/391): Loss: 0.4484926760196686\n",
            "Epoch(0/1): Batch(230/391): Loss: 0.4932488799095154\n",
            "Epoch(0/1): Batch(231/391): Loss: 0.5257279276847839\n",
            "Epoch(0/1): Batch(232/391): Loss: 0.4301776885986328\n",
            "Epoch(0/1): Batch(233/391): Loss: 0.5342928767204285\n",
            "Epoch(0/1): Batch(234/391): Loss: 0.5552966594696045\n",
            "Epoch(0/1): Batch(235/391): Loss: 0.7221938967704773\n",
            "Epoch(0/1): Batch(236/391): Loss: 0.7049926519393921\n",
            "Epoch(0/1): Batch(237/391): Loss: 0.6002933382987976\n",
            "Epoch(0/1): Batch(238/391): Loss: 0.5289038419723511\n",
            "Epoch(0/1): Batch(239/391): Loss: 0.5825833082199097\n",
            "Epoch(0/1): Batch(240/391): Loss: 0.6804373264312744\n",
            "Epoch(0/1): Batch(241/391): Loss: 0.5996299386024475\n",
            "Epoch(0/1): Batch(242/391): Loss: 0.5577263832092285\n",
            "Epoch(0/1): Batch(243/391): Loss: 0.6733030676841736\n",
            "Epoch(0/1): Batch(244/391): Loss: 0.5271644592285156\n",
            "Epoch(0/1): Batch(245/391): Loss: 0.5434682369232178\n",
            "Epoch(0/1): Batch(246/391): Loss: 0.6825008988380432\n",
            "Epoch(0/1): Batch(247/391): Loss: 0.43277421593666077\n",
            "Epoch(0/1): Batch(248/391): Loss: 0.5886916518211365\n",
            "Epoch(0/1): Batch(249/391): Loss: 0.547545313835144\n",
            "Epoch(0/1): Batch(250/391): Loss: 0.6088424921035767\n",
            "Epoch(0/1): Batch(251/391): Loss: 0.5456932187080383\n",
            "Epoch(0/1): Batch(252/391): Loss: 0.5033361315727234\n",
            "Epoch(0/1): Batch(253/391): Loss: 0.5792768001556396\n",
            "Epoch(0/1): Batch(254/391): Loss: 0.43937888741493225\n",
            "Epoch(0/1): Batch(255/391): Loss: 0.5471187233924866\n",
            "Epoch(0/1): Batch(256/391): Loss: 0.6036716103553772\n",
            "Epoch(0/1): Batch(257/391): Loss: 0.4975416660308838\n",
            "Epoch(0/1): Batch(258/391): Loss: 0.5896128416061401\n",
            "Epoch(0/1): Batch(259/391): Loss: 0.6922637224197388\n",
            "Epoch(0/1): Batch(260/391): Loss: 0.5495302677154541\n",
            "Epoch(0/1): Batch(261/391): Loss: 0.6235830187797546\n",
            "Epoch(0/1): Batch(262/391): Loss: 0.5838541984558105\n",
            "Epoch(0/1): Batch(263/391): Loss: 0.5327906608581543\n",
            "Epoch(0/1): Batch(264/391): Loss: 0.7049622535705566\n",
            "Epoch(0/1): Batch(265/391): Loss: 0.5112119913101196\n",
            "Epoch(0/1): Batch(266/391): Loss: 0.6549280881881714\n",
            "Epoch(0/1): Batch(267/391): Loss: 0.5559468865394592\n",
            "Epoch(0/1): Batch(268/391): Loss: 0.6007435321807861\n",
            "Epoch(0/1): Batch(269/391): Loss: 0.5380553603172302\n",
            "Epoch(0/1): Batch(270/391): Loss: 0.7434196472167969\n",
            "Epoch(0/1): Batch(271/391): Loss: 0.6195066571235657\n",
            "Epoch(0/1): Batch(272/391): Loss: 0.5075949430465698\n",
            "Epoch(0/1): Batch(273/391): Loss: 0.6223534345626831\n",
            "Epoch(0/1): Batch(274/391): Loss: 0.6224658489227295\n",
            "Epoch(0/1): Batch(275/391): Loss: 0.5526464581489563\n",
            "Epoch(0/1): Batch(276/391): Loss: 0.44401034712791443\n",
            "Epoch(0/1): Batch(277/391): Loss: 0.5031702518463135\n",
            "Epoch(0/1): Batch(278/391): Loss: 0.6338591575622559\n",
            "Epoch(0/1): Batch(279/391): Loss: 0.7643337249755859\n",
            "Epoch(0/1): Batch(280/391): Loss: 0.4823540449142456\n",
            "Epoch(0/1): Batch(281/391): Loss: 0.6655115485191345\n",
            "Epoch(0/1): Batch(282/391): Loss: 0.6868082284927368\n",
            "Epoch(0/1): Batch(283/391): Loss: 0.7574747204780579\n",
            "Epoch(0/1): Batch(284/391): Loss: 0.6880432963371277\n",
            "Epoch(0/1): Batch(285/391): Loss: 0.680885910987854\n",
            "Epoch(0/1): Batch(286/391): Loss: 0.6252580881118774\n",
            "Epoch(0/1): Batch(287/391): Loss: 0.5552597641944885\n",
            "Epoch(0/1): Batch(288/391): Loss: 0.550550103187561\n",
            "Epoch(0/1): Batch(289/391): Loss: 0.6978505849838257\n",
            "Epoch(0/1): Batch(290/391): Loss: 0.6235320568084717\n",
            "Epoch(0/1): Batch(291/391): Loss: 0.5527140498161316\n",
            "Epoch(0/1): Batch(292/391): Loss: 0.7042803168296814\n",
            "Epoch(0/1): Batch(293/391): Loss: 0.6111394166946411\n",
            "Epoch(0/1): Batch(294/391): Loss: 0.654156506061554\n",
            "Epoch(0/1): Batch(295/391): Loss: 0.5561538934707642\n",
            "Epoch(0/1): Batch(296/391): Loss: 0.5057293176651001\n",
            "Epoch(0/1): Batch(297/391): Loss: 0.6555230617523193\n",
            "Epoch(0/1): Batch(298/391): Loss: 0.6528307795524597\n",
            "Epoch(0/1): Batch(299/391): Loss: 0.5342622995376587\n",
            "Epoch(0/1): Batch(300/391): Loss: 0.543237030506134\n",
            "Epoch(0/1): Batch(301/391): Loss: 0.5995765924453735\n",
            "Epoch(0/1): Batch(302/391): Loss: 0.5535406470298767\n",
            "Epoch(0/1): Batch(303/391): Loss: 0.6724610924720764\n",
            "Epoch(0/1): Batch(304/391): Loss: 0.5684608817100525\n",
            "Epoch(0/1): Batch(305/391): Loss: 0.6024225950241089\n",
            "Epoch(0/1): Batch(306/391): Loss: 0.4447997808456421\n",
            "Epoch(0/1): Batch(307/391): Loss: 0.5686984062194824\n",
            "Epoch(0/1): Batch(308/391): Loss: 0.55549156665802\n",
            "Epoch(0/1): Batch(309/391): Loss: 0.7750328779220581\n",
            "Epoch(0/1): Batch(310/391): Loss: 0.5874130725860596\n",
            "Epoch(0/1): Batch(311/391): Loss: 0.5768622756004333\n",
            "Epoch(0/1): Batch(312/391): Loss: 0.6389572024345398\n",
            "Epoch(0/1): Batch(313/391): Loss: 0.6686232089996338\n",
            "Epoch(0/1): Batch(314/391): Loss: 0.5549516081809998\n",
            "Epoch(0/1): Batch(315/391): Loss: 0.5642979145050049\n",
            "Epoch(0/1): Batch(316/391): Loss: 0.6148373484611511\n",
            "Epoch(0/1): Batch(317/391): Loss: 0.3301750719547272\n",
            "Epoch(0/1): Batch(318/391): Loss: 0.49687111377716064\n",
            "Epoch(0/1): Batch(319/391): Loss: 0.7050608396530151\n",
            "Epoch(0/1): Batch(320/391): Loss: 0.4486526548862457\n",
            "Epoch(0/1): Batch(321/391): Loss: 0.6397719979286194\n",
            "Epoch(0/1): Batch(322/391): Loss: 0.4711802303791046\n",
            "Epoch(0/1): Batch(323/391): Loss: 0.5395539999008179\n",
            "Epoch(0/1): Batch(324/391): Loss: 0.5207778215408325\n",
            "Epoch(0/1): Batch(325/391): Loss: 0.5433774590492249\n",
            "Epoch(0/1): Batch(326/391): Loss: 0.624823272228241\n",
            "Epoch(0/1): Batch(327/391): Loss: 0.5972657203674316\n",
            "Epoch(0/1): Batch(328/391): Loss: 0.5960686206817627\n",
            "Epoch(0/1): Batch(329/391): Loss: 0.6157602071762085\n",
            "Epoch(0/1): Batch(330/391): Loss: 0.5791546106338501\n",
            "Epoch(0/1): Batch(331/391): Loss: 0.5688106417655945\n",
            "Epoch(0/1): Batch(332/391): Loss: 0.5630615949630737\n",
            "Epoch(0/1): Batch(333/391): Loss: 0.6579589247703552\n",
            "Epoch(0/1): Batch(334/391): Loss: 0.6762433052062988\n",
            "Epoch(0/1): Batch(335/391): Loss: 0.44693613052368164\n",
            "Epoch(0/1): Batch(336/391): Loss: 0.6752858757972717\n",
            "Epoch(0/1): Batch(337/391): Loss: 0.564335823059082\n",
            "Epoch(0/1): Batch(338/391): Loss: 0.589387059211731\n",
            "Epoch(0/1): Batch(339/391): Loss: 0.5509465336799622\n",
            "Epoch(0/1): Batch(340/391): Loss: 0.6338801383972168\n",
            "Epoch(0/1): Batch(341/391): Loss: 0.7039268016815186\n",
            "Epoch(0/1): Batch(342/391): Loss: 0.6346210837364197\n",
            "Epoch(0/1): Batch(343/391): Loss: 0.5441796779632568\n",
            "Epoch(0/1): Batch(344/391): Loss: 0.7051323056221008\n",
            "Epoch(0/1): Batch(345/391): Loss: 0.469064861536026\n",
            "Epoch(0/1): Batch(346/391): Loss: 0.6662508249282837\n",
            "Epoch(0/1): Batch(347/391): Loss: 0.6811051368713379\n",
            "Epoch(0/1): Batch(348/391): Loss: 0.5899878144264221\n",
            "Epoch(0/1): Batch(349/391): Loss: 0.5448057651519775\n",
            "Epoch(0/1): Batch(350/391): Loss: 0.4206656813621521\n",
            "Epoch(0/1): Batch(351/391): Loss: 0.5328763127326965\n",
            "Epoch(0/1): Batch(352/391): Loss: 0.634149968624115\n",
            "Epoch(0/1): Batch(353/391): Loss: 0.5074957609176636\n",
            "Epoch(0/1): Batch(354/391): Loss: 0.5097830295562744\n",
            "Epoch(0/1): Batch(355/391): Loss: 0.621478259563446\n",
            "Epoch(0/1): Batch(356/391): Loss: 0.6085591912269592\n",
            "Epoch(0/1): Batch(357/391): Loss: 0.5540964007377625\n",
            "Epoch(0/1): Batch(358/391): Loss: 0.598135232925415\n",
            "Epoch(0/1): Batch(359/391): Loss: 0.5518873929977417\n",
            "Epoch(0/1): Batch(360/391): Loss: 0.6787193417549133\n",
            "Epoch(0/1): Batch(361/391): Loss: 0.5558788776397705\n",
            "Epoch(0/1): Batch(362/391): Loss: 0.5351393222808838\n",
            "Epoch(0/1): Batch(363/391): Loss: 0.686190664768219\n",
            "Epoch(0/1): Batch(364/391): Loss: 0.6129499673843384\n",
            "Epoch(0/1): Batch(365/391): Loss: 0.6513938307762146\n",
            "Epoch(0/1): Batch(366/391): Loss: 0.6386139988899231\n",
            "Epoch(0/1): Batch(367/391): Loss: 0.5358400344848633\n",
            "Epoch(0/1): Batch(368/391): Loss: 0.5380076169967651\n",
            "Epoch(0/1): Batch(369/391): Loss: 0.5098155736923218\n",
            "Epoch(0/1): Batch(370/391): Loss: 0.5757653117179871\n",
            "Epoch(0/1): Batch(371/391): Loss: 0.4967414438724518\n",
            "Epoch(0/1): Batch(372/391): Loss: 0.572399914264679\n",
            "Epoch(0/1): Batch(373/391): Loss: 0.5282684564590454\n",
            "Epoch(0/1): Batch(374/391): Loss: 0.6675882339477539\n",
            "Epoch(0/1): Batch(375/391): Loss: 0.45498013496398926\n",
            "Epoch(0/1): Batch(376/391): Loss: 0.44702836871147156\n",
            "Epoch(0/1): Batch(377/391): Loss: 0.6027613282203674\n",
            "Epoch(0/1): Batch(378/391): Loss: 0.4965726137161255\n",
            "Epoch(0/1): Batch(379/391): Loss: 0.6379246711730957\n",
            "Epoch(0/1): Batch(380/391): Loss: 0.5126864314079285\n",
            "Epoch(0/1): Batch(381/391): Loss: 0.827492356300354\n",
            "Epoch(0/1): Batch(382/391): Loss: 0.6539439558982849\n",
            "Epoch(0/1): Batch(383/391): Loss: 0.6064848303794861\n",
            "Epoch(0/1): Batch(384/391): Loss: 0.5656779408454895\n",
            "Epoch(0/1): Batch(385/391): Loss: 0.4879419803619385\n",
            "Epoch(0/1): Batch(386/391): Loss: 0.6416502594947815\n",
            "Epoch(0/1): Batch(387/391): Loss: 0.373364120721817\n",
            "Epoch(0/1): Batch(388/391): Loss: 0.5837802290916443\n",
            "Epoch(0/1): Batch(389/391): Loss: 0.69599449634552\n",
            "Epoch(0/1): Batch(390/391): Loss: 0.6710100769996643\n",
            "Epoch(0/1): Batch(391/391): Loss: 0.6095101237297058\n",
            "Training loss : 0.6476045018419281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CIFAR10_classes = ['plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# move model to CPU\n",
        "model.to('cpu')\n",
        "# set to evaluation model to stop tracking gradients\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  X_test, y_test = next(iter(test_loader))\n",
        "  y_pred = model(X_test)\n",
        "\n",
        "  # take exp to remove log, this is now just to softmax values\n",
        "  output = torch.exp(y_pred)\n",
        "\n",
        "  # take argmax to get index value\n",
        "  y_pred_class = torch.argmax(output,1)\n",
        "\n",
        "# reset model to training mode\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWP9oQ6dOBmh",
        "outputId": "e69c9310-3e98-48c2-91e1-8b8b39a2d561"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Sequential(\n",
              "      (0): Linear(in_features=4096, out_features=10, bias=True)\n",
              "      (1): LogSoftmax(dim=1)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model"
      ],
      "metadata": {
        "id": "vXbLnyp8QIMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_corr = (y_test == y_pred_class).sum()\n",
        "n_total = len(y_pred)"
      ],
      "metadata": {
        "id": "LeRlkXp1QF86"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print accuracy\n",
        "print(f'Test acc: {n_corr/n_total}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjxFoaUYQbNy",
        "outputId": "3a5cfb5a-747a-4f4c-ca17-1132ee3ba78a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test acc: 0.828125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d5_T1GZpUQZE"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "12raxgTIWhPS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}