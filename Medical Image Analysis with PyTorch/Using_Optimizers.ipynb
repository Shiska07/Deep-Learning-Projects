{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBehvqZ8znsy"
      },
      "source": [
        "# Using optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCdIqY0tKbvS"
      },
      "source": [
        "# Setting seeds to try and ensure we have the same results - this is not guaranteed across PyTorch releases.\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCJzXv0OK1Bs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faed5008-158f-4fb3-9308-165884a45afa"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "mean, std = (0.5,), (0.5,)\n",
        "\n",
        "# Create a transform and normalise data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean, std)\n",
        "                              ])\n",
        "\n",
        "# Download FMNIST training dataset and load training data\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download FMNIST test dataset and load test data\n",
        "testset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:00<00:00, 122663177.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/FMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 5997232.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/FMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 4422102/4422102 [00:00<00:00, 62838209.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/FMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 1295122.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/FMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/FMNIST/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FMNIST(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(784, 128)\n",
        "    self.fc2 = nn.Linear(128,64)\n",
        "    self.fc3 = nn.Linear(64,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0], -1)\n",
        "\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "_elbWri3vywL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c0QgxCF3fD-"
      },
      "source": [
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iPQek2nz2yu"
      },
      "source": [
        "images, labels = next(iter(trainloader))\n",
        "images = images.view(images.shape[0], -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roihp-kN0Jw5"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "# define criteria and optimizer\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nf2WdmP5Gst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "354e0b74-f728-4285-bffe-93ed00ff52ee"
      },
      "source": [
        "output = model(images)\n",
        "\n",
        "# calculate loss\n",
        "loss = criterion(output, labels)\n",
        "\n",
        "# calcualte gradients\n",
        "loss.backward()\n",
        "print('Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
            "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
            "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0295,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0220, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0066,  0.0125,  ...,  0.0286,  0.0350, -0.0105]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  tensor([[-0.0030, -0.0030, -0.0030,  ..., -0.0030, -0.0030, -0.0030],\n",
            "        [ 0.0022,  0.0022,  0.0022,  ...,  0.0024,  0.0022,  0.0022],\n",
            "        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002],\n",
            "        ...,\n",
            "        [ 0.0014,  0.0014,  0.0014,  ...,  0.0014,  0.0014,  0.0014],\n",
            "        [ 0.0021,  0.0021,  0.0021,  ...,  0.0022,  0.0021,  0.0021],\n",
            "        [ 0.0038,  0.0038,  0.0038,  ...,  0.0038,  0.0038,  0.0038]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arwzAK-1EkEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf563a60-366b-4c18-d674-70cf4433d4be"
      },
      "source": [
        "# gradients associated with the first layer\n",
        "model[0].weight.grad.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD-u49yzEj6v"
      },
      "source": [
        "# apply gradients\n",
        "optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuGKi_nq6P0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89017a84-7de8-4e19-974f-ec752b0971e9"
      },
      "source": [
        "print('Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0002,  0.0192, -0.0294,  ...,  0.0220,  0.0038,  0.0021],\n",
            "        [-0.0198, -0.0150, -0.0105,  ..., -0.0203, -0.0060, -0.0300],\n",
            "        [-0.0202,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0296,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0221, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0065,  0.0125,  ...,  0.0285,  0.0349, -0.0106]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  tensor([[-0.0030, -0.0030, -0.0030,  ..., -0.0030, -0.0030, -0.0030],\n",
            "        [ 0.0022,  0.0022,  0.0022,  ...,  0.0024,  0.0022,  0.0022],\n",
            "        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002],\n",
            "        ...,\n",
            "        [ 0.0014,  0.0014,  0.0014,  ...,  0.0014,  0.0014,  0.0014],\n",
            "        [ 0.0021,  0.0021,  0.0021,  ...,  0.0022,  0.0021,  0.0021],\n",
            "        [ 0.0038,  0.0038,  0.0038,  ...,  0.0038,  0.0038,  0.0038]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8oIy5SkEpDn"
      },
      "source": [
        "# set gradients to zero as we want to calculate gradients w.r.t. loss of the next batch\n",
        "optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EniqxHDwDa8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c9a754-5cb2-4b19-d201-a26ad9c4b864"
      },
      "source": [
        "print('Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0002,  0.0192, -0.0294,  ...,  0.0220,  0.0038,  0.0021],\n",
            "        [-0.0198, -0.0150, -0.0105,  ..., -0.0203, -0.0060, -0.0300],\n",
            "        [-0.0202,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0296,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0221, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0065,  0.0125,  ...,  0.0285,  0.0349, -0.0106]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGZhQE3tDcqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "936729b5-9797-4cfe-b75f-071472667a41"
      },
      "source": [
        "model = FMNIST()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    cum_loss = 0\n",
        "    batch_n = 0\n",
        "\n",
        "    for batch_n, (images, labels) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cum_loss += loss.item()\n",
        "        print(f'batch:{batch_n}, loss:{loss.item()}')\n",
        "\n",
        "\n",
        "    print(f\"Training loss: {cum_loss/len(trainloader)}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch:0, loss:2.3215713500976562\n",
            "batch:1, loss:2.3074803352355957\n",
            "batch:2, loss:2.3164830207824707\n",
            "batch:3, loss:2.297412157058716\n",
            "batch:4, loss:2.2926530838012695\n",
            "batch:5, loss:2.2953672409057617\n",
            "batch:6, loss:2.3005030155181885\n",
            "batch:7, loss:2.2910714149475098\n",
            "batch:8, loss:2.2748677730560303\n",
            "batch:9, loss:2.2863876819610596\n",
            "batch:10, loss:2.268754720687866\n",
            "batch:11, loss:2.268362522125244\n",
            "batch:12, loss:2.286766529083252\n",
            "batch:13, loss:2.2703473567962646\n",
            "batch:14, loss:2.2913095951080322\n",
            "batch:15, loss:2.2611446380615234\n",
            "batch:16, loss:2.2757766246795654\n",
            "batch:17, loss:2.27260160446167\n",
            "batch:18, loss:2.27754807472229\n",
            "batch:19, loss:2.273784875869751\n",
            "batch:20, loss:2.230733633041382\n",
            "batch:21, loss:2.2553842067718506\n",
            "batch:22, loss:2.2328648567199707\n",
            "batch:23, loss:2.255995035171509\n",
            "batch:24, loss:2.2610247135162354\n",
            "batch:25, loss:2.240332841873169\n",
            "batch:26, loss:2.235386848449707\n",
            "batch:27, loss:2.2434561252593994\n",
            "batch:28, loss:2.2051637172698975\n",
            "batch:29, loss:2.2206692695617676\n",
            "batch:30, loss:2.242758274078369\n",
            "batch:31, loss:2.226162910461426\n",
            "batch:32, loss:2.244436740875244\n",
            "batch:33, loss:2.231757402420044\n",
            "batch:34, loss:2.2038097381591797\n",
            "batch:35, loss:2.1911041736602783\n",
            "batch:36, loss:2.1948752403259277\n",
            "batch:37, loss:2.1990513801574707\n",
            "batch:38, loss:2.214055061340332\n",
            "batch:39, loss:2.191995143890381\n",
            "batch:40, loss:2.1939537525177\n",
            "batch:41, loss:2.1965179443359375\n",
            "batch:42, loss:2.2005298137664795\n",
            "batch:43, loss:2.181551694869995\n",
            "batch:44, loss:2.1860036849975586\n",
            "batch:45, loss:2.164980411529541\n",
            "batch:46, loss:2.167433023452759\n",
            "batch:47, loss:2.163151979446411\n",
            "batch:48, loss:2.1516082286834717\n",
            "batch:49, loss:2.1331982612609863\n",
            "batch:50, loss:2.1778361797332764\n",
            "batch:51, loss:2.1359477043151855\n",
            "batch:52, loss:2.134805202484131\n",
            "batch:53, loss:2.1352760791778564\n",
            "batch:54, loss:2.143667697906494\n",
            "batch:55, loss:2.1367557048797607\n",
            "batch:56, loss:2.1460916996002197\n",
            "batch:57, loss:2.1325337886810303\n",
            "batch:58, loss:2.1505327224731445\n",
            "batch:59, loss:2.083942174911499\n",
            "batch:60, loss:2.0953426361083984\n",
            "batch:61, loss:2.0992610454559326\n",
            "batch:62, loss:2.1095194816589355\n",
            "batch:63, loss:2.0754876136779785\n",
            "batch:64, loss:2.073982000350952\n",
            "batch:65, loss:2.132622003555298\n",
            "batch:66, loss:2.099835157394409\n",
            "batch:67, loss:2.043778896331787\n",
            "batch:68, loss:2.0729687213897705\n",
            "batch:69, loss:2.0938217639923096\n",
            "batch:70, loss:2.10217547416687\n",
            "batch:71, loss:2.056648015975952\n",
            "batch:72, loss:2.081895112991333\n",
            "batch:73, loss:2.0219688415527344\n",
            "batch:74, loss:2.059443950653076\n",
            "batch:75, loss:2.068291425704956\n",
            "batch:76, loss:2.054886817932129\n",
            "batch:77, loss:2.02876615524292\n",
            "batch:78, loss:2.0386362075805664\n",
            "batch:79, loss:1.9955843687057495\n",
            "batch:80, loss:1.9903777837753296\n",
            "batch:81, loss:1.9961559772491455\n",
            "batch:82, loss:2.0275208950042725\n",
            "batch:83, loss:1.9773322343826294\n",
            "batch:84, loss:1.966648817062378\n",
            "batch:85, loss:1.9854862689971924\n",
            "batch:86, loss:1.9483871459960938\n",
            "batch:87, loss:1.979067087173462\n",
            "batch:88, loss:1.9612038135528564\n",
            "batch:89, loss:1.9842612743377686\n",
            "batch:90, loss:1.9231582880020142\n",
            "batch:91, loss:1.95497727394104\n",
            "batch:92, loss:1.9392272233963013\n",
            "batch:93, loss:1.8900784254074097\n",
            "batch:94, loss:1.9706110954284668\n",
            "batch:95, loss:1.8864530324935913\n",
            "batch:96, loss:1.9499945640563965\n",
            "batch:97, loss:1.8619617223739624\n",
            "batch:98, loss:1.9457640647888184\n",
            "batch:99, loss:1.9349318742752075\n",
            "batch:100, loss:1.9444459676742554\n",
            "batch:101, loss:1.9407682418823242\n",
            "batch:102, loss:1.8843700885772705\n",
            "batch:103, loss:1.907165765762329\n",
            "batch:104, loss:1.885127067565918\n",
            "batch:105, loss:1.838998556137085\n",
            "batch:106, loss:1.8788009881973267\n",
            "batch:107, loss:1.8974528312683105\n",
            "batch:108, loss:1.8449511528015137\n",
            "batch:109, loss:1.8473812341690063\n",
            "batch:110, loss:1.8380472660064697\n",
            "batch:111, loss:1.8186711072921753\n",
            "batch:112, loss:1.8119462728500366\n",
            "batch:113, loss:1.829735517501831\n",
            "batch:114, loss:1.7757610082626343\n",
            "batch:115, loss:1.799276351928711\n",
            "batch:116, loss:1.7289690971374512\n",
            "batch:117, loss:1.6927753686904907\n",
            "batch:118, loss:1.8219108581542969\n",
            "batch:119, loss:1.811837077140808\n",
            "batch:120, loss:1.7215633392333984\n",
            "batch:121, loss:1.7490637302398682\n",
            "batch:122, loss:1.7260392904281616\n",
            "batch:123, loss:1.8227908611297607\n",
            "batch:124, loss:1.7761588096618652\n",
            "batch:125, loss:1.7065585851669312\n",
            "batch:126, loss:1.6908060312271118\n",
            "batch:127, loss:1.7021220922470093\n",
            "batch:128, loss:1.6924667358398438\n",
            "batch:129, loss:1.7173854112625122\n",
            "batch:130, loss:1.828930377960205\n",
            "batch:131, loss:1.7294394969940186\n",
            "batch:132, loss:1.6882712841033936\n",
            "batch:133, loss:1.6525688171386719\n",
            "batch:134, loss:1.7551229000091553\n",
            "batch:135, loss:1.5355976819992065\n",
            "batch:136, loss:1.646278977394104\n",
            "batch:137, loss:1.6465942859649658\n",
            "batch:138, loss:1.6634668111801147\n",
            "batch:139, loss:1.6137865781784058\n",
            "batch:140, loss:1.6068750619888306\n",
            "batch:141, loss:1.5990670919418335\n",
            "batch:142, loss:1.549269676208496\n",
            "batch:143, loss:1.612372636795044\n",
            "batch:144, loss:1.6153498888015747\n",
            "batch:145, loss:1.5662177801132202\n",
            "batch:146, loss:1.6139767169952393\n",
            "batch:147, loss:1.571598768234253\n",
            "batch:148, loss:1.5634262561798096\n",
            "batch:149, loss:1.6037116050720215\n",
            "batch:150, loss:1.5231215953826904\n",
            "batch:151, loss:1.58145010471344\n",
            "batch:152, loss:1.4940907955169678\n",
            "batch:153, loss:1.570279598236084\n",
            "batch:154, loss:1.5212146043777466\n",
            "batch:155, loss:1.4687111377716064\n",
            "batch:156, loss:1.5223448276519775\n",
            "batch:157, loss:1.5174373388290405\n",
            "batch:158, loss:1.5285216569900513\n",
            "batch:159, loss:1.412540316581726\n",
            "batch:160, loss:1.523037075996399\n",
            "batch:161, loss:1.46878981590271\n",
            "batch:162, loss:1.4655696153640747\n",
            "batch:163, loss:1.5531837940216064\n",
            "batch:164, loss:1.363665223121643\n",
            "batch:165, loss:1.4294335842132568\n",
            "batch:166, loss:1.530592918395996\n",
            "batch:167, loss:1.4400734901428223\n",
            "batch:168, loss:1.5289416313171387\n",
            "batch:169, loss:1.333319067955017\n",
            "batch:170, loss:1.3673577308654785\n",
            "batch:171, loss:1.477495789527893\n",
            "batch:172, loss:1.402530312538147\n",
            "batch:173, loss:1.3398621082305908\n",
            "batch:174, loss:1.5014078617095947\n",
            "batch:175, loss:1.4087949991226196\n",
            "batch:176, loss:1.3808907270431519\n",
            "batch:177, loss:1.3921406269073486\n",
            "batch:178, loss:1.3133984804153442\n",
            "batch:179, loss:1.423866868019104\n",
            "batch:180, loss:1.3105669021606445\n",
            "batch:181, loss:1.4255033731460571\n",
            "batch:182, loss:1.2454816102981567\n",
            "batch:183, loss:1.2572320699691772\n",
            "batch:184, loss:1.4544849395751953\n",
            "batch:185, loss:1.4189144372940063\n",
            "batch:186, loss:1.3786885738372803\n",
            "batch:187, loss:1.3173240423202515\n",
            "batch:188, loss:1.3588656187057495\n",
            "batch:189, loss:1.3300764560699463\n",
            "batch:190, loss:1.39619779586792\n",
            "batch:191, loss:1.3493255376815796\n",
            "batch:192, loss:1.3257120847702026\n",
            "batch:193, loss:1.364181637763977\n",
            "batch:194, loss:1.2207283973693848\n",
            "batch:195, loss:1.2762483358383179\n",
            "batch:196, loss:1.2875434160232544\n",
            "batch:197, loss:1.354750156402588\n",
            "batch:198, loss:1.318312168121338\n",
            "batch:199, loss:1.2830575704574585\n",
            "batch:200, loss:1.2942707538604736\n",
            "batch:201, loss:1.2417126893997192\n",
            "batch:202, loss:1.2260706424713135\n",
            "batch:203, loss:1.241748571395874\n",
            "batch:204, loss:1.357450008392334\n",
            "batch:205, loss:1.3343126773834229\n",
            "batch:206, loss:1.3682434558868408\n",
            "batch:207, loss:1.2734856605529785\n",
            "batch:208, loss:1.1866130828857422\n",
            "batch:209, loss:1.1641842126846313\n",
            "batch:210, loss:1.1499658823013306\n",
            "batch:211, loss:1.2440028190612793\n",
            "batch:212, loss:1.1706194877624512\n",
            "batch:213, loss:1.174830436706543\n",
            "batch:214, loss:1.1569327116012573\n",
            "batch:215, loss:1.2802579402923584\n",
            "batch:216, loss:1.177161455154419\n",
            "batch:217, loss:1.1578035354614258\n",
            "batch:218, loss:1.2280371189117432\n",
            "batch:219, loss:1.180006980895996\n",
            "batch:220, loss:1.226579189300537\n",
            "batch:221, loss:1.267601490020752\n",
            "batch:222, loss:1.1983592510223389\n",
            "batch:223, loss:1.1959627866744995\n",
            "batch:224, loss:1.1601349115371704\n",
            "batch:225, loss:1.1433310508728027\n",
            "batch:226, loss:1.251500129699707\n",
            "batch:227, loss:1.1431307792663574\n",
            "batch:228, loss:1.122124433517456\n",
            "batch:229, loss:1.1665568351745605\n",
            "batch:230, loss:1.09718656539917\n",
            "batch:231, loss:1.1552088260650635\n",
            "batch:232, loss:1.1810898780822754\n",
            "batch:233, loss:1.0413883924484253\n",
            "batch:234, loss:1.1132420301437378\n",
            "batch:235, loss:1.0657382011413574\n",
            "batch:236, loss:1.1258310079574585\n",
            "batch:237, loss:1.1661055088043213\n",
            "batch:238, loss:1.1559984683990479\n",
            "batch:239, loss:1.1908355951309204\n",
            "batch:240, loss:1.015283465385437\n",
            "batch:241, loss:1.1262826919555664\n",
            "batch:242, loss:1.1268223524093628\n",
            "batch:243, loss:1.057759165763855\n",
            "batch:244, loss:1.172175407409668\n",
            "batch:245, loss:1.2536020278930664\n",
            "batch:246, loss:1.1177990436553955\n",
            "batch:247, loss:0.9556705951690674\n",
            "batch:248, loss:1.1890673637390137\n",
            "batch:249, loss:0.9430696368217468\n",
            "batch:250, loss:1.2418887615203857\n",
            "batch:251, loss:1.0399521589279175\n",
            "batch:252, loss:1.026871919631958\n",
            "batch:253, loss:1.122694492340088\n",
            "batch:254, loss:0.958888053894043\n",
            "batch:255, loss:1.0482428073883057\n",
            "batch:256, loss:1.0536381006240845\n",
            "batch:257, loss:0.9633442759513855\n",
            "batch:258, loss:0.9027984738349915\n",
            "batch:259, loss:1.047760009765625\n",
            "batch:260, loss:1.0276318788528442\n",
            "batch:261, loss:1.036224603652954\n",
            "batch:262, loss:1.041273593902588\n",
            "batch:263, loss:1.0915523767471313\n",
            "batch:264, loss:1.0031007528305054\n",
            "batch:265, loss:1.0471007823944092\n",
            "batch:266, loss:1.023746132850647\n",
            "batch:267, loss:0.9736934304237366\n",
            "batch:268, loss:1.0728448629379272\n",
            "batch:269, loss:1.12032949924469\n",
            "batch:270, loss:1.1260650157928467\n",
            "batch:271, loss:0.9762080311775208\n",
            "batch:272, loss:0.9753857851028442\n",
            "batch:273, loss:1.080201506614685\n",
            "batch:274, loss:0.9871788620948792\n",
            "batch:275, loss:0.921271562576294\n",
            "batch:276, loss:1.0659838914871216\n",
            "batch:277, loss:1.0326862335205078\n",
            "batch:278, loss:0.8639193773269653\n",
            "batch:279, loss:0.9340468049049377\n",
            "batch:280, loss:0.9103538990020752\n",
            "batch:281, loss:1.0290143489837646\n",
            "batch:282, loss:1.1040918827056885\n",
            "batch:283, loss:0.9259893894195557\n",
            "batch:284, loss:1.150700330734253\n",
            "batch:285, loss:0.9976696968078613\n",
            "batch:286, loss:1.0245956182479858\n",
            "batch:287, loss:0.9781970977783203\n",
            "batch:288, loss:0.9709538817405701\n",
            "batch:289, loss:0.9624162316322327\n",
            "batch:290, loss:0.9086658954620361\n",
            "batch:291, loss:0.9220131635665894\n",
            "batch:292, loss:1.1071761846542358\n",
            "batch:293, loss:0.9951992034912109\n",
            "batch:294, loss:1.0453171730041504\n",
            "batch:295, loss:0.9700947403907776\n",
            "batch:296, loss:0.9812923669815063\n",
            "batch:297, loss:0.8443306088447571\n",
            "batch:298, loss:1.0257091522216797\n",
            "batch:299, loss:0.92180997133255\n",
            "batch:300, loss:1.031323790550232\n",
            "batch:301, loss:0.9715271592140198\n",
            "batch:302, loss:1.0784144401550293\n",
            "batch:303, loss:0.8974742889404297\n",
            "batch:304, loss:0.9881243109703064\n",
            "batch:305, loss:0.8744966387748718\n",
            "batch:306, loss:1.0306466817855835\n",
            "batch:307, loss:1.001482605934143\n",
            "batch:308, loss:0.9778922200202942\n",
            "batch:309, loss:0.9148138165473938\n",
            "batch:310, loss:0.861985445022583\n",
            "batch:311, loss:0.9573820233345032\n",
            "batch:312, loss:1.0129903554916382\n",
            "batch:313, loss:1.0384464263916016\n",
            "batch:314, loss:0.8988887667655945\n",
            "batch:315, loss:0.9788470268249512\n",
            "batch:316, loss:0.9914155602455139\n",
            "batch:317, loss:0.8891151547431946\n",
            "batch:318, loss:0.8421124815940857\n",
            "batch:319, loss:1.0095983743667603\n",
            "batch:320, loss:0.9023210406303406\n",
            "batch:321, loss:1.0384455919265747\n",
            "batch:322, loss:0.8464905619621277\n",
            "batch:323, loss:0.9050508737564087\n",
            "batch:324, loss:0.8597528338432312\n",
            "batch:325, loss:1.0005767345428467\n",
            "batch:326, loss:0.8028961420059204\n",
            "batch:327, loss:1.0700253248214722\n",
            "batch:328, loss:0.9034140706062317\n",
            "batch:329, loss:1.0526052713394165\n",
            "batch:330, loss:0.9323996305465698\n",
            "batch:331, loss:0.7190316915512085\n",
            "batch:332, loss:0.9061477780342102\n",
            "batch:333, loss:0.9383627772331238\n",
            "batch:334, loss:0.7935499548912048\n",
            "batch:335, loss:0.8301109075546265\n",
            "batch:336, loss:0.9523021578788757\n",
            "batch:337, loss:1.0211318731307983\n",
            "batch:338, loss:0.8433555364608765\n",
            "batch:339, loss:0.7848140001296997\n",
            "batch:340, loss:0.8723663687705994\n",
            "batch:341, loss:0.7933780550956726\n",
            "batch:342, loss:0.7873713970184326\n",
            "batch:343, loss:0.96981281042099\n",
            "batch:344, loss:0.8194624185562134\n",
            "batch:345, loss:0.9096817970275879\n",
            "batch:346, loss:1.0181275606155396\n",
            "batch:347, loss:0.8567928671836853\n",
            "batch:348, loss:0.9397850632667542\n",
            "batch:349, loss:0.7980963587760925\n",
            "batch:350, loss:0.836601972579956\n",
            "batch:351, loss:0.8065356016159058\n",
            "batch:352, loss:0.7830615639686584\n",
            "batch:353, loss:0.8413993716239929\n",
            "batch:354, loss:0.8168015480041504\n",
            "batch:355, loss:0.9013144969940186\n",
            "batch:356, loss:0.8086021542549133\n",
            "batch:357, loss:1.017616868019104\n",
            "batch:358, loss:0.8533865809440613\n",
            "batch:359, loss:0.9138941764831543\n",
            "batch:360, loss:0.81352299451828\n",
            "batch:361, loss:0.867861807346344\n",
            "batch:362, loss:0.8142474889755249\n",
            "batch:363, loss:0.7950587868690491\n",
            "batch:364, loss:0.9438130855560303\n",
            "batch:365, loss:0.84843510389328\n",
            "batch:366, loss:0.9820589423179626\n",
            "batch:367, loss:0.8435500860214233\n",
            "batch:368, loss:0.9194377660751343\n",
            "batch:369, loss:0.7167849540710449\n",
            "batch:370, loss:0.9516265392303467\n",
            "batch:371, loss:0.7716405987739563\n",
            "batch:372, loss:0.9193195700645447\n",
            "batch:373, loss:0.9354747533798218\n",
            "batch:374, loss:0.8145610690116882\n",
            "batch:375, loss:0.8570785522460938\n",
            "batch:376, loss:0.8187587261199951\n",
            "batch:377, loss:0.7502798438072205\n",
            "batch:378, loss:0.7492271661758423\n",
            "batch:379, loss:0.9412727952003479\n",
            "batch:380, loss:0.8338159322738647\n",
            "batch:381, loss:0.9402385354042053\n",
            "batch:382, loss:0.8287283182144165\n",
            "batch:383, loss:0.8959277272224426\n",
            "batch:384, loss:0.8354538679122925\n",
            "batch:385, loss:0.8579855561256409\n",
            "batch:386, loss:0.8460618853569031\n",
            "batch:387, loss:0.8218604922294617\n",
            "batch:388, loss:0.7780641317367554\n",
            "batch:389, loss:0.8712314963340759\n",
            "batch:390, loss:0.8644263744354248\n",
            "batch:391, loss:0.9746073484420776\n",
            "batch:392, loss:0.6903963685035706\n",
            "batch:393, loss:0.815913736820221\n",
            "batch:394, loss:1.0024220943450928\n",
            "batch:395, loss:0.979579746723175\n",
            "batch:396, loss:0.7999640703201294\n",
            "batch:397, loss:0.8245099186897278\n",
            "batch:398, loss:0.7921676635742188\n",
            "batch:399, loss:0.8056285381317139\n",
            "batch:400, loss:0.8696582317352295\n",
            "batch:401, loss:1.008313775062561\n",
            "batch:402, loss:0.821925938129425\n",
            "batch:403, loss:0.7236857414245605\n",
            "batch:404, loss:0.8378642201423645\n",
            "batch:405, loss:0.8451458215713501\n",
            "batch:406, loss:0.8220269083976746\n",
            "batch:407, loss:0.8544408082962036\n",
            "batch:408, loss:0.8947476148605347\n",
            "batch:409, loss:0.8523430824279785\n",
            "batch:410, loss:0.686777651309967\n",
            "batch:411, loss:0.7630425691604614\n",
            "batch:412, loss:0.7564067244529724\n",
            "batch:413, loss:0.8719900846481323\n",
            "batch:414, loss:0.7124180793762207\n",
            "batch:415, loss:0.6385090351104736\n",
            "batch:416, loss:0.905329704284668\n",
            "batch:417, loss:0.7619565725326538\n",
            "batch:418, loss:0.7236204743385315\n",
            "batch:419, loss:0.6218743920326233\n",
            "batch:420, loss:0.8735451102256775\n",
            "batch:421, loss:0.8710263967514038\n",
            "batch:422, loss:0.6975619196891785\n",
            "batch:423, loss:0.7696322798728943\n",
            "batch:424, loss:0.708920419216156\n",
            "batch:425, loss:0.957980751991272\n",
            "batch:426, loss:0.8502087593078613\n",
            "batch:427, loss:0.6800288558006287\n",
            "batch:428, loss:0.6577925682067871\n",
            "batch:429, loss:0.85988450050354\n",
            "batch:430, loss:0.6876816749572754\n",
            "batch:431, loss:0.755408525466919\n",
            "batch:432, loss:0.8221731781959534\n",
            "batch:433, loss:0.6499769687652588\n",
            "batch:434, loss:0.7069469094276428\n",
            "batch:435, loss:0.9539620876312256\n",
            "batch:436, loss:0.9023582935333252\n",
            "batch:437, loss:0.7716598510742188\n",
            "batch:438, loss:0.777564525604248\n",
            "batch:439, loss:0.8212223649024963\n",
            "batch:440, loss:0.6453914046287537\n",
            "batch:441, loss:0.7416077852249146\n",
            "batch:442, loss:1.0310521125793457\n",
            "batch:443, loss:0.64264315366745\n",
            "batch:444, loss:0.9429836869239807\n",
            "batch:445, loss:0.6142268180847168\n",
            "batch:446, loss:0.8133031725883484\n",
            "batch:447, loss:0.7290821075439453\n",
            "batch:448, loss:0.748490571975708\n",
            "batch:449, loss:1.0006179809570312\n",
            "batch:450, loss:0.7622305750846863\n",
            "batch:451, loss:0.8721713423728943\n",
            "batch:452, loss:0.8406214714050293\n",
            "batch:453, loss:0.8123415112495422\n",
            "batch:454, loss:0.7909340858459473\n",
            "batch:455, loss:0.7534286975860596\n",
            "batch:456, loss:0.8640386462211609\n",
            "batch:457, loss:0.7669526934623718\n",
            "batch:458, loss:0.697235643863678\n",
            "batch:459, loss:0.7151793837547302\n",
            "batch:460, loss:0.714285135269165\n",
            "batch:461, loss:0.7664349675178528\n",
            "batch:462, loss:0.8185322284698486\n",
            "batch:463, loss:0.7587229609489441\n",
            "batch:464, loss:0.7474848628044128\n",
            "batch:465, loss:0.7164430618286133\n",
            "batch:466, loss:0.7266503572463989\n",
            "batch:467, loss:0.8036649823188782\n",
            "batch:468, loss:0.7027947306632996\n",
            "batch:469, loss:0.6876162886619568\n",
            "batch:470, loss:0.6561566591262817\n",
            "batch:471, loss:0.7451842427253723\n",
            "batch:472, loss:0.6233623027801514\n",
            "batch:473, loss:1.024216651916504\n",
            "batch:474, loss:0.7085868716239929\n",
            "batch:475, loss:0.7588778734207153\n",
            "batch:476, loss:0.7437729835510254\n",
            "batch:477, loss:0.8223629593849182\n",
            "batch:478, loss:0.8037599921226501\n",
            "batch:479, loss:0.7843420505523682\n",
            "batch:480, loss:0.7188528180122375\n",
            "batch:481, loss:0.9432944655418396\n",
            "batch:482, loss:0.6095778942108154\n",
            "batch:483, loss:0.809231698513031\n",
            "batch:484, loss:0.9115051627159119\n",
            "batch:485, loss:0.9219542145729065\n",
            "batch:486, loss:0.8695440292358398\n",
            "batch:487, loss:0.7691060304641724\n",
            "batch:488, loss:0.8392326235771179\n",
            "batch:489, loss:0.9045084714889526\n",
            "batch:490, loss:0.6989932656288147\n",
            "batch:491, loss:0.7305490970611572\n",
            "batch:492, loss:0.6524064540863037\n",
            "batch:493, loss:0.7411080598831177\n",
            "batch:494, loss:0.7518683075904846\n",
            "batch:495, loss:0.6646929979324341\n",
            "batch:496, loss:0.9094655513763428\n",
            "batch:497, loss:0.9493762850761414\n",
            "batch:498, loss:0.5900125503540039\n",
            "batch:499, loss:0.7292956113815308\n",
            "batch:500, loss:0.8917055130004883\n",
            "batch:501, loss:0.7111058831214905\n",
            "batch:502, loss:0.9055768847465515\n",
            "batch:503, loss:0.7318176627159119\n",
            "batch:504, loss:0.7245761156082153\n",
            "batch:505, loss:0.6376231908798218\n",
            "batch:506, loss:0.6962372660636902\n",
            "batch:507, loss:0.6461793780326843\n",
            "batch:508, loss:0.7605994343757629\n",
            "batch:509, loss:0.6399065256118774\n",
            "batch:510, loss:0.5486250519752502\n",
            "batch:511, loss:0.6086750626564026\n",
            "batch:512, loss:0.7511720657348633\n",
            "batch:513, loss:0.7772237062454224\n",
            "batch:514, loss:0.8357182741165161\n",
            "batch:515, loss:0.6972845792770386\n",
            "batch:516, loss:0.6551563739776611\n",
            "batch:517, loss:0.885707676410675\n",
            "batch:518, loss:0.6553475260734558\n",
            "batch:519, loss:0.7293595671653748\n",
            "batch:520, loss:0.8831605911254883\n",
            "batch:521, loss:0.9499047994613647\n",
            "batch:522, loss:0.7569442987442017\n",
            "batch:523, loss:0.6278385519981384\n",
            "batch:524, loss:0.5940654277801514\n",
            "batch:525, loss:0.8703826665878296\n",
            "batch:526, loss:0.7622965574264526\n",
            "batch:527, loss:0.8508467078208923\n",
            "batch:528, loss:0.6475473046302795\n",
            "batch:529, loss:0.7453390955924988\n",
            "batch:530, loss:0.8192614912986755\n",
            "batch:531, loss:0.8034867644309998\n",
            "batch:532, loss:0.7156414985656738\n",
            "batch:533, loss:0.9004601836204529\n",
            "batch:534, loss:0.6994414329528809\n",
            "batch:535, loss:0.6783607006072998\n",
            "batch:536, loss:0.5105931758880615\n",
            "batch:537, loss:0.6916530132293701\n",
            "batch:538, loss:0.7968937754631042\n",
            "batch:539, loss:0.6853781938552856\n",
            "batch:540, loss:0.5497510433197021\n",
            "batch:541, loss:0.656327486038208\n",
            "batch:542, loss:0.8119845390319824\n",
            "batch:543, loss:0.7034465670585632\n",
            "batch:544, loss:0.7863984704017639\n",
            "batch:545, loss:0.6656404137611389\n",
            "batch:546, loss:0.5760912299156189\n",
            "batch:547, loss:0.5988243222236633\n",
            "batch:548, loss:0.6492378115653992\n",
            "batch:549, loss:0.9244956970214844\n",
            "batch:550, loss:0.7666069269180298\n",
            "batch:551, loss:0.6802918314933777\n",
            "batch:552, loss:0.8263715505599976\n",
            "batch:553, loss:0.6526376605033875\n",
            "batch:554, loss:0.7449937462806702\n",
            "batch:555, loss:0.7429196238517761\n",
            "batch:556, loss:0.7299710512161255\n",
            "batch:557, loss:0.760004997253418\n",
            "batch:558, loss:0.738767683506012\n",
            "batch:559, loss:0.6839049458503723\n",
            "batch:560, loss:0.7147824764251709\n",
            "batch:561, loss:0.6360207200050354\n",
            "batch:562, loss:0.746308445930481\n",
            "batch:563, loss:0.749696671962738\n",
            "batch:564, loss:0.7229093909263611\n",
            "batch:565, loss:0.7618407607078552\n",
            "batch:566, loss:0.7319571375846863\n",
            "batch:567, loss:0.8861110210418701\n",
            "batch:568, loss:0.7592527866363525\n",
            "batch:569, loss:0.8594906330108643\n",
            "batch:570, loss:0.6393920183181763\n",
            "batch:571, loss:0.7068223357200623\n",
            "batch:572, loss:0.7009856700897217\n",
            "batch:573, loss:0.7143883109092712\n",
            "batch:574, loss:0.7381808161735535\n",
            "batch:575, loss:0.9086108803749084\n",
            "batch:576, loss:0.6745588779449463\n",
            "batch:577, loss:0.7607182860374451\n",
            "batch:578, loss:0.6843788027763367\n",
            "batch:579, loss:0.5348325371742249\n",
            "batch:580, loss:0.8006631135940552\n",
            "batch:581, loss:0.8183212280273438\n",
            "batch:582, loss:0.5916959047317505\n",
            "batch:583, loss:0.6697779893875122\n",
            "batch:584, loss:0.6426995992660522\n",
            "batch:585, loss:0.6925416588783264\n",
            "batch:586, loss:0.688721239566803\n",
            "batch:587, loss:0.6599916219711304\n",
            "batch:588, loss:0.6264340877532959\n",
            "batch:589, loss:0.8691897988319397\n",
            "batch:590, loss:0.5763455629348755\n",
            "batch:591, loss:0.8746542930603027\n",
            "batch:592, loss:0.6220310926437378\n",
            "batch:593, loss:0.4921497702598572\n",
            "batch:594, loss:0.7196513414382935\n",
            "batch:595, loss:0.8423633575439453\n",
            "batch:596, loss:0.6069219708442688\n",
            "batch:597, loss:0.6070460081100464\n",
            "batch:598, loss:0.7218278050422668\n",
            "batch:599, loss:0.6278942823410034\n",
            "batch:600, loss:0.7477478981018066\n",
            "batch:601, loss:0.5905880331993103\n",
            "batch:602, loss:0.5792530179023743\n",
            "batch:603, loss:0.696749210357666\n",
            "batch:604, loss:0.8390598297119141\n",
            "batch:605, loss:0.5644471049308777\n",
            "batch:606, loss:0.5655978918075562\n",
            "batch:607, loss:0.6638390421867371\n",
            "batch:608, loss:0.6166312098503113\n",
            "batch:609, loss:0.7301591634750366\n",
            "batch:610, loss:0.6034293174743652\n",
            "batch:611, loss:0.706328809261322\n",
            "batch:612, loss:0.6729917526245117\n",
            "batch:613, loss:0.5959288477897644\n",
            "batch:614, loss:0.5431894063949585\n",
            "batch:615, loss:0.645757794380188\n",
            "batch:616, loss:0.8167617321014404\n",
            "batch:617, loss:0.802730143070221\n",
            "batch:618, loss:0.8363057971000671\n",
            "batch:619, loss:0.6841832399368286\n",
            "batch:620, loss:0.7409456968307495\n",
            "batch:621, loss:0.5572080612182617\n",
            "batch:622, loss:0.6593300104141235\n",
            "batch:623, loss:0.795583963394165\n",
            "batch:624, loss:0.6263511776924133\n",
            "batch:625, loss:0.6702433824539185\n",
            "batch:626, loss:0.581233024597168\n",
            "batch:627, loss:0.6089920997619629\n",
            "batch:628, loss:0.601514995098114\n",
            "batch:629, loss:0.7224923372268677\n",
            "batch:630, loss:0.7291979193687439\n",
            "batch:631, loss:0.578123152256012\n",
            "batch:632, loss:0.6445505023002625\n",
            "batch:633, loss:0.6345677375793457\n",
            "batch:634, loss:0.7515628933906555\n",
            "batch:635, loss:0.767900288105011\n",
            "batch:636, loss:0.6441702246665955\n",
            "batch:637, loss:0.6960898637771606\n",
            "batch:638, loss:0.6463536024093628\n",
            "batch:639, loss:0.7103933691978455\n",
            "batch:640, loss:0.7670366764068604\n",
            "batch:641, loss:0.7376696467399597\n",
            "batch:642, loss:0.6627037525177002\n",
            "batch:643, loss:0.7628393173217773\n",
            "batch:644, loss:0.6822605133056641\n",
            "batch:645, loss:0.764560878276825\n",
            "batch:646, loss:0.8093379139900208\n",
            "batch:647, loss:0.7715760469436646\n",
            "batch:648, loss:0.6206375360488892\n",
            "batch:649, loss:0.6242222785949707\n",
            "batch:650, loss:0.6169900894165039\n",
            "batch:651, loss:0.6791025400161743\n",
            "batch:652, loss:0.8996403217315674\n",
            "batch:653, loss:0.9351795315742493\n",
            "batch:654, loss:0.7237681150436401\n",
            "batch:655, loss:0.6139572262763977\n",
            "batch:656, loss:0.7969054579734802\n",
            "batch:657, loss:0.7169980406761169\n",
            "batch:658, loss:0.8346990346908569\n",
            "batch:659, loss:0.8385842442512512\n",
            "batch:660, loss:0.690051257610321\n",
            "batch:661, loss:0.5257539749145508\n",
            "batch:662, loss:0.660486102104187\n",
            "batch:663, loss:0.6547372937202454\n",
            "batch:664, loss:0.6994713544845581\n",
            "batch:665, loss:0.6079408526420593\n",
            "batch:666, loss:0.6338478326797485\n",
            "batch:667, loss:0.6278578639030457\n",
            "batch:668, loss:0.7325486540794373\n",
            "batch:669, loss:0.7443001866340637\n",
            "batch:670, loss:0.570955216884613\n",
            "batch:671, loss:0.6993566155433655\n",
            "batch:672, loss:0.5940546989440918\n",
            "batch:673, loss:0.6609767079353333\n",
            "batch:674, loss:0.7274627685546875\n",
            "batch:675, loss:0.7595685720443726\n",
            "batch:676, loss:0.7219184637069702\n",
            "batch:677, loss:0.7027735710144043\n",
            "batch:678, loss:0.8414350152015686\n",
            "batch:679, loss:0.713499903678894\n",
            "batch:680, loss:0.7225098609924316\n",
            "batch:681, loss:0.8063880801200867\n",
            "batch:682, loss:0.5072847008705139\n",
            "batch:683, loss:0.6596932411193848\n",
            "batch:684, loss:0.7605757117271423\n",
            "batch:685, loss:0.8141723275184631\n",
            "batch:686, loss:0.7441927790641785\n",
            "batch:687, loss:0.6967683434486389\n",
            "batch:688, loss:0.8732810020446777\n",
            "batch:689, loss:0.6614017486572266\n",
            "batch:690, loss:0.5966011881828308\n",
            "batch:691, loss:0.5783404111862183\n",
            "batch:692, loss:0.608776867389679\n",
            "batch:693, loss:0.704170286655426\n",
            "batch:694, loss:0.7105695605278015\n",
            "batch:695, loss:0.7039583325386047\n",
            "batch:696, loss:0.6004263758659363\n",
            "batch:697, loss:0.6791659593582153\n",
            "batch:698, loss:0.5627185702323914\n",
            "batch:699, loss:0.6062299609184265\n",
            "batch:700, loss:0.7108470797538757\n",
            "batch:701, loss:0.6133620142936707\n",
            "batch:702, loss:0.5731286406517029\n",
            "batch:703, loss:0.7120789289474487\n",
            "batch:704, loss:0.6691102385520935\n",
            "batch:705, loss:0.7390308976173401\n",
            "batch:706, loss:0.746967077255249\n",
            "batch:707, loss:0.49962443113327026\n",
            "batch:708, loss:0.4330023229122162\n",
            "batch:709, loss:0.6791303753852844\n",
            "batch:710, loss:0.6365832090377808\n",
            "batch:711, loss:0.7446313500404358\n",
            "batch:712, loss:0.5448007583618164\n",
            "batch:713, loss:0.8494369387626648\n",
            "batch:714, loss:0.5621843934059143\n",
            "batch:715, loss:0.7278045415878296\n",
            "batch:716, loss:0.6596109867095947\n",
            "batch:717, loss:0.7067063450813293\n",
            "batch:718, loss:0.5991939902305603\n",
            "batch:719, loss:0.6555167436599731\n",
            "batch:720, loss:0.6998389959335327\n",
            "batch:721, loss:0.9175878763198853\n",
            "batch:722, loss:0.6742823123931885\n",
            "batch:723, loss:0.5369758009910583\n",
            "batch:724, loss:0.5198394656181335\n",
            "batch:725, loss:0.6928083896636963\n",
            "batch:726, loss:0.5085330009460449\n",
            "batch:727, loss:0.5330448746681213\n",
            "batch:728, loss:0.606035053730011\n",
            "batch:729, loss:0.795789361000061\n",
            "batch:730, loss:0.6359867453575134\n",
            "batch:731, loss:0.61832195520401\n",
            "batch:732, loss:0.6779637932777405\n",
            "batch:733, loss:0.6540175080299377\n",
            "batch:734, loss:0.662041425704956\n",
            "batch:735, loss:0.7819855213165283\n",
            "batch:736, loss:0.790953516960144\n",
            "batch:737, loss:0.7292911410331726\n",
            "batch:738, loss:0.586837649345398\n",
            "batch:739, loss:0.7399391531944275\n",
            "batch:740, loss:0.6375719904899597\n",
            "batch:741, loss:0.7020020484924316\n",
            "batch:742, loss:0.8851290345191956\n",
            "batch:743, loss:0.6075869798660278\n",
            "batch:744, loss:0.5651993155479431\n",
            "batch:745, loss:0.643153190612793\n",
            "batch:746, loss:0.8169606924057007\n",
            "batch:747, loss:0.4936267137527466\n",
            "batch:748, loss:0.6529186964035034\n",
            "batch:749, loss:0.7056780457496643\n",
            "batch:750, loss:0.5667251944541931\n",
            "batch:751, loss:0.6627215147018433\n",
            "batch:752, loss:0.7234948873519897\n",
            "batch:753, loss:0.7780356407165527\n",
            "batch:754, loss:0.6112712621688843\n",
            "batch:755, loss:0.5461321473121643\n",
            "batch:756, loss:0.5463974475860596\n",
            "batch:757, loss:0.5439186096191406\n",
            "batch:758, loss:0.7420536279678345\n",
            "batch:759, loss:0.8565472960472107\n",
            "batch:760, loss:0.5648456811904907\n",
            "batch:761, loss:0.7242218255996704\n",
            "batch:762, loss:0.4457944631576538\n",
            "batch:763, loss:0.7598641514778137\n",
            "batch:764, loss:0.5873723030090332\n",
            "batch:765, loss:0.642180323600769\n",
            "batch:766, loss:0.4973856508731842\n",
            "batch:767, loss:0.594163715839386\n",
            "batch:768, loss:0.6530301570892334\n",
            "batch:769, loss:0.5186834335327148\n",
            "batch:770, loss:0.5561070442199707\n",
            "batch:771, loss:0.6307617425918579\n",
            "batch:772, loss:0.6522845029830933\n",
            "batch:773, loss:0.6012808680534363\n",
            "batch:774, loss:0.6504566669464111\n",
            "batch:775, loss:0.6418780088424683\n",
            "batch:776, loss:0.7317736148834229\n",
            "batch:777, loss:0.6317461729049683\n",
            "batch:778, loss:0.5804013609886169\n",
            "batch:779, loss:0.5857151746749878\n",
            "batch:780, loss:0.711887001991272\n",
            "batch:781, loss:0.5815554261207581\n",
            "batch:782, loss:0.6674370765686035\n",
            "batch:783, loss:0.4903400242328644\n",
            "batch:784, loss:0.6573659777641296\n",
            "batch:785, loss:0.5734623670578003\n",
            "batch:786, loss:0.6519448757171631\n",
            "batch:787, loss:0.6992300748825073\n",
            "batch:788, loss:0.5815191268920898\n",
            "batch:789, loss:0.7863885164260864\n",
            "batch:790, loss:0.5039893388748169\n",
            "batch:791, loss:0.637828528881073\n",
            "batch:792, loss:0.5277212262153625\n",
            "batch:793, loss:0.7077639102935791\n",
            "batch:794, loss:0.5875266790390015\n",
            "batch:795, loss:0.4449573755264282\n",
            "batch:796, loss:0.6045602560043335\n",
            "batch:797, loss:0.4870290160179138\n",
            "batch:798, loss:0.601201057434082\n",
            "batch:799, loss:0.70818692445755\n",
            "batch:800, loss:0.5501648187637329\n",
            "batch:801, loss:0.8017626404762268\n",
            "batch:802, loss:0.7492866516113281\n",
            "batch:803, loss:0.7418342232704163\n",
            "batch:804, loss:0.5900188088417053\n",
            "batch:805, loss:0.5484856963157654\n",
            "batch:806, loss:0.6147404909133911\n",
            "batch:807, loss:0.6540844440460205\n",
            "batch:808, loss:0.7327963709831238\n",
            "batch:809, loss:0.5103501081466675\n",
            "batch:810, loss:0.8667293190956116\n",
            "batch:811, loss:0.6219337582588196\n",
            "batch:812, loss:0.7742714881896973\n",
            "batch:813, loss:0.4966507852077484\n",
            "batch:814, loss:0.654079020023346\n",
            "batch:815, loss:0.778545618057251\n",
            "batch:816, loss:0.5346110463142395\n",
            "batch:817, loss:0.640964686870575\n",
            "batch:818, loss:0.465219110250473\n",
            "batch:819, loss:0.576029896736145\n",
            "batch:820, loss:0.710536003112793\n",
            "batch:821, loss:0.6724981665611267\n",
            "batch:822, loss:0.4503422677516937\n",
            "batch:823, loss:0.7158914804458618\n",
            "batch:824, loss:0.6532310247421265\n",
            "batch:825, loss:0.6108523011207581\n",
            "batch:826, loss:0.6557115316390991\n",
            "batch:827, loss:0.4444047808647156\n",
            "batch:828, loss:0.7303073406219482\n",
            "batch:829, loss:0.5679183006286621\n",
            "batch:830, loss:0.7941261529922485\n",
            "batch:831, loss:0.7880492806434631\n",
            "batch:832, loss:0.6714304685592651\n",
            "batch:833, loss:0.6314257383346558\n",
            "batch:834, loss:0.520862877368927\n",
            "batch:835, loss:0.600486695766449\n",
            "batch:836, loss:0.5241124033927917\n",
            "batch:837, loss:0.6494101285934448\n",
            "batch:838, loss:0.5850082635879517\n",
            "batch:839, loss:0.7026344537734985\n",
            "batch:840, loss:0.6248612999916077\n",
            "batch:841, loss:0.6224510073661804\n",
            "batch:842, loss:0.7425289154052734\n",
            "batch:843, loss:0.559794545173645\n",
            "batch:844, loss:0.6531394720077515\n",
            "batch:845, loss:0.581684410572052\n",
            "batch:846, loss:0.7060238718986511\n",
            "batch:847, loss:0.6265017986297607\n",
            "batch:848, loss:0.5346806645393372\n",
            "batch:849, loss:0.6422976851463318\n",
            "batch:850, loss:0.6553049683570862\n",
            "batch:851, loss:0.8034654259681702\n",
            "batch:852, loss:0.7234256267547607\n",
            "batch:853, loss:0.4335469901561737\n",
            "batch:854, loss:0.7045259475708008\n",
            "batch:855, loss:0.5153584480285645\n",
            "batch:856, loss:0.6894383430480957\n",
            "batch:857, loss:0.7014239430427551\n",
            "batch:858, loss:0.4579143226146698\n",
            "batch:859, loss:0.4808395802974701\n",
            "batch:860, loss:0.6943961977958679\n",
            "batch:861, loss:0.7178170680999756\n",
            "batch:862, loss:0.732236921787262\n",
            "batch:863, loss:0.6357458233833313\n",
            "batch:864, loss:0.5959268808364868\n",
            "batch:865, loss:0.5891640782356262\n",
            "batch:866, loss:0.6948224902153015\n",
            "batch:867, loss:0.5384396910667419\n",
            "batch:868, loss:0.7787744402885437\n",
            "batch:869, loss:0.681394100189209\n",
            "batch:870, loss:0.537460207939148\n",
            "batch:871, loss:0.5882447957992554\n",
            "batch:872, loss:0.5431686639785767\n",
            "batch:873, loss:0.47140172123908997\n",
            "batch:874, loss:0.6038120985031128\n",
            "batch:875, loss:0.5997838973999023\n",
            "batch:876, loss:0.6978808641433716\n",
            "batch:877, loss:0.5951569080352783\n",
            "batch:878, loss:0.5383782386779785\n",
            "batch:879, loss:0.5749527215957642\n",
            "batch:880, loss:0.5655680894851685\n",
            "batch:881, loss:0.6115674376487732\n",
            "batch:882, loss:0.7410445213317871\n",
            "batch:883, loss:0.5708023309707642\n",
            "batch:884, loss:0.7692174911499023\n",
            "batch:885, loss:0.7127276659011841\n",
            "batch:886, loss:0.7047935724258423\n",
            "batch:887, loss:0.8208912014961243\n",
            "batch:888, loss:0.5910651683807373\n",
            "batch:889, loss:0.5925623774528503\n",
            "batch:890, loss:0.5608049631118774\n",
            "batch:891, loss:0.4928632974624634\n",
            "batch:892, loss:0.7502812743186951\n",
            "batch:893, loss:0.5671312212944031\n",
            "batch:894, loss:0.5789173245429993\n",
            "batch:895, loss:0.4581032693386078\n",
            "batch:896, loss:0.5306779146194458\n",
            "batch:897, loss:0.8519716858863831\n",
            "batch:898, loss:0.6131786704063416\n",
            "batch:899, loss:0.623596727848053\n",
            "batch:900, loss:0.49338018894195557\n",
            "batch:901, loss:0.6859598755836487\n",
            "batch:902, loss:0.8164395093917847\n",
            "batch:903, loss:0.4315987527370453\n",
            "batch:904, loss:0.8421529531478882\n",
            "batch:905, loss:0.62263023853302\n",
            "batch:906, loss:0.6078515648841858\n",
            "batch:907, loss:0.4633856415748596\n",
            "batch:908, loss:0.8858580589294434\n",
            "batch:909, loss:0.7519479393959045\n",
            "batch:910, loss:0.7246900796890259\n",
            "batch:911, loss:0.5361359715461731\n",
            "batch:912, loss:0.5157343745231628\n",
            "batch:913, loss:0.5323681235313416\n",
            "batch:914, loss:0.4860877990722656\n",
            "batch:915, loss:0.7857773303985596\n",
            "batch:916, loss:0.5573807954788208\n",
            "batch:917, loss:0.7211037278175354\n",
            "batch:918, loss:0.5109787583351135\n",
            "batch:919, loss:0.6325426697731018\n",
            "batch:920, loss:0.7359611392021179\n",
            "batch:921, loss:0.6473660469055176\n",
            "batch:922, loss:0.6046074628829956\n",
            "batch:923, loss:0.5667253732681274\n",
            "batch:924, loss:0.551591694355011\n",
            "batch:925, loss:0.726132333278656\n",
            "batch:926, loss:0.6372841596603394\n",
            "batch:927, loss:0.5113666653633118\n",
            "batch:928, loss:0.5263793468475342\n",
            "batch:929, loss:0.7144997119903564\n",
            "batch:930, loss:0.8634676933288574\n",
            "batch:931, loss:0.4328876733779907\n",
            "batch:932, loss:0.5267865657806396\n",
            "batch:933, loss:0.5840389132499695\n",
            "batch:934, loss:0.37605488300323486\n",
            "batch:935, loss:0.6382399201393127\n",
            "batch:936, loss:0.6882227063179016\n",
            "batch:937, loss:0.59434574842453\n",
            "Training loss: 1.0015089905846601\n",
            "batch:0, loss:0.6017816066741943\n",
            "batch:1, loss:0.579063355922699\n",
            "batch:2, loss:0.6356995701789856\n",
            "batch:3, loss:0.7676436305046082\n",
            "batch:4, loss:0.5076989531517029\n",
            "batch:5, loss:0.3882824182510376\n",
            "batch:6, loss:0.5519481897354126\n",
            "batch:7, loss:0.6152901649475098\n",
            "batch:8, loss:0.6190723776817322\n",
            "batch:9, loss:0.6636533737182617\n",
            "batch:10, loss:0.4851036071777344\n",
            "batch:11, loss:0.5815305113792419\n",
            "batch:12, loss:0.6225743889808655\n",
            "batch:13, loss:0.5602596998214722\n",
            "batch:14, loss:0.5830619931221008\n",
            "batch:15, loss:0.49796006083488464\n",
            "batch:16, loss:0.5523974895477295\n",
            "batch:17, loss:0.5057187080383301\n",
            "batch:18, loss:0.6323476433753967\n",
            "batch:19, loss:0.6264674067497253\n",
            "batch:20, loss:0.6668411493301392\n",
            "batch:21, loss:0.7298678159713745\n",
            "batch:22, loss:0.5582124590873718\n",
            "batch:23, loss:0.536013126373291\n",
            "batch:24, loss:0.6300154328346252\n",
            "batch:25, loss:0.6019022464752197\n",
            "batch:26, loss:0.5237587690353394\n",
            "batch:27, loss:0.7132761478424072\n",
            "batch:28, loss:0.6061760783195496\n",
            "batch:29, loss:0.7185917496681213\n",
            "batch:30, loss:0.8002157211303711\n",
            "batch:31, loss:0.5312625169754028\n",
            "batch:32, loss:0.597227156162262\n",
            "batch:33, loss:0.6035157442092896\n",
            "batch:34, loss:0.47907698154449463\n",
            "batch:35, loss:0.6754329204559326\n",
            "batch:36, loss:0.5766326189041138\n",
            "batch:37, loss:0.44945719838142395\n",
            "batch:38, loss:0.6807773113250732\n",
            "batch:39, loss:0.8067399859428406\n",
            "batch:40, loss:0.5304393172264099\n",
            "batch:41, loss:0.6395435929298401\n",
            "batch:42, loss:0.576107919216156\n",
            "batch:43, loss:0.5046405792236328\n",
            "batch:44, loss:0.5178534984588623\n",
            "batch:45, loss:0.5986268520355225\n",
            "batch:46, loss:0.6239764094352722\n",
            "batch:47, loss:0.5762988924980164\n",
            "batch:48, loss:0.7305024862289429\n",
            "batch:49, loss:0.40343114733695984\n",
            "batch:50, loss:0.5487620830535889\n",
            "batch:51, loss:0.5849626064300537\n",
            "batch:52, loss:0.48089221119880676\n",
            "batch:53, loss:0.7273172736167908\n",
            "batch:54, loss:0.5310890078544617\n",
            "batch:55, loss:0.5102176070213318\n",
            "batch:56, loss:0.4123082756996155\n",
            "batch:57, loss:0.6648299098014832\n",
            "batch:58, loss:0.5698386430740356\n",
            "batch:59, loss:0.6151270270347595\n",
            "batch:60, loss:0.5305895209312439\n",
            "batch:61, loss:0.4475047290325165\n",
            "batch:62, loss:0.5421750545501709\n",
            "batch:63, loss:0.5563218593597412\n",
            "batch:64, loss:0.5761839747428894\n",
            "batch:65, loss:0.5456848740577698\n",
            "batch:66, loss:0.6098248362541199\n",
            "batch:67, loss:0.7151801586151123\n",
            "batch:68, loss:0.45918208360671997\n",
            "batch:69, loss:0.4819948971271515\n",
            "batch:70, loss:0.5269090533256531\n",
            "batch:71, loss:0.4446253776550293\n",
            "batch:72, loss:0.6899744272232056\n",
            "batch:73, loss:0.6375197768211365\n",
            "batch:74, loss:0.5054680705070496\n",
            "batch:75, loss:0.6488249897956848\n",
            "batch:76, loss:0.5429620742797852\n",
            "batch:77, loss:0.582181990146637\n",
            "batch:78, loss:0.6390547156333923\n",
            "batch:79, loss:0.74051433801651\n",
            "batch:80, loss:0.5732637643814087\n",
            "batch:81, loss:0.7164610028266907\n",
            "batch:82, loss:0.5220248699188232\n",
            "batch:83, loss:0.4894791841506958\n",
            "batch:84, loss:0.4893748164176941\n",
            "batch:85, loss:0.5471649765968323\n",
            "batch:86, loss:0.6095699667930603\n",
            "batch:87, loss:0.5072382688522339\n",
            "batch:88, loss:0.6404476165771484\n",
            "batch:89, loss:0.7203065752983093\n",
            "batch:90, loss:0.5773127675056458\n",
            "batch:91, loss:0.6519421935081482\n",
            "batch:92, loss:0.6087566018104553\n",
            "batch:93, loss:0.7101261019706726\n",
            "batch:94, loss:0.6915853023529053\n",
            "batch:95, loss:0.4978172481060028\n",
            "batch:96, loss:0.7313530445098877\n",
            "batch:97, loss:0.5213942527770996\n",
            "batch:98, loss:0.6925981044769287\n",
            "batch:99, loss:0.6889364123344421\n",
            "batch:100, loss:0.6817569732666016\n",
            "batch:101, loss:0.5621234774589539\n",
            "batch:102, loss:0.5455625653266907\n",
            "batch:103, loss:0.6369698643684387\n",
            "batch:104, loss:0.6551755666732788\n",
            "batch:105, loss:0.5602397322654724\n",
            "batch:106, loss:0.6491014957427979\n",
            "batch:107, loss:0.4691726565361023\n",
            "batch:108, loss:0.6274411082267761\n",
            "batch:109, loss:0.46416378021240234\n",
            "batch:110, loss:0.5242041349411011\n",
            "batch:111, loss:0.5331563353538513\n",
            "batch:112, loss:0.6674072742462158\n",
            "batch:113, loss:0.7096263766288757\n",
            "batch:114, loss:0.5338875651359558\n",
            "batch:115, loss:0.7232044339179993\n",
            "batch:116, loss:0.5411373972892761\n",
            "batch:117, loss:0.620181143283844\n",
            "batch:118, loss:0.5521703362464905\n",
            "batch:119, loss:0.6878113746643066\n",
            "batch:120, loss:0.66590416431427\n",
            "batch:121, loss:0.6604909300804138\n",
            "batch:122, loss:0.6483600735664368\n",
            "batch:123, loss:0.515720009803772\n",
            "batch:124, loss:0.597623884677887\n",
            "batch:125, loss:0.5207114219665527\n",
            "batch:126, loss:0.5840105414390564\n",
            "batch:127, loss:0.4423659145832062\n",
            "batch:128, loss:0.43475106358528137\n",
            "batch:129, loss:0.6784138083457947\n",
            "batch:130, loss:0.6123791933059692\n",
            "batch:131, loss:0.5374091863632202\n",
            "batch:132, loss:0.774267315864563\n",
            "batch:133, loss:0.5561145544052124\n",
            "batch:134, loss:0.632818341255188\n",
            "batch:135, loss:0.5478852391242981\n",
            "batch:136, loss:0.5379999876022339\n",
            "batch:137, loss:0.5893649458885193\n",
            "batch:138, loss:0.5151076912879944\n",
            "batch:139, loss:0.5389848351478577\n",
            "batch:140, loss:0.4403230547904968\n",
            "batch:141, loss:0.5313317775726318\n",
            "batch:142, loss:0.5073240995407104\n",
            "batch:143, loss:0.49573275446891785\n",
            "batch:144, loss:0.5850260257720947\n",
            "batch:145, loss:0.7578415870666504\n",
            "batch:146, loss:0.6533743739128113\n",
            "batch:147, loss:0.7390229105949402\n",
            "batch:148, loss:0.6989544034004211\n",
            "batch:149, loss:0.554485023021698\n",
            "batch:150, loss:0.5370280146598816\n",
            "batch:151, loss:0.6933151483535767\n",
            "batch:152, loss:0.48085492849349976\n",
            "batch:153, loss:0.6105178594589233\n",
            "batch:154, loss:0.5350645184516907\n",
            "batch:155, loss:0.5001521706581116\n",
            "batch:156, loss:0.6977996230125427\n",
            "batch:157, loss:0.639180064201355\n",
            "batch:158, loss:0.6288575530052185\n",
            "batch:159, loss:0.6016344428062439\n",
            "batch:160, loss:0.586104691028595\n",
            "batch:161, loss:0.48602405190467834\n",
            "batch:162, loss:0.6890215277671814\n",
            "batch:163, loss:0.6286383271217346\n",
            "batch:164, loss:0.4749022126197815\n",
            "batch:165, loss:0.5124237537384033\n",
            "batch:166, loss:0.6606214046478271\n",
            "batch:167, loss:0.5376150608062744\n",
            "batch:168, loss:0.4883868396282196\n",
            "batch:169, loss:0.7779673337936401\n",
            "batch:170, loss:0.5421075224876404\n",
            "batch:171, loss:0.6432708501815796\n",
            "batch:172, loss:0.677274227142334\n",
            "batch:173, loss:0.48608455061912537\n",
            "batch:174, loss:0.6540337800979614\n",
            "batch:175, loss:0.45293736457824707\n",
            "batch:176, loss:0.6389056444168091\n",
            "batch:177, loss:0.5737948417663574\n",
            "batch:178, loss:0.5366160273551941\n",
            "batch:179, loss:0.7140837907791138\n",
            "batch:180, loss:0.6515901684761047\n",
            "batch:181, loss:0.567973792552948\n",
            "batch:182, loss:0.6077809929847717\n",
            "batch:183, loss:0.4784357249736786\n",
            "batch:184, loss:0.57264244556427\n",
            "batch:185, loss:0.5341454744338989\n",
            "batch:186, loss:0.7228580117225647\n",
            "batch:187, loss:0.5919156074523926\n",
            "batch:188, loss:0.3549075424671173\n",
            "batch:189, loss:0.6144195199012756\n",
            "batch:190, loss:0.7978459596633911\n",
            "batch:191, loss:0.5472182631492615\n",
            "batch:192, loss:0.44877704977989197\n",
            "batch:193, loss:0.4793771207332611\n",
            "batch:194, loss:0.6015863418579102\n",
            "batch:195, loss:0.5573350787162781\n",
            "batch:196, loss:0.7599130272865295\n",
            "batch:197, loss:0.6489390730857849\n",
            "batch:198, loss:0.651771605014801\n",
            "batch:199, loss:0.3367312550544739\n",
            "batch:200, loss:0.39874646067619324\n",
            "batch:201, loss:0.7338184714317322\n",
            "batch:202, loss:0.6212409138679504\n",
            "batch:203, loss:0.4598587453365326\n",
            "batch:204, loss:0.461734414100647\n",
            "batch:205, loss:0.5785719156265259\n",
            "batch:206, loss:0.6036750078201294\n",
            "batch:207, loss:0.625360906124115\n",
            "batch:208, loss:0.7997100353240967\n",
            "batch:209, loss:0.4684942662715912\n",
            "batch:210, loss:0.46794718503952026\n",
            "batch:211, loss:0.46253859996795654\n",
            "batch:212, loss:0.5852563977241516\n",
            "batch:213, loss:0.5342666506767273\n",
            "batch:214, loss:0.5953590869903564\n",
            "batch:215, loss:0.44732603430747986\n",
            "batch:216, loss:0.5826096534729004\n",
            "batch:217, loss:0.4667503833770752\n",
            "batch:218, loss:0.5077016353607178\n",
            "batch:219, loss:0.6791969537734985\n",
            "batch:220, loss:0.6631349921226501\n",
            "batch:221, loss:0.6481345891952515\n",
            "batch:222, loss:0.5486044883728027\n",
            "batch:223, loss:0.48864662647247314\n",
            "batch:224, loss:0.863348126411438\n",
            "batch:225, loss:0.7636116147041321\n",
            "batch:226, loss:0.6002120971679688\n",
            "batch:227, loss:0.5427836179733276\n",
            "batch:228, loss:0.580359160900116\n",
            "batch:229, loss:0.6382175087928772\n",
            "batch:230, loss:0.5172606706619263\n",
            "batch:231, loss:0.65245521068573\n",
            "batch:232, loss:0.5813865065574646\n",
            "batch:233, loss:0.4826003909111023\n",
            "batch:234, loss:0.6249474883079529\n",
            "batch:235, loss:0.6270668506622314\n",
            "batch:236, loss:0.4963884651660919\n",
            "batch:237, loss:0.6590378880500793\n",
            "batch:238, loss:0.5413434505462646\n",
            "batch:239, loss:0.7166478037834167\n",
            "batch:240, loss:0.5574148893356323\n",
            "batch:241, loss:0.7414138317108154\n",
            "batch:242, loss:0.5026653409004211\n",
            "batch:243, loss:0.49893736839294434\n",
            "batch:244, loss:0.6552819013595581\n",
            "batch:245, loss:0.6234628558158875\n",
            "batch:246, loss:0.6884864568710327\n",
            "batch:247, loss:0.6246170401573181\n",
            "batch:248, loss:0.588357150554657\n",
            "batch:249, loss:0.6016032099723816\n",
            "batch:250, loss:0.47523048520088196\n",
            "batch:251, loss:0.5662362575531006\n",
            "batch:252, loss:0.6281002759933472\n",
            "batch:253, loss:0.7954997420310974\n",
            "batch:254, loss:0.6211932301521301\n",
            "batch:255, loss:0.48466572165489197\n",
            "batch:256, loss:0.49700626730918884\n",
            "batch:257, loss:0.5409390926361084\n",
            "batch:258, loss:0.5356165766716003\n",
            "batch:259, loss:0.7879045009613037\n",
            "batch:260, loss:0.6210734844207764\n",
            "batch:261, loss:0.6361124515533447\n",
            "batch:262, loss:0.5064033269882202\n",
            "batch:263, loss:0.5150146484375\n",
            "batch:264, loss:0.6799424886703491\n",
            "batch:265, loss:0.5098210573196411\n",
            "batch:266, loss:0.576356828212738\n",
            "batch:267, loss:0.5903841853141785\n",
            "batch:268, loss:0.5224267244338989\n",
            "batch:269, loss:0.7086611986160278\n",
            "batch:270, loss:0.7461333274841309\n",
            "batch:271, loss:0.5609448552131653\n",
            "batch:272, loss:0.577297031879425\n",
            "batch:273, loss:0.6545543074607849\n",
            "batch:274, loss:0.5790204405784607\n",
            "batch:275, loss:0.6568944454193115\n",
            "batch:276, loss:0.38535866141319275\n",
            "batch:277, loss:0.46126589179039\n",
            "batch:278, loss:0.6556849479675293\n",
            "batch:279, loss:0.49379590153694153\n",
            "batch:280, loss:0.9277212619781494\n",
            "batch:281, loss:0.5993126630783081\n",
            "batch:282, loss:0.5154384970664978\n",
            "batch:283, loss:0.6681609749794006\n",
            "batch:284, loss:0.5081770420074463\n",
            "batch:285, loss:0.6673921346664429\n",
            "batch:286, loss:0.6588983535766602\n",
            "batch:287, loss:0.3052047789096832\n",
            "batch:288, loss:0.8732160925865173\n",
            "batch:289, loss:0.5548352003097534\n",
            "batch:290, loss:0.5968116521835327\n",
            "batch:291, loss:0.5201979875564575\n",
            "batch:292, loss:0.6538434028625488\n",
            "batch:293, loss:0.5567963719367981\n",
            "batch:294, loss:0.4614001214504242\n",
            "batch:295, loss:0.6453340649604797\n",
            "batch:296, loss:0.707361102104187\n",
            "batch:297, loss:0.4490654766559601\n",
            "batch:298, loss:0.5723230838775635\n",
            "batch:299, loss:0.6633810997009277\n",
            "batch:300, loss:0.5036472678184509\n",
            "batch:301, loss:0.5335829257965088\n",
            "batch:302, loss:0.5660897493362427\n",
            "batch:303, loss:0.6307991743087769\n",
            "batch:304, loss:0.5322695970535278\n",
            "batch:305, loss:0.44539976119995117\n",
            "batch:306, loss:0.6081601977348328\n",
            "batch:307, loss:0.6634388566017151\n",
            "batch:308, loss:0.5492923259735107\n",
            "batch:309, loss:0.48209527134895325\n",
            "batch:310, loss:0.5548041462898254\n",
            "batch:311, loss:0.4457077085971832\n",
            "batch:312, loss:0.6244798302650452\n",
            "batch:313, loss:0.5339599847793579\n",
            "batch:314, loss:0.6103140115737915\n",
            "batch:315, loss:0.5454985499382019\n",
            "batch:316, loss:0.42236873507499695\n",
            "batch:317, loss:0.5425142049789429\n",
            "batch:318, loss:0.7073312997817993\n",
            "batch:319, loss:0.5883593559265137\n",
            "batch:320, loss:0.6314301490783691\n",
            "batch:321, loss:0.4869518280029297\n",
            "batch:322, loss:0.6098575592041016\n",
            "batch:323, loss:0.701788067817688\n",
            "batch:324, loss:0.6642152667045593\n",
            "batch:325, loss:0.6916582584381104\n",
            "batch:326, loss:0.5031663775444031\n",
            "batch:327, loss:0.43814557790756226\n",
            "batch:328, loss:0.6735644936561584\n",
            "batch:329, loss:0.4908807575702667\n",
            "batch:330, loss:0.6946461796760559\n",
            "batch:331, loss:0.516802191734314\n",
            "batch:332, loss:0.4967123866081238\n",
            "batch:333, loss:0.45628759264945984\n",
            "batch:334, loss:0.4756689965724945\n",
            "batch:335, loss:0.7304050326347351\n",
            "batch:336, loss:0.5865010023117065\n",
            "batch:337, loss:0.6165276169776917\n",
            "batch:338, loss:0.6072907447814941\n",
            "batch:339, loss:0.7596897482872009\n",
            "batch:340, loss:0.6442033052444458\n",
            "batch:341, loss:0.39108335971832275\n",
            "batch:342, loss:0.4930623471736908\n",
            "batch:343, loss:0.6125692129135132\n",
            "batch:344, loss:0.41833654046058655\n",
            "batch:345, loss:0.5671652555465698\n",
            "batch:346, loss:0.5315703749656677\n",
            "batch:347, loss:0.46574366092681885\n",
            "batch:348, loss:0.6716229319572449\n",
            "batch:349, loss:0.5040399432182312\n",
            "batch:350, loss:0.6844899654388428\n",
            "batch:351, loss:0.7020311951637268\n",
            "batch:352, loss:0.3779526352882385\n",
            "batch:353, loss:0.5158094763755798\n",
            "batch:354, loss:0.5262601971626282\n",
            "batch:355, loss:0.5365861654281616\n",
            "batch:356, loss:0.6088460087776184\n",
            "batch:357, loss:0.48884817957878113\n",
            "batch:358, loss:0.6718655228614807\n",
            "batch:359, loss:0.55559241771698\n",
            "batch:360, loss:0.5675169825553894\n",
            "batch:361, loss:0.5416767001152039\n",
            "batch:362, loss:0.5203886032104492\n",
            "batch:363, loss:0.35663101077079773\n",
            "batch:364, loss:0.4777666926383972\n",
            "batch:365, loss:0.5615922212600708\n",
            "batch:366, loss:0.45933711528778076\n",
            "batch:367, loss:0.6530144214630127\n",
            "batch:368, loss:0.5420898795127869\n",
            "batch:369, loss:0.5520080924034119\n",
            "batch:370, loss:0.5580905079841614\n",
            "batch:371, loss:0.4732258915901184\n",
            "batch:372, loss:0.5622100830078125\n",
            "batch:373, loss:0.49036166071891785\n",
            "batch:374, loss:0.64183509349823\n",
            "batch:375, loss:0.49730774760246277\n",
            "batch:376, loss:0.4898468255996704\n",
            "batch:377, loss:0.6692507863044739\n",
            "batch:378, loss:0.41175734996795654\n",
            "batch:379, loss:0.8653037548065186\n",
            "batch:380, loss:0.48474887013435364\n",
            "batch:381, loss:0.5245591402053833\n",
            "batch:382, loss:0.5238254070281982\n",
            "batch:383, loss:0.8033919334411621\n",
            "batch:384, loss:0.7133960723876953\n",
            "batch:385, loss:0.583145260810852\n",
            "batch:386, loss:0.5263540744781494\n",
            "batch:387, loss:0.7048507928848267\n",
            "batch:388, loss:0.6628216505050659\n",
            "batch:389, loss:0.527205765247345\n",
            "batch:390, loss:0.44002777338027954\n",
            "batch:391, loss:0.481923371553421\n",
            "batch:392, loss:0.5788739323616028\n",
            "batch:393, loss:0.41923847794532776\n",
            "batch:394, loss:0.48905619978904724\n",
            "batch:395, loss:0.47245141863822937\n",
            "batch:396, loss:0.41967934370040894\n",
            "batch:397, loss:0.5068721771240234\n",
            "batch:398, loss:0.49075669050216675\n",
            "batch:399, loss:0.5256370306015015\n",
            "batch:400, loss:0.44331932067871094\n",
            "batch:401, loss:0.7005212903022766\n",
            "batch:402, loss:0.4448847472667694\n",
            "batch:403, loss:0.6338874697685242\n",
            "batch:404, loss:0.5018972158432007\n",
            "batch:405, loss:0.4866068959236145\n",
            "batch:406, loss:0.6595216989517212\n",
            "batch:407, loss:0.6635093092918396\n",
            "batch:408, loss:0.5845772624015808\n",
            "batch:409, loss:0.49070924520492554\n",
            "batch:410, loss:0.6701329946517944\n",
            "batch:411, loss:0.5307756662368774\n",
            "batch:412, loss:0.5479817986488342\n",
            "batch:413, loss:0.5179563760757446\n",
            "batch:414, loss:0.4760201871395111\n",
            "batch:415, loss:0.5769513845443726\n",
            "batch:416, loss:0.41316941380500793\n",
            "batch:417, loss:0.5038477182388306\n",
            "batch:418, loss:0.5100113749504089\n",
            "batch:419, loss:0.42354243993759155\n",
            "batch:420, loss:0.3992999494075775\n",
            "batch:421, loss:0.5956025719642639\n",
            "batch:422, loss:0.45905590057373047\n",
            "batch:423, loss:0.5327423214912415\n",
            "batch:424, loss:0.45169878005981445\n",
            "batch:425, loss:0.36863142251968384\n",
            "batch:426, loss:0.49245163798332214\n",
            "batch:427, loss:0.5414760708808899\n",
            "batch:428, loss:0.3842299282550812\n",
            "batch:429, loss:0.713275671005249\n",
            "batch:430, loss:0.6437664031982422\n",
            "batch:431, loss:0.6498355865478516\n",
            "batch:432, loss:0.4491885304450989\n",
            "batch:433, loss:0.44221067428588867\n",
            "batch:434, loss:0.6192485094070435\n",
            "batch:435, loss:0.3194781541824341\n",
            "batch:436, loss:0.5402088165283203\n",
            "batch:437, loss:0.4797177314758301\n",
            "batch:438, loss:0.5082834959030151\n",
            "batch:439, loss:0.5541864633560181\n",
            "batch:440, loss:0.4830184578895569\n",
            "batch:441, loss:0.7560890316963196\n",
            "batch:442, loss:0.4788346290588379\n",
            "batch:443, loss:0.596182107925415\n",
            "batch:444, loss:0.677577018737793\n",
            "batch:445, loss:0.5064730644226074\n",
            "batch:446, loss:0.6479088664054871\n",
            "batch:447, loss:0.5394347906112671\n",
            "batch:448, loss:0.4636997580528259\n",
            "batch:449, loss:0.46891021728515625\n",
            "batch:450, loss:0.4817759096622467\n",
            "batch:451, loss:0.6249052286148071\n",
            "batch:452, loss:0.5723204016685486\n",
            "batch:453, loss:0.7110883593559265\n",
            "batch:454, loss:0.45028752088546753\n",
            "batch:455, loss:0.5557132959365845\n",
            "batch:456, loss:0.6516514420509338\n",
            "batch:457, loss:0.4836883842945099\n",
            "batch:458, loss:0.6996840834617615\n",
            "batch:459, loss:0.5426764488220215\n",
            "batch:460, loss:0.800174355506897\n",
            "batch:461, loss:0.3585808575153351\n",
            "batch:462, loss:0.38480886816978455\n",
            "batch:463, loss:0.48597386479377747\n",
            "batch:464, loss:0.43326205015182495\n",
            "batch:465, loss:0.3939017653465271\n",
            "batch:466, loss:0.5436519980430603\n",
            "batch:467, loss:0.5105422139167786\n",
            "batch:468, loss:0.6033028960227966\n",
            "batch:469, loss:0.5728067755699158\n",
            "batch:470, loss:0.48909106850624084\n",
            "batch:471, loss:0.5422382354736328\n",
            "batch:472, loss:0.5237688422203064\n",
            "batch:473, loss:0.4402714669704437\n",
            "batch:474, loss:0.5811161398887634\n",
            "batch:475, loss:0.5062870383262634\n",
            "batch:476, loss:0.5210709571838379\n",
            "batch:477, loss:0.5334439873695374\n",
            "batch:478, loss:0.5604866147041321\n",
            "batch:479, loss:0.5646815896034241\n",
            "batch:480, loss:0.5179428458213806\n",
            "batch:481, loss:0.6226252913475037\n",
            "batch:482, loss:0.5551548004150391\n",
            "batch:483, loss:0.39736267924308777\n",
            "batch:484, loss:0.5771964192390442\n",
            "batch:485, loss:0.5446237921714783\n",
            "batch:486, loss:0.5463064908981323\n",
            "batch:487, loss:0.40089288353919983\n",
            "batch:488, loss:0.6253491044044495\n",
            "batch:489, loss:0.37511584162712097\n",
            "batch:490, loss:0.40283381938934326\n",
            "batch:491, loss:0.5792618989944458\n",
            "batch:492, loss:0.5925561785697937\n",
            "batch:493, loss:0.5489605665206909\n",
            "batch:494, loss:0.46939465403556824\n",
            "batch:495, loss:0.5004873871803284\n",
            "batch:496, loss:0.5167896747589111\n",
            "batch:497, loss:0.4767477810382843\n",
            "batch:498, loss:0.44324609637260437\n",
            "batch:499, loss:0.6101320385932922\n",
            "batch:500, loss:0.6618363857269287\n",
            "batch:501, loss:0.6672613620758057\n",
            "batch:502, loss:0.5451140999794006\n",
            "batch:503, loss:0.5281384587287903\n",
            "batch:504, loss:0.5796607732772827\n",
            "batch:505, loss:0.545231819152832\n",
            "batch:506, loss:0.6573339700698853\n",
            "batch:507, loss:0.6179217100143433\n",
            "batch:508, loss:0.5546371340751648\n",
            "batch:509, loss:0.6204273104667664\n",
            "batch:510, loss:0.6229074001312256\n",
            "batch:511, loss:0.4457676410675049\n",
            "batch:512, loss:0.4101735055446625\n",
            "batch:513, loss:0.4566497504711151\n",
            "batch:514, loss:0.6511773467063904\n",
            "batch:515, loss:0.4135216772556305\n",
            "batch:516, loss:0.5628412961959839\n",
            "batch:517, loss:0.5703672170639038\n",
            "batch:518, loss:0.4689328372478485\n",
            "batch:519, loss:0.5499342679977417\n",
            "batch:520, loss:0.6668034195899963\n",
            "batch:521, loss:0.5221024751663208\n",
            "batch:522, loss:0.6408569812774658\n",
            "batch:523, loss:0.5137134790420532\n",
            "batch:524, loss:0.3756142556667328\n",
            "batch:525, loss:0.569805920124054\n",
            "batch:526, loss:0.666632890701294\n",
            "batch:527, loss:0.5726223587989807\n",
            "batch:528, loss:0.5927779078483582\n",
            "batch:529, loss:0.7480186820030212\n",
            "batch:530, loss:0.6767974495887756\n",
            "batch:531, loss:0.5234849452972412\n",
            "batch:532, loss:0.547173261642456\n",
            "batch:533, loss:0.5451876521110535\n",
            "batch:534, loss:0.4478895664215088\n",
            "batch:535, loss:0.5505473017692566\n",
            "batch:536, loss:0.4852704703807831\n",
            "batch:537, loss:0.4592204988002777\n",
            "batch:538, loss:0.2815152406692505\n",
            "batch:539, loss:0.5326092839241028\n",
            "batch:540, loss:0.5485160946846008\n",
            "batch:541, loss:0.5107855796813965\n",
            "batch:542, loss:0.41209691762924194\n",
            "batch:543, loss:0.7895527482032776\n",
            "batch:544, loss:0.6835896968841553\n",
            "batch:545, loss:0.6897236704826355\n",
            "batch:546, loss:0.5922072529792786\n",
            "batch:547, loss:0.5621911883354187\n",
            "batch:548, loss:0.5026068091392517\n",
            "batch:549, loss:0.433371365070343\n",
            "batch:550, loss:0.4763079881668091\n",
            "batch:551, loss:0.4431518614292145\n",
            "batch:552, loss:0.5834242701530457\n",
            "batch:553, loss:0.47763386368751526\n",
            "batch:554, loss:0.4035256505012512\n",
            "batch:555, loss:0.4673783481121063\n",
            "batch:556, loss:0.46766552329063416\n",
            "batch:557, loss:0.5118753910064697\n",
            "batch:558, loss:0.6899611949920654\n",
            "batch:559, loss:0.5972706079483032\n",
            "batch:560, loss:0.6770926117897034\n",
            "batch:561, loss:0.3708798289299011\n",
            "batch:562, loss:0.390290230512619\n",
            "batch:563, loss:0.5501164793968201\n",
            "batch:564, loss:0.6618788838386536\n",
            "batch:565, loss:0.3695325255393982\n",
            "batch:566, loss:0.39073923230171204\n",
            "batch:567, loss:0.7029269933700562\n",
            "batch:568, loss:0.4834795892238617\n",
            "batch:569, loss:0.750016450881958\n",
            "batch:570, loss:0.6333541870117188\n",
            "batch:571, loss:0.4640093445777893\n",
            "batch:572, loss:0.5978238582611084\n",
            "batch:573, loss:0.7625488638877869\n",
            "batch:574, loss:0.5623411536216736\n",
            "batch:575, loss:0.6699993014335632\n",
            "batch:576, loss:0.4909650385379791\n",
            "batch:577, loss:0.558384120464325\n",
            "batch:578, loss:0.6616231799125671\n",
            "batch:579, loss:0.6671680212020874\n",
            "batch:580, loss:0.7852538228034973\n",
            "batch:581, loss:0.42473942041397095\n",
            "batch:582, loss:0.5238986015319824\n",
            "batch:583, loss:0.3593485355377197\n",
            "batch:584, loss:0.4957610070705414\n",
            "batch:585, loss:0.7716026306152344\n",
            "batch:586, loss:0.48297038674354553\n",
            "batch:587, loss:0.6596792340278625\n",
            "batch:588, loss:0.5195949673652649\n",
            "batch:589, loss:0.5366243124008179\n",
            "batch:590, loss:0.5629099607467651\n",
            "batch:591, loss:0.654532253742218\n",
            "batch:592, loss:0.439434289932251\n",
            "batch:593, loss:0.6801344752311707\n",
            "batch:594, loss:0.5346701741218567\n",
            "batch:595, loss:0.45344340801239014\n",
            "batch:596, loss:0.4899364113807678\n",
            "batch:597, loss:0.3817650377750397\n",
            "batch:598, loss:0.46301722526550293\n",
            "batch:599, loss:0.4990133047103882\n",
            "batch:600, loss:0.620902955532074\n",
            "batch:601, loss:0.7343202233314514\n",
            "batch:602, loss:0.3156375586986542\n",
            "batch:603, loss:0.6156489849090576\n",
            "batch:604, loss:0.6368380784988403\n",
            "batch:605, loss:0.36449065804481506\n",
            "batch:606, loss:0.4893074929714203\n",
            "batch:607, loss:0.6115671992301941\n",
            "batch:608, loss:0.644968569278717\n",
            "batch:609, loss:0.3920421004295349\n",
            "batch:610, loss:0.4524182379245758\n",
            "batch:611, loss:0.5840097069740295\n",
            "batch:612, loss:0.5031765103340149\n",
            "batch:613, loss:0.48037075996398926\n",
            "batch:614, loss:0.4261314570903778\n",
            "batch:615, loss:0.6007695198059082\n",
            "batch:616, loss:0.3771863579750061\n",
            "batch:617, loss:0.6204039454460144\n",
            "batch:618, loss:0.4837245047092438\n",
            "batch:619, loss:0.4921973943710327\n",
            "batch:620, loss:0.5297094583511353\n",
            "batch:621, loss:0.5781894326210022\n",
            "batch:622, loss:0.6594849824905396\n",
            "batch:623, loss:0.5794169306755066\n",
            "batch:624, loss:0.6716736555099487\n",
            "batch:625, loss:0.3804466128349304\n",
            "batch:626, loss:0.7118955850601196\n",
            "batch:627, loss:0.38451293110847473\n",
            "batch:628, loss:0.4789384603500366\n",
            "batch:629, loss:0.5923666954040527\n",
            "batch:630, loss:0.6031929850578308\n",
            "batch:631, loss:0.6406476497650146\n",
            "batch:632, loss:0.608090341091156\n",
            "batch:633, loss:0.41875898838043213\n",
            "batch:634, loss:0.6365421414375305\n",
            "batch:635, loss:0.5754789113998413\n",
            "batch:636, loss:0.712705135345459\n",
            "batch:637, loss:0.2764856815338135\n",
            "batch:638, loss:0.4454890191555023\n",
            "batch:639, loss:0.6115254759788513\n",
            "batch:640, loss:0.5435569286346436\n",
            "batch:641, loss:0.44189971685409546\n",
            "batch:642, loss:0.48827895522117615\n",
            "batch:643, loss:0.5770848989486694\n",
            "batch:644, loss:0.6461597681045532\n",
            "batch:645, loss:0.5212358832359314\n",
            "batch:646, loss:0.5208747386932373\n",
            "batch:647, loss:0.5381622910499573\n",
            "batch:648, loss:0.3476170599460602\n",
            "batch:649, loss:0.5305606126785278\n",
            "batch:650, loss:0.555387020111084\n",
            "batch:651, loss:0.5979225039482117\n",
            "batch:652, loss:0.5579562187194824\n",
            "batch:653, loss:0.4449830949306488\n",
            "batch:654, loss:0.6538305282592773\n",
            "batch:655, loss:0.5911651253700256\n",
            "batch:656, loss:0.7108602523803711\n",
            "batch:657, loss:0.5998554229736328\n",
            "batch:658, loss:0.4551866352558136\n",
            "batch:659, loss:0.5674421191215515\n",
            "batch:660, loss:0.6352216005325317\n",
            "batch:661, loss:0.42520490288734436\n",
            "batch:662, loss:0.5118037462234497\n",
            "batch:663, loss:0.5762778520584106\n",
            "batch:664, loss:0.2646084129810333\n",
            "batch:665, loss:0.6253041625022888\n",
            "batch:666, loss:0.6533846259117126\n",
            "batch:667, loss:0.5448455810546875\n",
            "batch:668, loss:0.5804111957550049\n",
            "batch:669, loss:0.5015080571174622\n",
            "batch:670, loss:0.33722957968711853\n",
            "batch:671, loss:0.5434297919273376\n",
            "batch:672, loss:0.43717461824417114\n",
            "batch:673, loss:0.562623918056488\n",
            "batch:674, loss:0.47271719574928284\n",
            "batch:675, loss:0.560538113117218\n",
            "batch:676, loss:0.6323670148849487\n",
            "batch:677, loss:0.6237896084785461\n",
            "batch:678, loss:0.5859826803207397\n",
            "batch:679, loss:0.6082947254180908\n",
            "batch:680, loss:0.4545258581638336\n",
            "batch:681, loss:0.61720210313797\n",
            "batch:682, loss:0.4590747356414795\n",
            "batch:683, loss:0.45489102602005005\n",
            "batch:684, loss:0.43111705780029297\n",
            "batch:685, loss:0.6038716435432434\n",
            "batch:686, loss:0.41922834515571594\n",
            "batch:687, loss:0.7094956636428833\n",
            "batch:688, loss:0.44316989183425903\n",
            "batch:689, loss:0.5149624347686768\n",
            "batch:690, loss:0.621302604675293\n",
            "batch:691, loss:0.4512391686439514\n",
            "batch:692, loss:0.5766268372535706\n",
            "batch:693, loss:0.48120489716529846\n",
            "batch:694, loss:0.6614182591438293\n",
            "batch:695, loss:0.469627320766449\n",
            "batch:696, loss:0.6099117398262024\n",
            "batch:697, loss:0.5088185667991638\n",
            "batch:698, loss:0.5618268251419067\n",
            "batch:699, loss:0.6624639630317688\n",
            "batch:700, loss:0.5237039923667908\n",
            "batch:701, loss:0.6669689416885376\n",
            "batch:702, loss:0.4708687365055084\n",
            "batch:703, loss:0.7074505686759949\n",
            "batch:704, loss:0.5801696181297302\n",
            "batch:705, loss:0.5815929174423218\n",
            "batch:706, loss:0.6457447409629822\n",
            "batch:707, loss:0.3679291903972626\n",
            "batch:708, loss:0.60173100233078\n",
            "batch:709, loss:0.5275200009346008\n",
            "batch:710, loss:0.6233230829238892\n",
            "batch:711, loss:0.4895411431789398\n",
            "batch:712, loss:0.40070590376853943\n",
            "batch:713, loss:0.54982590675354\n",
            "batch:714, loss:0.5894343256950378\n",
            "batch:715, loss:0.48439913988113403\n",
            "batch:716, loss:0.724553644657135\n",
            "batch:717, loss:0.5349110960960388\n",
            "batch:718, loss:0.43378210067749023\n",
            "batch:719, loss:0.7064751386642456\n",
            "batch:720, loss:0.45199236273765564\n",
            "batch:721, loss:0.6073070764541626\n",
            "batch:722, loss:0.643185019493103\n",
            "batch:723, loss:0.524548351764679\n",
            "batch:724, loss:0.5454983711242676\n",
            "batch:725, loss:0.5834838151931763\n",
            "batch:726, loss:0.4703592360019684\n",
            "batch:727, loss:0.43023911118507385\n",
            "batch:728, loss:0.6647927761077881\n",
            "batch:729, loss:0.3895743191242218\n",
            "batch:730, loss:0.5760358572006226\n",
            "batch:731, loss:0.46275588870048523\n",
            "batch:732, loss:0.5790941715240479\n",
            "batch:733, loss:0.3751792013645172\n",
            "batch:734, loss:0.5736705660820007\n",
            "batch:735, loss:0.4402027428150177\n",
            "batch:736, loss:0.5903534293174744\n",
            "batch:737, loss:0.6948001384735107\n",
            "batch:738, loss:0.45907700061798096\n",
            "batch:739, loss:0.5540257096290588\n",
            "batch:740, loss:0.5372665524482727\n",
            "batch:741, loss:0.42466065287590027\n",
            "batch:742, loss:0.4864323139190674\n",
            "batch:743, loss:0.8443397283554077\n",
            "batch:744, loss:0.3754844665527344\n",
            "batch:745, loss:0.5182449221611023\n",
            "batch:746, loss:0.49209868907928467\n",
            "batch:747, loss:0.5613463521003723\n",
            "batch:748, loss:0.4520546793937683\n",
            "batch:749, loss:0.3837451934814453\n",
            "batch:750, loss:0.7977704405784607\n",
            "batch:751, loss:0.6094757318496704\n",
            "batch:752, loss:0.559548020362854\n",
            "batch:753, loss:0.5798807740211487\n",
            "batch:754, loss:0.33416983485221863\n",
            "batch:755, loss:0.38849207758903503\n",
            "batch:756, loss:0.6750337481498718\n",
            "batch:757, loss:0.6409337520599365\n",
            "batch:758, loss:0.5969205498695374\n",
            "batch:759, loss:0.6070259809494019\n",
            "batch:760, loss:0.47845062613487244\n",
            "batch:761, loss:0.6307925581932068\n",
            "batch:762, loss:0.5273334980010986\n",
            "batch:763, loss:0.5151540040969849\n",
            "batch:764, loss:0.45653554797172546\n",
            "batch:765, loss:0.6737033128738403\n",
            "batch:766, loss:0.6404402256011963\n",
            "batch:767, loss:0.7726194262504578\n",
            "batch:768, loss:0.5364052653312683\n",
            "batch:769, loss:0.4495656490325928\n",
            "batch:770, loss:0.36730465292930603\n",
            "batch:771, loss:0.2964155673980713\n",
            "batch:772, loss:0.5417169332504272\n",
            "batch:773, loss:0.4992988407611847\n",
            "batch:774, loss:0.48667439818382263\n",
            "batch:775, loss:0.6346743106842041\n",
            "batch:776, loss:0.47952017188072205\n",
            "batch:777, loss:0.5207239389419556\n",
            "batch:778, loss:0.6539855599403381\n",
            "batch:779, loss:0.461161732673645\n",
            "batch:780, loss:0.5004397034645081\n",
            "batch:781, loss:0.5219473242759705\n",
            "batch:782, loss:0.4983576834201813\n",
            "batch:783, loss:0.5247530341148376\n",
            "batch:784, loss:0.5518447160720825\n",
            "batch:785, loss:0.38169926404953003\n",
            "batch:786, loss:0.5562019348144531\n",
            "batch:787, loss:0.5246278643608093\n",
            "batch:788, loss:0.5638564229011536\n",
            "batch:789, loss:0.5597235560417175\n",
            "batch:790, loss:0.7316397428512573\n",
            "batch:791, loss:0.3474310636520386\n",
            "batch:792, loss:0.6977656483650208\n",
            "batch:793, loss:0.4355789124965668\n",
            "batch:794, loss:0.5259317755699158\n",
            "batch:795, loss:0.5802143216133118\n",
            "batch:796, loss:0.4878866970539093\n",
            "batch:797, loss:0.5741408467292786\n",
            "batch:798, loss:0.4673581123352051\n",
            "batch:799, loss:0.47669070959091187\n",
            "batch:800, loss:0.6365988850593567\n",
            "batch:801, loss:0.4232204556465149\n",
            "batch:802, loss:0.5052031874656677\n",
            "batch:803, loss:0.5753989815711975\n",
            "batch:804, loss:0.5940402150154114\n",
            "batch:805, loss:0.4988734722137451\n",
            "batch:806, loss:0.581922709941864\n",
            "batch:807, loss:0.46632078289985657\n",
            "batch:808, loss:0.4752521216869354\n",
            "batch:809, loss:0.4226333498954773\n",
            "batch:810, loss:0.6232072114944458\n",
            "batch:811, loss:0.4628640413284302\n",
            "batch:812, loss:0.36453765630722046\n",
            "batch:813, loss:0.5732057690620422\n",
            "batch:814, loss:0.46100544929504395\n",
            "batch:815, loss:0.525235116481781\n",
            "batch:816, loss:0.685662567615509\n",
            "batch:817, loss:0.6176198124885559\n",
            "batch:818, loss:0.42515280842781067\n",
            "batch:819, loss:0.3915887176990509\n",
            "batch:820, loss:0.49323099851608276\n",
            "batch:821, loss:0.8011018633842468\n",
            "batch:822, loss:0.5285918116569519\n",
            "batch:823, loss:0.7075499892234802\n",
            "batch:824, loss:0.3187723159790039\n",
            "batch:825, loss:0.5634884238243103\n",
            "batch:826, loss:0.3875522315502167\n",
            "batch:827, loss:0.44598475098609924\n",
            "batch:828, loss:0.42036569118499756\n",
            "batch:829, loss:0.502144992351532\n",
            "batch:830, loss:0.6289515495300293\n",
            "batch:831, loss:0.47852376103401184\n",
            "batch:832, loss:0.4794765114784241\n",
            "batch:833, loss:0.5302202701568604\n",
            "batch:834, loss:0.579833447933197\n",
            "batch:835, loss:0.4345439672470093\n",
            "batch:836, loss:0.48128771781921387\n",
            "batch:837, loss:0.46618008613586426\n",
            "batch:838, loss:0.46992605924606323\n",
            "batch:839, loss:0.553712785243988\n",
            "batch:840, loss:0.5391234755516052\n",
            "batch:841, loss:0.38532379269599915\n",
            "batch:842, loss:0.5809612274169922\n",
            "batch:843, loss:0.5034902691841125\n",
            "batch:844, loss:0.4697009325027466\n",
            "batch:845, loss:0.4719325304031372\n",
            "batch:846, loss:0.44000402092933655\n",
            "batch:847, loss:0.5779728889465332\n",
            "batch:848, loss:0.5683448910713196\n",
            "batch:849, loss:0.5467692613601685\n",
            "batch:850, loss:0.5794902443885803\n",
            "batch:851, loss:0.5711604952812195\n",
            "batch:852, loss:0.35483208298683167\n",
            "batch:853, loss:0.4665854275226593\n",
            "batch:854, loss:0.3334585130214691\n",
            "batch:855, loss:0.48975804448127747\n",
            "batch:856, loss:0.4186145067214966\n",
            "batch:857, loss:0.5159814953804016\n",
            "batch:858, loss:0.5201325416564941\n",
            "batch:859, loss:0.6758934855461121\n",
            "batch:860, loss:0.39862754940986633\n",
            "batch:861, loss:0.6050580739974976\n",
            "batch:862, loss:0.5321602821350098\n",
            "batch:863, loss:0.6047563552856445\n",
            "batch:864, loss:0.5896064639091492\n",
            "batch:865, loss:0.7264680862426758\n",
            "batch:866, loss:0.6383368372917175\n",
            "batch:867, loss:0.4769989848136902\n",
            "batch:868, loss:0.4810184836387634\n",
            "batch:869, loss:0.40401169657707214\n",
            "batch:870, loss:0.49164509773254395\n",
            "batch:871, loss:0.514100968837738\n",
            "batch:872, loss:0.4922013282775879\n",
            "batch:873, loss:0.3497912287712097\n",
            "batch:874, loss:0.7726179957389832\n",
            "batch:875, loss:0.52071613073349\n",
            "batch:876, loss:0.2911747992038727\n",
            "batch:877, loss:0.5542340874671936\n",
            "batch:878, loss:0.5254800915718079\n",
            "batch:879, loss:0.7075279355049133\n",
            "batch:880, loss:0.738682746887207\n",
            "batch:881, loss:0.6616389155387878\n",
            "batch:882, loss:0.4765888452529907\n",
            "batch:883, loss:0.41280534863471985\n",
            "batch:884, loss:0.45405149459838867\n",
            "batch:885, loss:0.6459493041038513\n",
            "batch:886, loss:0.5169304609298706\n",
            "batch:887, loss:0.43439024686813354\n",
            "batch:888, loss:0.4681485891342163\n",
            "batch:889, loss:0.6593117117881775\n",
            "batch:890, loss:0.5650481581687927\n",
            "batch:891, loss:0.46719008684158325\n",
            "batch:892, loss:0.54103684425354\n",
            "batch:893, loss:0.450915664434433\n",
            "batch:894, loss:0.4790610671043396\n",
            "batch:895, loss:0.43761682510375977\n",
            "batch:896, loss:0.6295509934425354\n",
            "batch:897, loss:0.6704291105270386\n",
            "batch:898, loss:0.37670764327049255\n",
            "batch:899, loss:0.39617544412612915\n",
            "batch:900, loss:0.4490894377231598\n",
            "batch:901, loss:0.5034051537513733\n",
            "batch:902, loss:0.5450485348701477\n",
            "batch:903, loss:0.3916678726673126\n",
            "batch:904, loss:0.5492597818374634\n",
            "batch:905, loss:0.30657798051834106\n",
            "batch:906, loss:0.4195510745048523\n",
            "batch:907, loss:0.368735134601593\n",
            "batch:908, loss:0.5256409645080566\n",
            "batch:909, loss:0.5128390789031982\n",
            "batch:910, loss:0.8021320104598999\n",
            "batch:911, loss:0.4634101092815399\n",
            "batch:912, loss:0.5009793043136597\n",
            "batch:913, loss:0.5802792906761169\n",
            "batch:914, loss:0.48011112213134766\n",
            "batch:915, loss:0.5771390795707703\n",
            "batch:916, loss:0.5403398871421814\n",
            "batch:917, loss:0.6480543613433838\n",
            "batch:918, loss:0.6656826734542847\n",
            "batch:919, loss:0.48290640115737915\n",
            "batch:920, loss:0.46535831689834595\n",
            "batch:921, loss:0.4064836800098419\n",
            "batch:922, loss:0.547258198261261\n",
            "batch:923, loss:0.4005475342273712\n",
            "batch:924, loss:0.6591445803642273\n",
            "batch:925, loss:0.5300706624984741\n",
            "batch:926, loss:0.5233541131019592\n",
            "batch:927, loss:0.560082197189331\n",
            "batch:928, loss:0.636570930480957\n",
            "batch:929, loss:0.6463028192520142\n",
            "batch:930, loss:0.5800130367279053\n",
            "batch:931, loss:0.48532554507255554\n",
            "batch:932, loss:0.5749198198318481\n",
            "batch:933, loss:0.5722561478614807\n",
            "batch:934, loss:0.4816208481788635\n",
            "batch:935, loss:0.5197337865829468\n",
            "batch:936, loss:0.36534741520881653\n",
            "batch:937, loss:0.4635781943798065\n",
            "Training loss: 0.5539280211112139\n",
            "batch:0, loss:0.5156254768371582\n",
            "batch:1, loss:0.5670064091682434\n",
            "batch:2, loss:0.5112748742103577\n",
            "batch:3, loss:0.48768457770347595\n",
            "batch:4, loss:0.3246456980705261\n",
            "batch:5, loss:0.3240947723388672\n",
            "batch:6, loss:0.4104844033718109\n",
            "batch:7, loss:0.4793762266635895\n",
            "batch:8, loss:0.5640838742256165\n",
            "batch:9, loss:0.5675820708274841\n",
            "batch:10, loss:0.5512557625770569\n",
            "batch:11, loss:0.587335467338562\n",
            "batch:12, loss:0.5456506609916687\n",
            "batch:13, loss:0.5389165282249451\n",
            "batch:14, loss:0.6768497824668884\n",
            "batch:15, loss:0.4929896295070648\n",
            "batch:16, loss:0.5409409999847412\n",
            "batch:17, loss:0.4043269455432892\n",
            "batch:18, loss:0.5557175278663635\n",
            "batch:19, loss:0.6778459548950195\n",
            "batch:20, loss:0.4376470148563385\n",
            "batch:21, loss:0.5179727673530579\n",
            "batch:22, loss:0.5048986077308655\n",
            "batch:23, loss:0.3750899136066437\n",
            "batch:24, loss:0.5323318243026733\n",
            "batch:25, loss:0.5000126957893372\n",
            "batch:26, loss:0.4030756652355194\n",
            "batch:27, loss:0.632928192615509\n",
            "batch:28, loss:0.5832213759422302\n",
            "batch:29, loss:0.39696723222732544\n",
            "batch:30, loss:0.4719880223274231\n",
            "batch:31, loss:0.6541992425918579\n",
            "batch:32, loss:0.44005563855171204\n",
            "batch:33, loss:0.5669005513191223\n",
            "batch:34, loss:0.4843631982803345\n",
            "batch:35, loss:0.638704240322113\n",
            "batch:36, loss:0.6708762645721436\n",
            "batch:37, loss:0.572939395904541\n",
            "batch:38, loss:0.6363871097564697\n",
            "batch:39, loss:0.6454066634178162\n",
            "batch:40, loss:0.6960838437080383\n",
            "batch:41, loss:0.4730008840560913\n",
            "batch:42, loss:0.6759186387062073\n",
            "batch:43, loss:0.5569961071014404\n",
            "batch:44, loss:0.4407027065753937\n",
            "batch:45, loss:0.388185977935791\n",
            "batch:46, loss:0.4349881112575531\n",
            "batch:47, loss:0.5358924865722656\n",
            "batch:48, loss:0.49605026841163635\n",
            "batch:49, loss:0.5182463526725769\n",
            "batch:50, loss:0.4658268690109253\n",
            "batch:51, loss:0.4243532717227936\n",
            "batch:52, loss:0.4339987635612488\n",
            "batch:53, loss:0.6966660022735596\n",
            "batch:54, loss:0.5003281831741333\n",
            "batch:55, loss:0.35273468494415283\n",
            "batch:56, loss:0.35516220331192017\n",
            "batch:57, loss:0.46117424964904785\n",
            "batch:58, loss:0.5990786552429199\n",
            "batch:59, loss:0.5142753720283508\n",
            "batch:60, loss:0.38240954279899597\n",
            "batch:61, loss:0.5448974370956421\n",
            "batch:62, loss:0.4913729131221771\n",
            "batch:63, loss:0.42309415340423584\n",
            "batch:64, loss:0.5525492429733276\n",
            "batch:65, loss:0.48494118452072144\n",
            "batch:66, loss:0.435087651014328\n",
            "batch:67, loss:0.5493525266647339\n",
            "batch:68, loss:0.4246305525302887\n",
            "batch:69, loss:0.4407116174697876\n",
            "batch:70, loss:0.4244060814380646\n",
            "batch:71, loss:0.5819045305252075\n",
            "batch:72, loss:0.3321632742881775\n",
            "batch:73, loss:0.5221287608146667\n",
            "batch:74, loss:0.6347814202308655\n",
            "batch:75, loss:0.4922424554824829\n",
            "batch:76, loss:0.3616129457950592\n",
            "batch:77, loss:0.40573060512542725\n",
            "batch:78, loss:0.42377495765686035\n",
            "batch:79, loss:0.5767348408699036\n",
            "batch:80, loss:0.5601166486740112\n",
            "batch:81, loss:0.3865218460559845\n",
            "batch:82, loss:0.46099424362182617\n",
            "batch:83, loss:0.5160579085350037\n",
            "batch:84, loss:0.4416007399559021\n",
            "batch:85, loss:0.4739587604999542\n",
            "batch:86, loss:0.5164389610290527\n",
            "batch:87, loss:0.49379482865333557\n",
            "batch:88, loss:0.47084560990333557\n",
            "batch:89, loss:0.5174903869628906\n",
            "batch:90, loss:0.48162639141082764\n",
            "batch:91, loss:0.5518133044242859\n",
            "batch:92, loss:0.5000331401824951\n",
            "batch:93, loss:0.4060474634170532\n",
            "batch:94, loss:0.5321448445320129\n",
            "batch:95, loss:0.5607041716575623\n",
            "batch:96, loss:0.5500388145446777\n",
            "batch:97, loss:0.6280032396316528\n",
            "batch:98, loss:0.6989034414291382\n",
            "batch:99, loss:0.5107360482215881\n",
            "batch:100, loss:0.5857752561569214\n",
            "batch:101, loss:0.37958788871765137\n",
            "batch:102, loss:0.5278939604759216\n",
            "batch:103, loss:0.6815721988677979\n",
            "batch:104, loss:0.5134718418121338\n",
            "batch:105, loss:0.4676518142223358\n",
            "batch:106, loss:0.5631789565086365\n",
            "batch:107, loss:0.4485164284706116\n",
            "batch:108, loss:0.3147953748703003\n",
            "batch:109, loss:0.5804322957992554\n",
            "batch:110, loss:0.46036165952682495\n",
            "batch:111, loss:0.5724696516990662\n",
            "batch:112, loss:0.4803828299045563\n",
            "batch:113, loss:0.654366135597229\n",
            "batch:114, loss:0.6638325452804565\n",
            "batch:115, loss:0.5665245652198792\n",
            "batch:116, loss:0.7143422365188599\n",
            "batch:117, loss:0.34421443939208984\n",
            "batch:118, loss:0.4322853982448578\n",
            "batch:119, loss:0.5518621802330017\n",
            "batch:120, loss:0.5896700024604797\n",
            "batch:121, loss:0.48501262068748474\n",
            "batch:122, loss:0.5590899586677551\n",
            "batch:123, loss:0.46246209740638733\n",
            "batch:124, loss:0.6643391251564026\n",
            "batch:125, loss:0.5055714249610901\n",
            "batch:126, loss:0.5422000885009766\n",
            "batch:127, loss:0.48828285932540894\n",
            "batch:128, loss:0.3298082947731018\n",
            "batch:129, loss:0.5034522414207458\n",
            "batch:130, loss:0.42483019828796387\n",
            "batch:131, loss:0.5074635148048401\n",
            "batch:132, loss:0.41822725534439087\n",
            "batch:133, loss:0.64564448595047\n",
            "batch:134, loss:0.5473670363426208\n",
            "batch:135, loss:0.5378704071044922\n",
            "batch:136, loss:0.49481400847435\n",
            "batch:137, loss:0.5407040119171143\n",
            "batch:138, loss:0.4227137863636017\n",
            "batch:139, loss:0.42365890741348267\n",
            "batch:140, loss:0.6957301497459412\n",
            "batch:141, loss:0.5762487649917603\n",
            "batch:142, loss:0.44831910729408264\n",
            "batch:143, loss:0.47519364953041077\n",
            "batch:144, loss:0.5133206844329834\n",
            "batch:145, loss:0.4435408413410187\n",
            "batch:146, loss:0.5435502529144287\n",
            "batch:147, loss:0.5844189524650574\n",
            "batch:148, loss:0.45508262515068054\n",
            "batch:149, loss:0.39311274886131287\n",
            "batch:150, loss:0.5705309510231018\n",
            "batch:151, loss:0.48201462626457214\n",
            "batch:152, loss:0.3536019027233124\n",
            "batch:153, loss:0.32004499435424805\n",
            "batch:154, loss:0.4629914462566376\n",
            "batch:155, loss:0.3935222923755646\n",
            "batch:156, loss:0.5945368409156799\n",
            "batch:157, loss:0.5788770914077759\n",
            "batch:158, loss:0.4366645812988281\n",
            "batch:159, loss:0.3709212839603424\n",
            "batch:160, loss:0.2805313766002655\n",
            "batch:161, loss:0.7348926663398743\n",
            "batch:162, loss:0.5468783378601074\n",
            "batch:163, loss:0.5705510377883911\n",
            "batch:164, loss:0.32436031103134155\n",
            "batch:165, loss:0.6777932047843933\n",
            "batch:166, loss:0.4166373908519745\n",
            "batch:167, loss:0.35702744126319885\n",
            "batch:168, loss:0.3997233510017395\n",
            "batch:169, loss:0.4430772066116333\n",
            "batch:170, loss:0.4209856688976288\n",
            "batch:171, loss:0.6107342839241028\n",
            "batch:172, loss:0.47714340686798096\n",
            "batch:173, loss:0.5712667107582092\n",
            "batch:174, loss:0.4863034188747406\n",
            "batch:175, loss:0.4726254343986511\n",
            "batch:176, loss:0.5738946795463562\n",
            "batch:177, loss:0.41765880584716797\n",
            "batch:178, loss:0.6956903338432312\n",
            "batch:179, loss:0.4841333031654358\n",
            "batch:180, loss:0.37493696808815\n",
            "batch:181, loss:0.40189695358276367\n",
            "batch:182, loss:0.4650735557079315\n",
            "batch:183, loss:0.49174758791923523\n",
            "batch:184, loss:0.33281955122947693\n",
            "batch:185, loss:0.6581289768218994\n",
            "batch:186, loss:0.4102003574371338\n",
            "batch:187, loss:0.6867555975914001\n",
            "batch:188, loss:0.29781925678253174\n",
            "batch:189, loss:0.6082656979560852\n",
            "batch:190, loss:0.4879755675792694\n",
            "batch:191, loss:0.6044583916664124\n",
            "batch:192, loss:0.6960604190826416\n",
            "batch:193, loss:0.47316980361938477\n",
            "batch:194, loss:0.5963579416275024\n",
            "batch:195, loss:0.2966674864292145\n",
            "batch:196, loss:0.6443501710891724\n",
            "batch:197, loss:0.564943253993988\n",
            "batch:198, loss:0.3894753158092499\n",
            "batch:199, loss:0.3970029652118683\n",
            "batch:200, loss:0.6684757471084595\n",
            "batch:201, loss:0.5391682386398315\n",
            "batch:202, loss:0.4929399788379669\n",
            "batch:203, loss:0.5562606453895569\n",
            "batch:204, loss:0.4396756589412689\n",
            "batch:205, loss:0.44474804401397705\n",
            "batch:206, loss:0.6298946738243103\n",
            "batch:207, loss:0.4294491708278656\n",
            "batch:208, loss:0.4773273169994354\n",
            "batch:209, loss:0.3895842432975769\n",
            "batch:210, loss:0.4517170190811157\n",
            "batch:211, loss:0.5234298706054688\n",
            "batch:212, loss:0.486644983291626\n",
            "batch:213, loss:0.47519850730895996\n",
            "batch:214, loss:0.4135024845600128\n",
            "batch:215, loss:0.7784162759780884\n",
            "batch:216, loss:0.4305448532104492\n",
            "batch:217, loss:0.4662727117538452\n",
            "batch:218, loss:0.6136265397071838\n",
            "batch:219, loss:0.6261013150215149\n",
            "batch:220, loss:0.38507580757141113\n",
            "batch:221, loss:0.2687477469444275\n",
            "batch:222, loss:0.5160530209541321\n",
            "batch:223, loss:0.8273530602455139\n",
            "batch:224, loss:0.4380004405975342\n",
            "batch:225, loss:0.5325811505317688\n",
            "batch:226, loss:0.4350281059741974\n",
            "batch:227, loss:0.4001404941082001\n",
            "batch:228, loss:0.49406254291534424\n",
            "batch:229, loss:0.5203972458839417\n",
            "batch:230, loss:0.43842118978500366\n",
            "batch:231, loss:0.469146192073822\n",
            "batch:232, loss:0.6016570329666138\n",
            "batch:233, loss:0.5599894523620605\n",
            "batch:234, loss:0.36918309330940247\n",
            "batch:235, loss:0.4656215012073517\n",
            "batch:236, loss:0.5403498411178589\n",
            "batch:237, loss:0.5868343114852905\n",
            "batch:238, loss:0.5057192444801331\n",
            "batch:239, loss:0.5899191498756409\n",
            "batch:240, loss:0.5753427743911743\n",
            "batch:241, loss:0.46883201599121094\n",
            "batch:242, loss:0.40685704350471497\n",
            "batch:243, loss:0.616362988948822\n",
            "batch:244, loss:0.30957552790641785\n",
            "batch:245, loss:0.442492812871933\n",
            "batch:246, loss:0.5480002164840698\n",
            "batch:247, loss:0.6157276630401611\n",
            "batch:248, loss:0.5255085229873657\n",
            "batch:249, loss:0.3613114058971405\n",
            "batch:250, loss:0.5040140748023987\n",
            "batch:251, loss:0.4224618375301361\n",
            "batch:252, loss:0.5375947952270508\n",
            "batch:253, loss:0.37004220485687256\n",
            "batch:254, loss:0.4969930648803711\n",
            "batch:255, loss:0.5751765966415405\n",
            "batch:256, loss:0.598964273929596\n",
            "batch:257, loss:0.4498651921749115\n",
            "batch:258, loss:0.5239130854606628\n",
            "batch:259, loss:0.508062481880188\n",
            "batch:260, loss:0.48375481367111206\n",
            "batch:261, loss:0.5087066292762756\n",
            "batch:262, loss:0.3963581323623657\n",
            "batch:263, loss:0.5859733819961548\n",
            "batch:264, loss:0.556607723236084\n",
            "batch:265, loss:0.5291945338249207\n",
            "batch:266, loss:0.38855019211769104\n",
            "batch:267, loss:0.5297718048095703\n",
            "batch:268, loss:0.4197031557559967\n",
            "batch:269, loss:0.5041936635971069\n",
            "batch:270, loss:0.4324904978275299\n",
            "batch:271, loss:0.407791405916214\n",
            "batch:272, loss:0.42357388138771057\n",
            "batch:273, loss:0.3773552179336548\n",
            "batch:274, loss:0.4690721333026886\n",
            "batch:275, loss:0.3981405794620514\n",
            "batch:276, loss:0.5335769057273865\n",
            "batch:277, loss:0.41940557956695557\n",
            "batch:278, loss:0.3645554482936859\n",
            "batch:279, loss:0.4180256128311157\n",
            "batch:280, loss:0.512360155582428\n",
            "batch:281, loss:0.4160005450248718\n",
            "batch:282, loss:0.4681033790111542\n",
            "batch:283, loss:0.4618263840675354\n",
            "batch:284, loss:0.6060935258865356\n",
            "batch:285, loss:0.39907291531562805\n",
            "batch:286, loss:0.4368434250354767\n",
            "batch:287, loss:0.5396018624305725\n",
            "batch:288, loss:0.5048186779022217\n",
            "batch:289, loss:0.48312440514564514\n",
            "batch:290, loss:0.44692009687423706\n",
            "batch:291, loss:0.5889586210250854\n",
            "batch:292, loss:0.3255767524242401\n",
            "batch:293, loss:0.7081673741340637\n",
            "batch:294, loss:0.5907544493675232\n",
            "batch:295, loss:0.5008413791656494\n",
            "batch:296, loss:0.7596467733383179\n",
            "batch:297, loss:0.5895472168922424\n",
            "batch:298, loss:0.6429500579833984\n",
            "batch:299, loss:0.4932352602481842\n",
            "batch:300, loss:0.49153444170951843\n",
            "batch:301, loss:0.7004889845848083\n",
            "batch:302, loss:0.6012226939201355\n",
            "batch:303, loss:0.6545760035514832\n",
            "batch:304, loss:0.6292842030525208\n",
            "batch:305, loss:0.430403470993042\n",
            "batch:306, loss:0.3853274881839752\n",
            "batch:307, loss:0.571276068687439\n",
            "batch:308, loss:0.5952777862548828\n",
            "batch:309, loss:0.7537972927093506\n",
            "batch:310, loss:0.37751340866088867\n",
            "batch:311, loss:0.4080311954021454\n",
            "batch:312, loss:0.543795645236969\n",
            "batch:313, loss:0.6219232082366943\n",
            "batch:314, loss:0.5089962482452393\n",
            "batch:315, loss:0.3487420976161957\n",
            "batch:316, loss:0.3575907349586487\n",
            "batch:317, loss:0.7303188443183899\n",
            "batch:318, loss:0.5831059813499451\n",
            "batch:319, loss:0.5489433407783508\n",
            "batch:320, loss:0.3322896361351013\n",
            "batch:321, loss:0.5816587805747986\n",
            "batch:322, loss:0.3496026396751404\n",
            "batch:323, loss:0.49933454394340515\n",
            "batch:324, loss:0.531478226184845\n",
            "batch:325, loss:0.5537381768226624\n",
            "batch:326, loss:0.5163635611534119\n",
            "batch:327, loss:0.27303799986839294\n",
            "batch:328, loss:0.45720458030700684\n",
            "batch:329, loss:0.446906715631485\n",
            "batch:330, loss:0.3997161090373993\n",
            "batch:331, loss:0.3867364823818207\n",
            "batch:332, loss:0.4800920784473419\n",
            "batch:333, loss:0.3922007083892822\n",
            "batch:334, loss:0.4514431357383728\n",
            "batch:335, loss:0.34549403190612793\n",
            "batch:336, loss:0.6338339447975159\n",
            "batch:337, loss:0.640423595905304\n",
            "batch:338, loss:0.4822347164154053\n",
            "batch:339, loss:0.5170304775238037\n",
            "batch:340, loss:0.4508899450302124\n",
            "batch:341, loss:0.5241910219192505\n",
            "batch:342, loss:0.3264271914958954\n",
            "batch:343, loss:0.7136213183403015\n",
            "batch:344, loss:0.5347575545310974\n",
            "batch:345, loss:0.793070912361145\n",
            "batch:346, loss:0.5980775356292725\n",
            "batch:347, loss:0.4983333349227905\n",
            "batch:348, loss:0.37560439109802246\n",
            "batch:349, loss:0.49762940406799316\n",
            "batch:350, loss:0.791452169418335\n",
            "batch:351, loss:0.4468059241771698\n",
            "batch:352, loss:0.4824658930301666\n",
            "batch:353, loss:0.47827383875846863\n",
            "batch:354, loss:0.4765002131462097\n",
            "batch:355, loss:0.5641593933105469\n",
            "batch:356, loss:0.5473482012748718\n",
            "batch:357, loss:0.5818473696708679\n",
            "batch:358, loss:0.47358793020248413\n",
            "batch:359, loss:0.42509159445762634\n",
            "batch:360, loss:0.4244596064090729\n",
            "batch:361, loss:0.461061954498291\n",
            "batch:362, loss:0.5509229302406311\n",
            "batch:363, loss:0.5042498707771301\n",
            "batch:364, loss:0.3754103481769562\n",
            "batch:365, loss:0.4304889440536499\n",
            "batch:366, loss:0.45457497239112854\n",
            "batch:367, loss:0.4603120684623718\n",
            "batch:368, loss:0.451882541179657\n",
            "batch:369, loss:0.4528215229511261\n",
            "batch:370, loss:0.5621607899665833\n",
            "batch:371, loss:0.4048125445842743\n",
            "batch:372, loss:0.319419264793396\n",
            "batch:373, loss:0.3957643508911133\n",
            "batch:374, loss:0.3676316738128662\n",
            "batch:375, loss:0.5421346426010132\n",
            "batch:376, loss:0.6897795796394348\n",
            "batch:377, loss:0.5892153978347778\n",
            "batch:378, loss:0.40231141448020935\n",
            "batch:379, loss:0.6067870259284973\n",
            "batch:380, loss:0.3220182955265045\n",
            "batch:381, loss:0.6547585129737854\n",
            "batch:382, loss:0.33606112003326416\n",
            "batch:383, loss:0.39922016859054565\n",
            "batch:384, loss:0.6405072808265686\n",
            "batch:385, loss:0.34883368015289307\n",
            "batch:386, loss:0.3841942548751831\n",
            "batch:387, loss:0.47966882586479187\n",
            "batch:388, loss:0.5381565093994141\n",
            "batch:389, loss:0.2646901607513428\n",
            "batch:390, loss:0.4780592918395996\n",
            "batch:391, loss:0.4506828188896179\n",
            "batch:392, loss:0.40564805269241333\n",
            "batch:393, loss:0.45731210708618164\n",
            "batch:394, loss:0.5319899320602417\n",
            "batch:395, loss:0.49903130531311035\n",
            "batch:396, loss:0.6613186597824097\n",
            "batch:397, loss:0.5926750302314758\n",
            "batch:398, loss:0.42599713802337646\n",
            "batch:399, loss:0.5230580568313599\n",
            "batch:400, loss:0.42284059524536133\n",
            "batch:401, loss:0.6021234393119812\n",
            "batch:402, loss:0.6188668608665466\n",
            "batch:403, loss:0.475972056388855\n",
            "batch:404, loss:0.5391356945037842\n",
            "batch:405, loss:0.5813155770301819\n",
            "batch:406, loss:0.5954249501228333\n",
            "batch:407, loss:0.33407142758369446\n",
            "batch:408, loss:0.4357461631298065\n",
            "batch:409, loss:0.5391536951065063\n",
            "batch:410, loss:0.5216858386993408\n",
            "batch:411, loss:0.49717065691947937\n",
            "batch:412, loss:0.4512718617916107\n",
            "batch:413, loss:0.5619947910308838\n",
            "batch:414, loss:0.5494592785835266\n",
            "batch:415, loss:0.4428485333919525\n",
            "batch:416, loss:0.7425172924995422\n",
            "batch:417, loss:0.28711840510368347\n",
            "batch:418, loss:0.3661227822303772\n",
            "batch:419, loss:0.3729049861431122\n",
            "batch:420, loss:0.5694808959960938\n",
            "batch:421, loss:0.34337225556373596\n",
            "batch:422, loss:0.6788651943206787\n",
            "batch:423, loss:0.5589988231658936\n",
            "batch:424, loss:0.4312162399291992\n",
            "batch:425, loss:0.46191176772117615\n",
            "batch:426, loss:0.31379368901252747\n",
            "batch:427, loss:0.5462406873703003\n",
            "batch:428, loss:0.3458009362220764\n",
            "batch:429, loss:0.4176377058029175\n",
            "batch:430, loss:0.39433878660202026\n",
            "batch:431, loss:0.49879470467567444\n",
            "batch:432, loss:0.554313063621521\n",
            "batch:433, loss:0.37796249985694885\n",
            "batch:434, loss:0.4715878963470459\n",
            "batch:435, loss:0.4556241035461426\n",
            "batch:436, loss:0.5866597890853882\n",
            "batch:437, loss:0.6726447939872742\n",
            "batch:438, loss:0.7558524012565613\n",
            "batch:439, loss:0.6974251866340637\n",
            "batch:440, loss:0.40224525332450867\n",
            "batch:441, loss:0.6417176723480225\n",
            "batch:442, loss:0.4656282663345337\n",
            "batch:443, loss:0.5420645475387573\n",
            "batch:444, loss:0.6284487247467041\n",
            "batch:445, loss:0.5147828459739685\n",
            "batch:446, loss:0.8216847777366638\n",
            "batch:447, loss:0.4294596016407013\n",
            "batch:448, loss:0.5095074772834778\n",
            "batch:449, loss:0.5241734385490417\n",
            "batch:450, loss:0.4779690206050873\n",
            "batch:451, loss:0.5501611232757568\n",
            "batch:452, loss:0.5154383778572083\n",
            "batch:453, loss:0.43024277687072754\n",
            "batch:454, loss:0.5210245847702026\n",
            "batch:455, loss:0.2783898413181305\n",
            "batch:456, loss:0.3779541254043579\n",
            "batch:457, loss:0.5360730290412903\n",
            "batch:458, loss:0.4828655421733856\n",
            "batch:459, loss:0.4281798303127289\n",
            "batch:460, loss:0.5125888586044312\n",
            "batch:461, loss:0.6206531524658203\n",
            "batch:462, loss:0.47245872020721436\n",
            "batch:463, loss:0.4357491135597229\n",
            "batch:464, loss:0.4174615144729614\n",
            "batch:465, loss:0.45888426899909973\n",
            "batch:466, loss:0.4570772349834442\n",
            "batch:467, loss:0.6049765944480896\n",
            "batch:468, loss:0.5744835734367371\n",
            "batch:469, loss:0.28186023235321045\n",
            "batch:470, loss:0.38501566648483276\n",
            "batch:471, loss:0.5191885232925415\n",
            "batch:472, loss:0.49934351444244385\n",
            "batch:473, loss:0.4259169101715088\n",
            "batch:474, loss:0.4220132529735565\n",
            "batch:475, loss:0.5220001339912415\n",
            "batch:476, loss:0.4614902436733246\n",
            "batch:477, loss:0.508188784122467\n",
            "batch:478, loss:0.40849632024765015\n",
            "batch:479, loss:0.4170970618724823\n",
            "batch:480, loss:0.4526948630809784\n",
            "batch:481, loss:0.4957635700702667\n",
            "batch:482, loss:0.40975311398506165\n",
            "batch:483, loss:0.5269182324409485\n",
            "batch:484, loss:0.5490115880966187\n",
            "batch:485, loss:0.3005596995353699\n",
            "batch:486, loss:0.38215529918670654\n",
            "batch:487, loss:0.42417892813682556\n",
            "batch:488, loss:0.4001094400882721\n",
            "batch:489, loss:0.3498256206512451\n",
            "batch:490, loss:0.5026582479476929\n",
            "batch:491, loss:0.48267632722854614\n",
            "batch:492, loss:0.44531479477882385\n",
            "batch:493, loss:0.5799474716186523\n",
            "batch:494, loss:0.6269608736038208\n",
            "batch:495, loss:0.42884352803230286\n",
            "batch:496, loss:0.39739641547203064\n",
            "batch:497, loss:0.39997902512550354\n",
            "batch:498, loss:0.46638646721839905\n",
            "batch:499, loss:0.4864964187145233\n",
            "batch:500, loss:0.4381240904331207\n",
            "batch:501, loss:0.7353327870368958\n",
            "batch:502, loss:0.3008806109428406\n",
            "batch:503, loss:0.5901265740394592\n",
            "batch:504, loss:0.32121890783309937\n",
            "batch:505, loss:0.42622819542884827\n",
            "batch:506, loss:0.568952202796936\n",
            "batch:507, loss:0.5357635617256165\n",
            "batch:508, loss:0.29473352432250977\n",
            "batch:509, loss:0.6803847551345825\n",
            "batch:510, loss:0.5861829519271851\n",
            "batch:511, loss:0.5282942056655884\n",
            "batch:512, loss:0.5245010256767273\n",
            "batch:513, loss:0.3265998363494873\n",
            "batch:514, loss:0.32551631331443787\n",
            "batch:515, loss:0.6749696731567383\n",
            "batch:516, loss:0.4670124650001526\n",
            "batch:517, loss:0.3489132821559906\n",
            "batch:518, loss:0.507280170917511\n",
            "batch:519, loss:0.3959839344024658\n",
            "batch:520, loss:0.5338852405548096\n",
            "batch:521, loss:0.7359850406646729\n",
            "batch:522, loss:0.29172149300575256\n",
            "batch:523, loss:0.4490591883659363\n",
            "batch:524, loss:0.42661985754966736\n",
            "batch:525, loss:0.5663432478904724\n",
            "batch:526, loss:0.39896947145462036\n",
            "batch:527, loss:0.42682355642318726\n",
            "batch:528, loss:0.5058067440986633\n",
            "batch:529, loss:0.3299976885318756\n",
            "batch:530, loss:0.5177438259124756\n",
            "batch:531, loss:0.48480939865112305\n",
            "batch:532, loss:0.37162020802497864\n",
            "batch:533, loss:0.4442918300628662\n",
            "batch:534, loss:0.6836021542549133\n",
            "batch:535, loss:0.5642180442810059\n",
            "batch:536, loss:0.466699481010437\n",
            "batch:537, loss:0.48346880078315735\n",
            "batch:538, loss:0.4571133255958557\n",
            "batch:539, loss:0.36766788363456726\n",
            "batch:540, loss:0.5016506314277649\n",
            "batch:541, loss:0.30317774415016174\n",
            "batch:542, loss:0.48816242814064026\n",
            "batch:543, loss:0.47462379932403564\n",
            "batch:544, loss:0.4912970960140228\n",
            "batch:545, loss:0.46140438318252563\n",
            "batch:546, loss:0.4855864346027374\n",
            "batch:547, loss:0.4845123887062073\n",
            "batch:548, loss:0.5098291039466858\n",
            "batch:549, loss:0.5648089051246643\n",
            "batch:550, loss:0.44208717346191406\n",
            "batch:551, loss:0.28055045008659363\n",
            "batch:552, loss:0.26608899235725403\n",
            "batch:553, loss:0.5704059600830078\n",
            "batch:554, loss:0.4930014908313751\n",
            "batch:555, loss:0.4548508822917938\n",
            "batch:556, loss:0.4417123794555664\n",
            "batch:557, loss:0.31986984610557556\n",
            "batch:558, loss:0.6454004645347595\n",
            "batch:559, loss:0.4947323799133301\n",
            "batch:560, loss:0.5316628217697144\n",
            "batch:561, loss:0.478824257850647\n",
            "batch:562, loss:0.4519478678703308\n",
            "batch:563, loss:0.5102512240409851\n",
            "batch:564, loss:0.49332019686698914\n",
            "batch:565, loss:0.43538036942481995\n",
            "batch:566, loss:0.46469876170158386\n",
            "batch:567, loss:0.4714009761810303\n",
            "batch:568, loss:0.48635026812553406\n",
            "batch:569, loss:0.7271578311920166\n",
            "batch:570, loss:0.5776955485343933\n",
            "batch:571, loss:0.6082084774971008\n",
            "batch:572, loss:0.4754736125469208\n",
            "batch:573, loss:0.5178372263908386\n",
            "batch:574, loss:0.5435487627983093\n",
            "batch:575, loss:0.7223585247993469\n",
            "batch:576, loss:0.7192442417144775\n",
            "batch:577, loss:0.5120673179626465\n",
            "batch:578, loss:0.693466305732727\n",
            "batch:579, loss:0.42539963126182556\n",
            "batch:580, loss:0.3298353850841522\n",
            "batch:581, loss:0.36661389470100403\n",
            "batch:582, loss:0.5287219285964966\n",
            "batch:583, loss:0.4743860960006714\n",
            "batch:584, loss:0.6018127799034119\n",
            "batch:585, loss:0.45085108280181885\n",
            "batch:586, loss:0.4917069375514984\n",
            "batch:587, loss:0.45636335015296936\n",
            "batch:588, loss:0.3959493637084961\n",
            "batch:589, loss:0.46538102626800537\n",
            "batch:590, loss:0.6994582414627075\n",
            "batch:591, loss:0.49070754647254944\n",
            "batch:592, loss:0.5249099135398865\n",
            "batch:593, loss:0.47957828640937805\n",
            "batch:594, loss:0.4989526569843292\n",
            "batch:595, loss:0.5831219553947449\n",
            "batch:596, loss:0.48768359422683716\n",
            "batch:597, loss:0.4412215054035187\n",
            "batch:598, loss:0.6016225814819336\n",
            "batch:599, loss:0.463618665933609\n",
            "batch:600, loss:0.5547511577606201\n",
            "batch:601, loss:0.5523739457130432\n",
            "batch:602, loss:0.511243462562561\n",
            "batch:603, loss:0.44118958711624146\n",
            "batch:604, loss:0.39532265067100525\n",
            "batch:605, loss:0.4124116897583008\n",
            "batch:606, loss:0.45148757100105286\n",
            "batch:607, loss:0.8044291138648987\n",
            "batch:608, loss:0.49361729621887207\n",
            "batch:609, loss:0.29725655913352966\n",
            "batch:610, loss:0.499602347612381\n",
            "batch:611, loss:0.5877895951271057\n",
            "batch:612, loss:0.33007872104644775\n",
            "batch:613, loss:0.45478594303131104\n",
            "batch:614, loss:0.4814097285270691\n",
            "batch:615, loss:0.3132235109806061\n",
            "batch:616, loss:0.6091712713241577\n",
            "batch:617, loss:0.4645750820636749\n",
            "batch:618, loss:0.34162402153015137\n",
            "batch:619, loss:0.5095011591911316\n",
            "batch:620, loss:0.5379015207290649\n",
            "batch:621, loss:0.4985549747943878\n",
            "batch:622, loss:0.3047545850276947\n",
            "batch:623, loss:0.47224321961402893\n",
            "batch:624, loss:0.47249576449394226\n",
            "batch:625, loss:0.63515704870224\n",
            "batch:626, loss:0.44271600246429443\n",
            "batch:627, loss:0.313068151473999\n",
            "batch:628, loss:0.499673455953598\n",
            "batch:629, loss:0.42816367745399475\n",
            "batch:630, loss:0.4770674705505371\n",
            "batch:631, loss:0.34635961055755615\n",
            "batch:632, loss:0.4252191185951233\n",
            "batch:633, loss:0.6506639719009399\n",
            "batch:634, loss:0.3592921495437622\n",
            "batch:635, loss:0.47012412548065186\n",
            "batch:636, loss:0.4961429536342621\n",
            "batch:637, loss:0.3970268964767456\n",
            "batch:638, loss:0.4104047417640686\n",
            "batch:639, loss:0.44699954986572266\n",
            "batch:640, loss:0.64586341381073\n",
            "batch:641, loss:0.6620706915855408\n",
            "batch:642, loss:0.6882008910179138\n",
            "batch:643, loss:0.37227699160575867\n",
            "batch:644, loss:0.40698954463005066\n",
            "batch:645, loss:0.5752434134483337\n",
            "batch:646, loss:0.41698700189590454\n",
            "batch:647, loss:0.409565269947052\n",
            "batch:648, loss:0.6475076079368591\n",
            "batch:649, loss:0.44933775067329407\n",
            "batch:650, loss:0.6358988881111145\n",
            "batch:651, loss:0.5087379217147827\n",
            "batch:652, loss:0.5161978602409363\n",
            "batch:653, loss:0.558269202709198\n",
            "batch:654, loss:0.5148993730545044\n",
            "batch:655, loss:0.45844659209251404\n",
            "batch:656, loss:0.40734899044036865\n",
            "batch:657, loss:0.4189698100090027\n",
            "batch:658, loss:0.504389226436615\n",
            "batch:659, loss:0.5753782987594604\n",
            "batch:660, loss:0.4079064428806305\n",
            "batch:661, loss:0.6131103038787842\n",
            "batch:662, loss:0.47878900170326233\n",
            "batch:663, loss:0.5205625891685486\n",
            "batch:664, loss:0.4571325182914734\n",
            "batch:665, loss:0.5540078282356262\n",
            "batch:666, loss:0.5111905932426453\n",
            "batch:667, loss:0.5700530409812927\n",
            "batch:668, loss:0.46274569630622864\n",
            "batch:669, loss:0.5154682397842407\n",
            "batch:670, loss:0.41620898246765137\n",
            "batch:671, loss:0.29880595207214355\n",
            "batch:672, loss:0.5072985887527466\n",
            "batch:673, loss:0.4886004328727722\n",
            "batch:674, loss:0.4736870229244232\n",
            "batch:675, loss:0.5196560621261597\n",
            "batch:676, loss:0.7045770883560181\n",
            "batch:677, loss:0.4228038787841797\n",
            "batch:678, loss:0.3982491195201874\n",
            "batch:679, loss:0.5621699094772339\n",
            "batch:680, loss:0.3938829004764557\n",
            "batch:681, loss:0.4356430470943451\n",
            "batch:682, loss:0.6682727336883545\n",
            "batch:683, loss:0.3651188611984253\n",
            "batch:684, loss:0.3846035301685333\n",
            "batch:685, loss:0.5011600852012634\n",
            "batch:686, loss:0.35259321331977844\n",
            "batch:687, loss:0.26845258474349976\n",
            "batch:688, loss:0.7012509107589722\n",
            "batch:689, loss:0.5420162081718445\n",
            "batch:690, loss:0.4224318265914917\n",
            "batch:691, loss:0.38511180877685547\n",
            "batch:692, loss:0.5456379652023315\n",
            "batch:693, loss:0.40633657574653625\n",
            "batch:694, loss:0.5347934365272522\n",
            "batch:695, loss:0.39018887281417847\n",
            "batch:696, loss:0.41144537925720215\n",
            "batch:697, loss:0.4292103350162506\n",
            "batch:698, loss:0.4350055456161499\n",
            "batch:699, loss:0.3792995512485504\n",
            "batch:700, loss:0.4601372480392456\n",
            "batch:701, loss:0.4219772517681122\n",
            "batch:702, loss:0.27876776456832886\n",
            "batch:703, loss:0.5568450689315796\n",
            "batch:704, loss:0.3616018295288086\n",
            "batch:705, loss:0.5322009325027466\n",
            "batch:706, loss:0.3354214131832123\n",
            "batch:707, loss:0.7309913635253906\n",
            "batch:708, loss:0.6223504543304443\n",
            "batch:709, loss:0.6073523759841919\n",
            "batch:710, loss:0.45249393582344055\n",
            "batch:711, loss:0.5102949142456055\n",
            "batch:712, loss:0.4574635326862335\n",
            "batch:713, loss:0.506935715675354\n",
            "batch:714, loss:0.3322894871234894\n",
            "batch:715, loss:0.4371691942214966\n",
            "batch:716, loss:0.5339017510414124\n",
            "batch:717, loss:0.4072846472263336\n",
            "batch:718, loss:0.5689645409584045\n",
            "batch:719, loss:0.5104835629463196\n",
            "batch:720, loss:0.3545905351638794\n",
            "batch:721, loss:0.4008280336856842\n",
            "batch:722, loss:0.3005031645298004\n",
            "batch:723, loss:0.5117889046669006\n",
            "batch:724, loss:0.5028758645057678\n",
            "batch:725, loss:0.47733303904533386\n",
            "batch:726, loss:0.6897154450416565\n",
            "batch:727, loss:0.4675876796245575\n",
            "batch:728, loss:0.5183035731315613\n",
            "batch:729, loss:0.31985971331596375\n",
            "batch:730, loss:0.5608265399932861\n",
            "batch:731, loss:0.3324778974056244\n",
            "batch:732, loss:0.5851356387138367\n",
            "batch:733, loss:0.5070610046386719\n",
            "batch:734, loss:0.4661325514316559\n",
            "batch:735, loss:0.5718262791633606\n",
            "batch:736, loss:0.3816426396369934\n",
            "batch:737, loss:0.3958100378513336\n",
            "batch:738, loss:0.40804794430732727\n",
            "batch:739, loss:0.6538040041923523\n",
            "batch:740, loss:0.5029156804084778\n",
            "batch:741, loss:0.374763160943985\n",
            "batch:742, loss:0.4086719751358032\n",
            "batch:743, loss:0.6461501121520996\n",
            "batch:744, loss:0.47483763098716736\n",
            "batch:745, loss:0.6761766672134399\n",
            "batch:746, loss:0.38067612051963806\n",
            "batch:747, loss:0.4718865156173706\n",
            "batch:748, loss:0.4766795337200165\n",
            "batch:749, loss:0.6814334392547607\n",
            "batch:750, loss:0.5286461710929871\n",
            "batch:751, loss:0.39642617106437683\n",
            "batch:752, loss:0.44347038865089417\n",
            "batch:753, loss:0.5996609330177307\n",
            "batch:754, loss:0.6250326037406921\n",
            "batch:755, loss:0.49238350987434387\n",
            "batch:756, loss:0.8625381588935852\n",
            "batch:757, loss:0.5345008373260498\n",
            "batch:758, loss:0.5355476140975952\n",
            "batch:759, loss:0.6672857999801636\n",
            "batch:760, loss:0.4528037905693054\n",
            "batch:761, loss:0.5782982110977173\n",
            "batch:762, loss:0.4076720178127289\n",
            "batch:763, loss:0.5543395280838013\n",
            "batch:764, loss:0.5168206691741943\n",
            "batch:765, loss:0.4956575930118561\n",
            "batch:766, loss:0.44682857394218445\n",
            "batch:767, loss:0.43773311376571655\n",
            "batch:768, loss:0.6850742697715759\n",
            "batch:769, loss:0.5151990056037903\n",
            "batch:770, loss:0.4713560938835144\n",
            "batch:771, loss:0.4793526530265808\n",
            "batch:772, loss:0.40243402123451233\n",
            "batch:773, loss:0.4826669991016388\n",
            "batch:774, loss:0.40821391344070435\n",
            "batch:775, loss:0.5205943584442139\n",
            "batch:776, loss:0.6038954257965088\n",
            "batch:777, loss:0.3304986357688904\n",
            "batch:778, loss:0.6076419353485107\n",
            "batch:779, loss:0.3011360168457031\n",
            "batch:780, loss:0.3990655839443207\n",
            "batch:781, loss:0.3644334673881531\n",
            "batch:782, loss:0.556747555732727\n",
            "batch:783, loss:0.4123174846172333\n",
            "batch:784, loss:0.5634917616844177\n",
            "batch:785, loss:0.5653759241104126\n",
            "batch:786, loss:0.3982119560241699\n",
            "batch:787, loss:0.5280352830886841\n",
            "batch:788, loss:0.43755871057510376\n",
            "batch:789, loss:0.45190927386283875\n",
            "batch:790, loss:0.47659531235694885\n",
            "batch:791, loss:0.40579330921173096\n",
            "batch:792, loss:0.49373379349708557\n",
            "batch:793, loss:0.5315742492675781\n",
            "batch:794, loss:0.5664658546447754\n",
            "batch:795, loss:0.35371047258377075\n",
            "batch:796, loss:0.5534213185310364\n",
            "batch:797, loss:0.663425087928772\n",
            "batch:798, loss:0.46581774950027466\n",
            "batch:799, loss:0.4267386496067047\n",
            "batch:800, loss:0.2930601239204407\n",
            "batch:801, loss:0.47454598546028137\n",
            "batch:802, loss:0.45294681191444397\n",
            "batch:803, loss:0.5433486700057983\n",
            "batch:804, loss:0.4572473466396332\n",
            "batch:805, loss:0.5285173654556274\n",
            "batch:806, loss:0.5671035647392273\n",
            "batch:807, loss:0.40546661615371704\n",
            "batch:808, loss:0.4455111622810364\n",
            "batch:809, loss:0.42779675126075745\n",
            "batch:810, loss:0.34903985261917114\n",
            "batch:811, loss:0.33507436513900757\n",
            "batch:812, loss:0.540227472782135\n",
            "batch:813, loss:0.4772967994213104\n",
            "batch:814, loss:0.5693437457084656\n",
            "batch:815, loss:0.5506986379623413\n",
            "batch:816, loss:0.46242836117744446\n",
            "batch:817, loss:0.4390120208263397\n",
            "batch:818, loss:0.537861704826355\n",
            "batch:819, loss:0.5318794846534729\n",
            "batch:820, loss:0.4441370964050293\n",
            "batch:821, loss:0.6700692772865295\n",
            "batch:822, loss:0.5038436651229858\n",
            "batch:823, loss:0.37530800700187683\n",
            "batch:824, loss:0.4027257561683655\n",
            "batch:825, loss:0.3506917357444763\n",
            "batch:826, loss:0.4523172080516815\n",
            "batch:827, loss:0.4781637489795685\n",
            "batch:828, loss:0.588097095489502\n",
            "batch:829, loss:0.4361870288848877\n",
            "batch:830, loss:0.5795052647590637\n",
            "batch:831, loss:0.42624449729919434\n",
            "batch:832, loss:0.5936344861984253\n",
            "batch:833, loss:0.48162841796875\n",
            "batch:834, loss:0.3773488402366638\n",
            "batch:835, loss:0.4601835012435913\n",
            "batch:836, loss:0.4174962639808655\n",
            "batch:837, loss:0.3844579756259918\n",
            "batch:838, loss:0.4424554109573364\n",
            "batch:839, loss:0.32956159114837646\n",
            "batch:840, loss:0.29945889115333557\n",
            "batch:841, loss:0.4236750602722168\n",
            "batch:842, loss:0.3842383325099945\n",
            "batch:843, loss:0.318873792886734\n",
            "batch:844, loss:0.48975351452827454\n",
            "batch:845, loss:0.38012370467185974\n",
            "batch:846, loss:0.6746140122413635\n",
            "batch:847, loss:0.48624187707901\n",
            "batch:848, loss:0.4006098508834839\n",
            "batch:849, loss:0.5655615329742432\n",
            "batch:850, loss:0.31813523173332214\n",
            "batch:851, loss:0.3712833523750305\n",
            "batch:852, loss:0.4991912543773651\n",
            "batch:853, loss:0.33437788486480713\n",
            "batch:854, loss:0.46635574102401733\n",
            "batch:855, loss:0.40504297614097595\n",
            "batch:856, loss:0.5264754295349121\n",
            "batch:857, loss:0.39303988218307495\n",
            "batch:858, loss:0.282202810049057\n",
            "batch:859, loss:0.5453838109970093\n",
            "batch:860, loss:0.7707222700119019\n",
            "batch:861, loss:0.4549199640750885\n",
            "batch:862, loss:0.4003555178642273\n",
            "batch:863, loss:0.4265970289707184\n",
            "batch:864, loss:0.403624027967453\n",
            "batch:865, loss:0.38943609595298767\n",
            "batch:866, loss:0.4427375793457031\n",
            "batch:867, loss:0.7008900046348572\n",
            "batch:868, loss:0.3980104327201843\n",
            "batch:869, loss:0.25155580043792725\n",
            "batch:870, loss:0.43189677596092224\n",
            "batch:871, loss:0.4952384829521179\n",
            "batch:872, loss:0.4783271551132202\n",
            "batch:873, loss:0.3941471576690674\n",
            "batch:874, loss:0.5188441872596741\n",
            "batch:875, loss:0.5672459006309509\n",
            "batch:876, loss:0.3865223228931427\n",
            "batch:877, loss:0.5041853785514832\n",
            "batch:878, loss:0.34883663058280945\n",
            "batch:879, loss:0.474851131439209\n",
            "batch:880, loss:0.55797278881073\n",
            "batch:881, loss:0.44629648327827454\n",
            "batch:882, loss:0.45492833852767944\n",
            "batch:883, loss:0.6129939556121826\n",
            "batch:884, loss:0.6245505213737488\n",
            "batch:885, loss:0.37592813372612\n",
            "batch:886, loss:0.418163537979126\n",
            "batch:887, loss:0.45904478430747986\n",
            "batch:888, loss:0.6124574542045593\n",
            "batch:889, loss:0.45761069655418396\n",
            "batch:890, loss:0.43736186623573303\n",
            "batch:891, loss:0.48673442006111145\n",
            "batch:892, loss:0.40334880352020264\n",
            "batch:893, loss:0.8013625144958496\n",
            "batch:894, loss:0.5366343259811401\n",
            "batch:895, loss:0.5923365354537964\n",
            "batch:896, loss:0.4020756781101227\n",
            "batch:897, loss:0.3161953091621399\n",
            "batch:898, loss:0.49697476625442505\n",
            "batch:899, loss:0.3744416832923889\n",
            "batch:900, loss:0.3524014949798584\n",
            "batch:901, loss:0.293144553899765\n",
            "batch:902, loss:0.42868608236312866\n",
            "batch:903, loss:0.3702163100242615\n",
            "batch:904, loss:0.4198760688304901\n",
            "batch:905, loss:0.4540631175041199\n",
            "batch:906, loss:0.2618541717529297\n",
            "batch:907, loss:0.48395949602127075\n",
            "batch:908, loss:0.5210151076316833\n",
            "batch:909, loss:0.531746506690979\n",
            "batch:910, loss:0.4577115774154663\n",
            "batch:911, loss:0.5439916253089905\n",
            "batch:912, loss:0.40685826539993286\n",
            "batch:913, loss:0.27833154797554016\n",
            "batch:914, loss:0.5968088507652283\n",
            "batch:915, loss:0.4388863444328308\n",
            "batch:916, loss:0.4028451442718506\n",
            "batch:917, loss:0.5548349022865295\n",
            "batch:918, loss:0.48593342304229736\n",
            "batch:919, loss:0.39236027002334595\n",
            "batch:920, loss:0.5802857875823975\n",
            "batch:921, loss:0.43590497970581055\n",
            "batch:922, loss:0.3012669086456299\n",
            "batch:923, loss:0.5921518802642822\n",
            "batch:924, loss:0.36148005723953247\n",
            "batch:925, loss:0.8283287286758423\n",
            "batch:926, loss:0.5127347707748413\n",
            "batch:927, loss:0.3448520302772522\n",
            "batch:928, loss:0.5442491173744202\n",
            "batch:929, loss:0.4318482577800751\n",
            "batch:930, loss:0.4841291904449463\n",
            "batch:931, loss:0.40975096821784973\n",
            "batch:932, loss:0.5879741311073303\n",
            "batch:933, loss:0.42969468235969543\n",
            "batch:934, loss:0.5321981906890869\n",
            "batch:935, loss:0.6206340789794922\n",
            "batch:936, loss:0.6857048869132996\n",
            "batch:937, loss:0.5350091457366943\n",
            "Training loss: 0.48916289378712174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OVDFUnzFGpr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWf1SWuiFGmn"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ_QUMXLFGjr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}