{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8da3d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class ConvNetPL(pl.LightningModule):\n",
    "    def __init__(self, pretrained_model_name, pretrained_model_path, num_classes, base_lr, batch_size, train_path, test_path):\n",
    "        super(ConvNetPL, self).__init__()\n",
    "\n",
    "        self.pretrained_model_name = pretrained_model_name\n",
    "        self.pretrained_model_path = pretrained_model_path\n",
    "        self.num_classes = num_classes\n",
    "        self.lr = base_lr\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        # dict to store training progress\n",
    "        self.history = {'train_loss': [],\n",
    "               'val_loss': [],\n",
    "               'train_acc':[],\n",
    "               'val_acc':[]\n",
    "               }\n",
    "        self.in_feat = None\n",
    "        self.model = None\n",
    "        \n",
    "        # transfer learning parameters\n",
    "        self.classifiers_n = -1          \n",
    "        self.features_n = -1\n",
    "        \n",
    "        # check for GPU availability\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "\n",
    "        # load model architectures without weight\n",
    "        if use_gpu:\n",
    "            self.model = getattr(models, self.pretrained_model_name)().cuda()\n",
    "        else:\n",
    "            self.model = getattr(models, self.pretrained_model_name)()\n",
    "\n",
    "        # load pre-trained weights\n",
    "        self.model.load_state_dict(torch.load(self.pretrained_model_path))\n",
    "\n",
    "        # get input dimension of the fc layer to be replaced and index of the last fc layer\n",
    "        self.in_feat = self.model.classifier[-1].in_features\n",
    "        fc_idx = len(self.model.classifier) - 1\n",
    "\n",
    "        custom_fc = nn.Sequential(nn.Linear(self.in_feat, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(512, self.num_classes),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.LogSoftmax(dim=1))\n",
    "\n",
    "        # add custom fc layers to model\n",
    "        self.model.classifier[fc_idx] = custom_fc\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    # freezes all layers in the model\n",
    "    def freeze_all_layers(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # unfreeze last 'n' fully connected layers\n",
    "    def unfreeze_last_n_fc_layers(model, n):\n",
    "\n",
    "        # if n == -1 don't unfreeze any layers\n",
    "        if n == -1:\n",
    "            return 0\n",
    "\n",
    "        n = n*2 # since weights and bias are included as separate\n",
    "        total_layers = len(list(self.classifier.parameters()))\n",
    "\n",
    "        # invalid n\n",
    "        if n > total_layers:\n",
    "            print(f\"Warning: There are only {total_layers} layers in the model. Cannot unfreeze {n} layers.\")\n",
    "\n",
    "        # if n == 0 unfreeze all layers\n",
    "        elif n == 0:\n",
    "            for param in self.classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for i, param in enumerate(self.classifier.parameters()):\n",
    "                if i >= (total_layers - n):\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "                    \n",
    "\n",
    "    # unfreeze last 'n' fully connected layers\n",
    "    def unfreeze_last_n_conv_layers(self, n):\n",
    "        \n",
    "        # if n == -1 don't unfreeze any layers\n",
    "        if n == -1:\n",
    "            return 0\n",
    "\n",
    "        n = n*2 # since weights and bias are included as separate\n",
    "        total_layers = len(list(self.features.parameters()))\n",
    "\n",
    "        # invalid n\n",
    "        if n > total_layers:\n",
    "            print(f\"Warning: There are only {total_layers} layers in the model. Cannot unfreeze {n} layers.\")\n",
    "        # if n == 0 unfreeze all layers\n",
    "        elif n == 0:\n",
    "            for param in self.features.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for i, param in enumerate(self.features.parameters()):\n",
    "                if i >= total_layers - n:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    pass\n",
    "    \n",
    "    # set parameters for transfer learning\n",
    "    def set_transfer_learning_params(self, unfreeze_n_fc, unfreeze_n_conv):\n",
    "        self.classifier_n = unfreeze_n_fc\n",
    "        self.features_n = unfreeze_n_conv\n",
    "        self.freeze_all_layers()\n",
    "        self.unfreeze_last_n_fc_layers(unfreeze_n_fc)\n",
    "        self.unfreeze_last_n_conv_layers(unfreeze_n_conv)\n",
    "    \n",
    "    \n",
    "    def get_optimizer_params_list(self):\n",
    "        # list of dictionaries to store parameter values\n",
    "        params_list = []\n",
    "\n",
    "        # dividing factor\n",
    "        f_fc = 2\n",
    "        f_conv = 3\n",
    "\n",
    "        if self.classifier_n != -1:\n",
    "            if self.classifier_n == 0:\n",
    "                named_params = list(name for name, _ in model.model.classifier.named_parameters())\n",
    "                layer_indices = list(set([int(name.split('.')[0]) for name in named_params]))\n",
    "            else:\n",
    "                # get indices of the last 'n' layers in the model\n",
    "                named_params = list(name for name, _ in model.model.classifier.named_parameters())\n",
    "                layer_indices = list(set([int(name.split('.')[0]) for name in named_params[-classifier_n*2:]]))\n",
    "            for i, index_val in enumerate(layer_indices):\n",
    "                params_list.append({'params':model.model.classifier[index_val].parameters(), 'lr': self.lr*(f_fc**i)})\n",
    "\n",
    "\n",
    "        if self.features_n != -1:\n",
    "            if self.features_n == 0:\n",
    "                named_params = list(name for name, _ in model.model.features.named_parameters())\n",
    "                layer_indices = list(set([int(name.split('.')[0]) for name in named_params]))\n",
    "            else:\n",
    "                # get indices of the last 'n' layers in the model\n",
    "                named_params = list(name for name, _ in model.model.features.named_parameters())\n",
    "                layer_indices = list(set([int(name.split('.')[0]) for name in named_params[-features_n*2:]]))\n",
    "            for i, index_val in enumerate(layer_indices):\n",
    "                params_list.append({'params':model.model.features[index_val].parameters(), 'lr': self.lr*(f_conv**(i+1))})\n",
    "                \n",
    "        if self.classifier_n == self.features_n == -1:\n",
    "            return self.parameters()\n",
    "                \n",
    "        return params_list\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        params_list = self.get_optimizer_params_list()\n",
    "        optimizer = Adam(params_list, lr = self.lr)\n",
    "        return optimizer\n",
    "        \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logps = self(x)\n",
    "        loss = F.nll_loss(logps, y)\n",
    "        y_pred = torch.argmax(torch.exp(logps), 1)\n",
    "        acc = (y_pred == y).sum().item()/len(y)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_acc', acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss, acc    \n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        num_items = len(outputs)\n",
    "        cum_loss = 0\n",
    "        cum_acc = 0\n",
    "        for i, (loss, acc) in enumerate(outputs):\n",
    "            cum_loss += loss\n",
    "            cum_acc += acc\n",
    "        \n",
    "        self.history['train_loss'].append(cum_loss/num_items)\n",
    "        self.history['train_acc'].append(cum_acc/num_items)     \n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logps = self(x)\n",
    "        loss = F.nll_loss(logps, y)\n",
    "        y_pred = torch.argmax(torch.exp(logps, 1))\n",
    "        acc = (y_pred == y).sum().item()/len(y)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss, acc\n",
    "    \n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        num_items = len(outputs)\n",
    "        cum_loss = 0\n",
    "        cum_acc = 0\n",
    "        for i, (loss, acc) in enumerate(outputs):\n",
    "            cum_loss += loss\n",
    "            cum_acc += acc\n",
    "        \n",
    "        self.history['val_loss'].append(cum_loss/num_items)\n",
    "        self.history['val_acc'].append(cum_acc/num_items)\n",
    "        \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logps = self(x)\n",
    "        loss = F.nll_loss(logps, y)\n",
    "        y_pred = torch.argmax(torch.exp(logps, 1))\n",
    "        acc = (y_pred == y).sum().item()/len(y)\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('test_acc', acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss, acc\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        # Define your test epoch end logic here\n",
    "        pass\n",
    "    \n",
    "    # create dataset objects\n",
    "    def setup(self, stage=None):\n",
    "        # define transformers\n",
    "        train_transform = transforms.Compose([\n",
    "                transforms.Resize(resizing_factor),\n",
    "                transforms.RandomHorizontalFlip(0.5),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.RandomAffine(degrees = 10,\n",
    "                                        translate = (0.2, 0.2), shear = 10),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)])\n",
    "\n",
    "        test_transform = transforms.Compose([transforms.Resize(resizing_factor),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize(mean, std)])\n",
    "        \n",
    "        # create datasets\n",
    "        train = torchvision.datasets.ImageFolder(self.train_path, transform=train_transform)\n",
    "        total_items = len(train)\n",
    "        val_size = int(total_items*0.2)\n",
    "        train_size = total_items - val_size\n",
    "        self.train_dataset, self.val_dataset = random_split(train, [train_size, val_size])\n",
    "        self.test_dataset = torchvision.datasets.ImageFolder(self.test_path, transform=test_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, self.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Instantiate the Lightning Trainer\n",
    "trainer = pl.Trainer(max_epochs=epochs)\n",
    "\n",
    "# Instantiate the model with metadata\n",
    "model = ConvNetPL(pretrained_model_name, pretrained_model_path, num_classes)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
