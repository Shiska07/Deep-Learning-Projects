{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acd74601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "032b5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images should be normalized before being fed to a network for efficient training\n",
    "\n",
    "# these are the mean and std of the data per channel\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cefb125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "testset = datasets.CIFAR10(root='~/.pytorch/CIFAR10',download = True, train=False, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "030836c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82cdef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bde4ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 224, 224])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a355da",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "744cbd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = images[image_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7ec291d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1212b4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([224, 224, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to view image we need the dimensions to be (h, w, channels)\n",
    "np.transpose(img, (1,2,0)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637be0a5",
   "metadata": {},
   "source": [
    "### Denormaliztion\n",
    "To denormalize the images for viewing, we need to multiply with the standard deviation and then add the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7985dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.ones((5,5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97a746bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c41e75db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = np.array([1,2,3], ndmin = 2)\n",
    "mean = np.array([0.25, 0.5, 0.75], ndmin = 2)\n",
    "std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "270f00ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "what would be the result of arr*std+mean?\n",
    "we can see that the shapes of std and mean = (3,)\n",
    "So think about how 'arr' can be even ly divided into 3 parts. \n",
    "As 'arr' has 3 channels in the third dimension, the effect will also be\n",
    "in the third dimension i.e. each element in 'std' amd 'mean' will be applied\n",
    "to each of the channels in the third dimension\n",
    "'''\n",
    "res = arr*std+mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6dcd8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.25, 1.25, 1.25, 1.25, 1.25],\n",
       "       [1.25, 1.25, 1.25, 1.25, 1.25],\n",
       "       [1.25, 1.25, 1.25, 1.25, 1.25],\n",
       "       [1.25, 1.25, 1.25, 1.25, 1.25],\n",
       "       [1.25, 1.25, 1.25, 1.25, 1.25]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ffa7b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.5, 2.5, 2.5, 2.5, 2.5],\n",
       "       [2.5, 2.5, 2.5, 2.5, 2.5],\n",
       "       [2.5, 2.5, 2.5, 2.5, 2.5],\n",
       "       [2.5, 2.5, 2.5, 2.5, 2.5],\n",
       "       [2.5, 2.5, 2.5, 2.5, 2.5]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a5afeb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.75, 3.75, 3.75, 3.75, 3.75],\n",
       "       [3.75, 3.75, 3.75, 3.75, 3.75],\n",
       "       [3.75, 3.75, 3.75, 3.75, 3.75],\n",
       "       [3.75, 3.75, 3.75, 3.75, 3.75],\n",
       "       [3.75, 3.75, 3.75, 3.75, 3.75]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:,:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0442279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49557590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.3584211 ,  1.8683473 ,  1.9776908 ],\n",
       "        [ 1.3584211 ,  1.8683473 ,  1.9776908 ],\n",
       "        [ 1.3584211 ,  1.8683473 ,  1.9776908 ],\n",
       "        ...,\n",
       "        [-0.04580877,  0.8004202 ,  0.70535964],\n",
       "        [-0.04580877,  0.8004202 ,  0.70535964],\n",
       "        [-0.04580877,  0.8004202 ,  0.70535964]],\n",
       "\n",
       "       [[ 1.3584211 ,  1.8683473 ,  1.9776908 ],\n",
       "        [ 1.3584211 ,  1.8683473 ,  1.9776908 ],\n",
       "        [ 1.3584211 ,  1.8683473 ,  1.9776908 ],\n",
       "        ...,\n",
       "        [-0.04580877,  0.8004202 ,  0.70535964],\n",
       "        [-0.04580877,  0.8004202 ,  0.70535964],\n",
       "        [-0.04580877,  0.8004202 ,  0.70535964]],\n",
       "\n",
       "       [[ 1.3584211 ,  1.8683473 ,  1.9776908 ],\n",
       "        [ 1.3584211 ,  1.8683473 ,  1.9776908 ],\n",
       "        [ 1.3584211 ,  1.8683473 ,  1.9776908 ],\n",
       "        ...,\n",
       "        [-0.04580877,  0.8004202 ,  0.70535964],\n",
       "        [-0.04580877,  0.8004202 ,  0.70535964],\n",
       "        [-0.04580877,  0.8004202 ,  0.70535964]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.6726604 ,  0.11764706, -0.30553368],\n",
       "        [-1.6726604 ,  0.11764706, -0.30553368],\n",
       "        [-1.6726604 ,  0.11764706, -0.30553368],\n",
       "        ...,\n",
       "        [-2.0836544 , -1.4229691 , -1.3861438 ],\n",
       "        [-2.0836544 , -1.4229691 , -1.3861438 ],\n",
       "        [-2.0836544 , -1.4229691 , -1.3861438 ]],\n",
       "\n",
       "       [[-1.6726604 ,  0.11764706, -0.30553368],\n",
       "        [-1.6726604 ,  0.11764706, -0.30553368],\n",
       "        [-1.6726604 ,  0.11764706, -0.30553368],\n",
       "        ...,\n",
       "        [-2.0836544 , -1.4229691 , -1.3861438 ],\n",
       "        [-2.0836544 , -1.4229691 , -1.3861438 ],\n",
       "        [-2.0836544 , -1.4229691 , -1.3861438 ]],\n",
       "\n",
       "       [[-1.6726604 ,  0.11764706, -0.30553368],\n",
       "        [-1.6726604 ,  0.11764706, -0.30553368],\n",
       "        [-1.6726604 ,  0.11764706, -0.30553368],\n",
       "        ...,\n",
       "        [-2.0836544 , -1.4229691 , -1.3861438 ],\n",
       "        [-2.0836544 , -1.4229691 , -1.3861438 ],\n",
       "        [-2.0836544 , -1.4229691 , -1.3861438 ]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(images[image_id],(1,2,0)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a71e8b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = (np.transpose(images[image_id],(1,2,0))).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a9418a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fc5219c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        ...,\n",
       "        [0.        , 0.8004202 , 0.70535964],\n",
       "        [0.        , 0.8004202 , 0.70535964],\n",
       "        [0.        , 0.8004202 , 0.70535964]],\n",
       "\n",
       "       [[1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        ...,\n",
       "        [0.        , 0.8004202 , 0.70535964],\n",
       "        [0.        , 0.8004202 , 0.70535964],\n",
       "        [0.        , 0.8004202 , 0.70535964]],\n",
       "\n",
       "       [[1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        ...,\n",
       "        [0.        , 0.8004202 , 0.70535964],\n",
       "        [0.        , 0.8004202 , 0.70535964],\n",
       "        [0.        , 0.8004202 , 0.70535964]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.        , 0.11764706, 0.        ],\n",
       "        [0.        , 0.11764706, 0.        ],\n",
       "        [0.        , 0.11764706, 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.11764706, 0.        ],\n",
       "        [0.        , 0.11764706, 0.        ],\n",
       "        [0.        , 0.11764706, 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.11764706, 0.        ],\n",
       "        [0.        , 0.11764706, 0.        ],\n",
       "        [0.        , 0.11764706, 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.clip(img, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bfee736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the mean and std of the data per channel\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96085042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor):\n",
    "    tensor = tensor*std + mean\n",
    "    return tensor\n",
    "\n",
    "def show_img(img):\n",
    "    # arrange channels\n",
    "    img = img.numpy().transpose((1,2,0))\n",
    "    \n",
    "    # use mean and std values\n",
    "    img = denormalize(img)\n",
    "    \n",
    "    # clip values and view image\n",
    "    img = np.clip(img,0,1)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ad3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(images[image_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.ones((50, 50))\n",
    "arr[15:35,15:35] = 0\n",
    "plt.imshow(arr, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bfa335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
