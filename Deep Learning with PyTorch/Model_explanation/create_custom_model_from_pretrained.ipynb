{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHnA5p8qsU80",
        "outputId": "de607d09-a28e-4a8d-8e91-9ff8e2697b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "aU93QHqtsZwD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchsummary\n",
        "from torch.optim import Adam\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "U9LO8WWesalB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oh8xCPSt8M0",
        "outputId": "68e60797-d842-468c-a772-df52dfd0b933"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning\n",
            "  Downloading lightning-2.1.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]<2025.0,>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2023.6.0)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.23.5)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (23.2)\n",
            "Requirement already satisfied: torch<4.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.1.0+cu118)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.5.0)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.1.2-py3-none-any.whl (776 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.9/776.9 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>2021.06.0->lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>2021.06.0->lightning) (3.8.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=1.12.0->lightning) (2.1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.12.0->lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.1.2 lightning-utilities-0.9.0 pytorch-lightning-2.1.2 torchmetrics-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl"
      ],
      "metadata": {
        "id": "IRQFZNoDuxfN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50SKFXLYsfzS",
        "outputId": "1f348466-2a81-48a3-98ec-36d78e570c43"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# these are the mean and std of the data per channel\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "\n",
        "def denormalize(tensor):\n",
        "    tensor = tensor*std + mean\n",
        "    return tensor\n",
        "\n",
        "# function for viewling image\n",
        "def show_img(img):\n",
        "    # arrange channels\n",
        "    img = img.numpy().transpose((1,2,0))\n",
        "\n",
        "    # use mean and std values\n",
        "    img = denormalize(img)\n",
        "\n",
        "    # clip values and view image\n",
        "    rgb_img = np.clip(img,0,1)\n",
        "\n",
        "    return np.float32(rgb_img)"
      ],
      "metadata": {
        "id": "Jd88dcSttS7w"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load and return model\n",
        "def return_model(name, path):\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "\n",
        "    # load model architectures without weight\n",
        "    if use_gpu:\n",
        "        model = getattr(models, name)().cuda()\n",
        "    else:\n",
        "        model = getattr(models, name)()\n",
        "\n",
        "    # load pre-trained weights\n",
        "    model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "hEiJpg_RshNh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize model class"
      ],
      "metadata": {
        "id": "7IgrXAQ4x_B1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# provide model name and path\n",
        "name = 'vgg16'\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/pretrained_models/vgg16.pth'"
      ],
      "metadata": {
        "id": "YbhjoudztxxI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load test dataset\n",
        "test_data = datasets.CIFAR10(root='./data', train=False, download=True,\n",
        "                                              transform=transform)\n",
        "test_dataloader = DataLoader(test_data, batch_size=1, num_workers=2)"
      ],
      "metadata": {
        "id": "euIy1VA4y9Qf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baa67d75-310b-460e-c42f-c6af23620604"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10Classifier(pl.LightningModule):\n",
        "    def __init__(self, pretrained_model):\n",
        "        super(CIFAR10Classifier, self).__init__()\n",
        "        self.model = pretrained_model\n",
        "        self.batch_size = 64\n",
        "        self.resizing_factor = 224\n",
        "        self.num_classes = 10\n",
        "        self.val_ratio = 0.3\n",
        "        self.loss_fn = nn.NLLLoss()\n",
        "        self.train_data, self.val_data, self.test_data = None, None, None\n",
        "        self.model_dest_folder = '/content/drive/MyDrive/Colab Notebooks/modified_model'\n",
        "\n",
        "        # transfer learning parameters\n",
        "        self.mean = (0.485, 0.456, 0.406)\n",
        "        self.std = (0.229, 0.224, 0.225)\n",
        "        self.classifiers_n = -1\n",
        "        self.features_n = -1\n",
        "\n",
        "        # lists to store outputs from each train/val step\n",
        "        self.training_step_outputs = []\n",
        "        self.validation_step_outputs = []\n",
        "        self.test_step_outputs = []\n",
        "\n",
        "        self.history = {'train_loss': [], 'train_acc': [],\n",
        "                    'val_loss': [], 'val_acc': []}\n",
        "\n",
        "        # modify model classifier\n",
        "        self.model.classifier[-1] = nn.Sequential(nn.Linear(4096, 512),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Dropout(0.5),\n",
        "                                nn.Linear(512, self.num_classes),\n",
        "                                nn.LogSoftmax(dim=1))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def configure_optimizers(self, mode = None):\n",
        "        optimizer = Adam(filter(lambda p: p.requires_grad,\n",
        "                            self.model.parameters()), lr=0.001)\n",
        "        return optimizer\n",
        "\n",
        "    # freezes all layers in the model\n",
        "    def freeze_all_layers(self):\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # unfreeze last 'n' fully connected layers\n",
        "    def unfreeze_last_n_fc_layers(self, n):\n",
        "\n",
        "        # if n == -1 don't unfreeze any layers\n",
        "        if n == -1:\n",
        "            return 0\n",
        "\n",
        "        n = n*2  # since weights and bias are included as separate\n",
        "        total_layers = len(list(self.model.classifier.parameters()))\n",
        "\n",
        "        # invalid n\n",
        "        if n > total_layers:\n",
        "            print(f\"Warning: There are only {total_layers} layers in the model. Cannot unfreeze {n} layers.\")\n",
        "\n",
        "        # if n == 0 unfreeze all layers\n",
        "        elif n == 0:\n",
        "            for param in self.model.classifier.parameters():\n",
        "                param.requires_grad = True\n",
        "        else:\n",
        "            for i, param in enumerate(self.model.classifier.parameters()):\n",
        "                if i >= (total_layers - n):\n",
        "                    param.requires_grad = True\n",
        "                else:\n",
        "                    param.requires_grad = False\n",
        "\n",
        "    # unfreeze last 'n' fully connected layers\n",
        "    def unfreeze_last_n_conv_layers(self, n):\n",
        "\n",
        "        # if n == -1 don't unfreeze any layers\n",
        "        if n == -1:\n",
        "            return 0\n",
        "\n",
        "        n = n*2  # since weights and bias are included as separate\n",
        "        total_layers = len(list(self.model.features.parameters()))\n",
        "\n",
        "        # invalid n\n",
        "        if n > total_layers:\n",
        "            print(\n",
        "                f\"Warning: There are only {total_layers} layers in the model. Cannot unfreeze {n} layers.\")\n",
        "        # if n == 0 unfreeze all layers\n",
        "        elif n == 0:\n",
        "            for param in self.model.features.parameters():\n",
        "                param.requires_grad = True\n",
        "        else:\n",
        "            for i, param in enumerate(self.model.features.parameters()):\n",
        "                if i >= total_layers - n:\n",
        "                    param.requires_grad = True\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "    # set parameters for transfer learning\n",
        "    def set_transfer_learning_params(self, unfreeze_n_fc, unfreeze_n_conv):\n",
        "        self.classifiers_n = unfreeze_n_fc\n",
        "        self.features_n = unfreeze_n_conv\n",
        "        self.freeze_all_layers()\n",
        "        self.unfreeze_last_n_fc_layers(unfreeze_n_fc)\n",
        "        self.unfreeze_last_n_conv_layers(unfreeze_n_conv)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self.model(x)\n",
        "        loss = self.loss_fn(logits, y)\n",
        "        y_pred = torch.argmax(torch.exp(logits), 1)\n",
        "        acc = (y_pred == y).sum().item()/self.batch_size\n",
        "        self.training_step_outputs.append((loss.item(), acc))\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        num_items = len(self.training_step_outputs)\n",
        "        cum_loss = 0\n",
        "        cum_acc = 0\n",
        "        for loss, acc in self.training_step_outputs:\n",
        "            cum_loss += loss\n",
        "            cum_acc += acc\n",
        "\n",
        "        avg_epoch_loss = cum_loss/num_items\n",
        "        avg_epoch_acc = cum_acc/num_items\n",
        "        self.history['train_loss'].append(avg_epoch_loss)\n",
        "        self.history['train_acc'].append(avg_epoch_acc)\n",
        "        print(f'\\nTraining Epoch({self.current_epoch}): loss: {avg_epoch_loss}, acc:{avg_epoch_acc}')\n",
        "        self.training_step_outputs.clear()\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self.model(x)\n",
        "        loss = self.loss_fn(logits, y)\n",
        "        y_pred = torch.argmax(torch.exp(logits), 1)\n",
        "        acc = (y_pred == y).sum().item()/self.batch_size\n",
        "        self.validation_step_outputs.append((loss.item(), acc))\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        num_items = len(self.validation_step_outputs)\n",
        "        cum_loss = 0\n",
        "        cum_acc = 0\n",
        "        for loss, acc in self.validation_step_outputs:\n",
        "            cum_loss += loss\n",
        "            cum_acc += acc\n",
        "\n",
        "        avg_epoch_loss = cum_loss/num_items\n",
        "        avg_epoch_acc = cum_acc/num_items\n",
        "        self.history['val_loss'].append(avg_epoch_loss)\n",
        "        self.history['val_acc'].append(avg_epoch_acc)\n",
        "        print(f'\\nValidation Epoch({self.current_epoch}): loss: {avg_epoch_loss}, acc:{avg_epoch_acc}')\n",
        "        self.validation_step_outputs.clear()\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self.model(x)\n",
        "        loss = self.loss_fn(logits, y)\n",
        "        y_pred = torch.argmax(torch.exp(logits), 1)\n",
        "        acc = (y_pred == y).sum().item() / self.batch_size\n",
        "        self.test_step_outputs.append((loss.item(), acc))\n",
        "        return loss\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        num_items = len(self.test_step_outputs)\n",
        "        cum_loss = 0\n",
        "        cum_acc = 0\n",
        "        for loss, acc in self.test_step_outputs:\n",
        "            cum_loss += loss\n",
        "            cum_acc += acc\n",
        "\n",
        "        avg_epoch_loss = cum_loss / num_items\n",
        "        avg_epoch_acc = cum_acc / num_items\n",
        "        print(f'Test Epoch loss: {avg_epoch_loss} Test epoch Acc: {avg_epoch_acc}')\n",
        "        self.test_step_outputs.clear()\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # Download CIFAR-10 dataset\n",
        "        datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "        datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "\n",
        "    def setup(self, stage = None):\n",
        "        transform = transforms.Compose([\n",
        "                transforms.Resize((self.resizing_factor, self.resizing_factor)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(self.mean, self.std)\n",
        "            ])\n",
        "\n",
        "        if stage == 'fit' or stage == 'validate' or stage is None:\n",
        "\n",
        "\n",
        "            # load train and validation datasets\n",
        "            train = datasets.CIFAR10(root='./data', train=True, transform=transform)\n",
        "            val_size = int(self.val_ratio * len(train))\n",
        "            train_size = len(train) - val_size\n",
        "            self.train_data, self.val_data = random_split(train, [train_size, val_size])\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_data, batch_size=self.batch_size, num_workers=2)\n",
        "\n",
        "    def get_history(self):\n",
        "        # remove the first validation epoch data\n",
        "        self.history['val_loss'].pop(0)\n",
        "        self.history['val_acc'].pop(0)\n",
        "        return self.history\n",
        "\n",
        "    def clear_history(self):\n",
        "        for key in self.history:\n",
        "            self.history[key] = []\n",
        "\n",
        "    def save_model(self):\n",
        "        # save the entire model\n",
        "        arc_final_path = os.path.join(self.model_dest_folder, 'vgg16_arc.pth')\n",
        "        weights_final_path = os.path.join(self.model_dest_folder, 'vgg16_weights.pth')\n",
        "        torch.save(self.model, arc_final_path)\n",
        "        torch.save(self.model.state_dict(), weights_final_path)\n"
      ],
      "metadata": {
        "id": "vLfRla2Gt6Pz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Transfer Learning Pipeline"
      ],
      "metadata": {
        "id": "OB2rN5ZnyA75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransferLearningPipiline:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "        # funny connected layers to unfreeze from last\n",
        "        self.n_fc = 2\n",
        "\n",
        "        # number of total fully connected layers to unfreeze\n",
        "        self.n_compfc = 0\n",
        "\n",
        "        self.epochs = dict()\n",
        "        self.epochs['fc'] = 2\n",
        "        self.epochs['compfc'] = 2\n",
        "        self.trainer = None\n",
        "\n",
        "    def initalize_trainer(self, mode):\n",
        "        self.trainer = pl.Trainer(accelerator=\"gpu\", devices=1,  max_epochs=self.epochs[mode],\n",
        "                                 enable_progress_bar=False, limit_val_batches=10,\n",
        "                                 enable_checkpointing=True, logger=False)\n",
        "\n",
        "    def train_custom_fc_layers(self):\n",
        "\n",
        "        # freeze all layers except the last two fc layers\n",
        "        self.model.set_transfer_learning_params(self.n_fc, -1)\n",
        "        self.model.configure_optimizers('lr_fc')\n",
        "        self.model.clear_history()\n",
        "\n",
        "        # train model\n",
        "        self.initalize_trainer('fc')\n",
        "        self.trainer.fit(self.model)\n",
        "\n",
        "\n",
        "    def train_all_fc_layers(self):\n",
        "\n",
        "        # freeze all layers except the last two fc layers\n",
        "        self.model.set_transfer_learning_params(self.n_compfc, -1)\n",
        "        self.model.configure_optimizers('lr_compfc')\n",
        "        self.model.clear_history()\n",
        "\n",
        "        # train model\n",
        "        self.initalize_trainer('compfc')\n",
        "        self.trainer.fit(self.model)\n",
        "\n",
        "    # complete transfer learning pipeline\n",
        "    def train_model(self):\n",
        "        self.train_custom_fc_layers()\n",
        "        self.train_all_fc_layers()\n",
        "\n",
        "    def save_model(self):\n",
        "        self.model.save_model()\n",
        "\n"
      ],
      "metadata": {
        "id": "UMbBrbYZyCcy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = return_model(name, path)"
      ],
      "metadata": {
        "id": "6Hxpktc9-qgF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_model = CIFAR10Classifier(pretrained_model)"
      ],
      "metadata": {
        "id": "lWJ4Cg-8-3P2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initalize trainer\n",
        "trainer = TransferLearningPipiline(custom_model)"
      ],
      "metadata": {
        "id": "vUFyF1Xd_BM2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit model\n",
        "trainer.train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuF_uNDT_KXG",
        "outputId": "6fcebc2b-7829-45db-cba1-748c1e344898"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name    | Type    | Params\n",
            "------------------------------------\n",
            "0 | model   | VGG     | 136 M \n",
            "1 | loss_fn | NLLLoss | 0     \n",
            "------------------------------------\n",
            "2.1 M     Trainable params\n",
            "134 M     Non-trainable params\n",
            "136 M     Total params\n",
            "545.453   Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Epoch(0): loss: 2.3530668020248413, acc:0.03125\n",
            "\n",
            "Validation Epoch(0): loss: 0.9280479192733765, acc:0.703125\n",
            "\n",
            "Training Epoch(0): loss: 1.2147839065007797, acc:0.5536448811700183\n",
            "\n",
            "Validation Epoch(1): loss: 0.9605607450008392, acc:0.6796875\n",
            "\n",
            "Training Epoch(1): loss: 1.132152301301904, acc:0.5868372943327239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=2` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory /content/checkpoints exists and is not empty.\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name    | Type    | Params\n",
            "------------------------------------\n",
            "0 | model   | VGG     | 136 M \n",
            "1 | loss_fn | NLLLoss | 0     \n",
            "------------------------------------\n",
            "121 M     Trainable params\n",
            "14.7 M    Non-trainable params\n",
            "136 M     Total params\n",
            "545.453   Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Epoch(0): loss: 0.9534895420074463, acc:0.7265625\n",
            "\n",
            "Validation Epoch(0): loss: 0.6159978538751603, acc:0.821875\n",
            "\n",
            "Training Epoch(0): loss: 0.8382723775489674, acc:0.735974634369287\n",
            "\n",
            "Validation Epoch(1): loss: 0.5258054852485656, acc:0.846875\n",
            "\n",
            "Training Epoch(1): loss: 0.5536500936343421, acc:0.8330952925045704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=2` reached.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "fWq0oz9hg_6N"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sHp8ualr85eO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}